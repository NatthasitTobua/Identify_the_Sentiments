{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identify the Sentiments",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXeySQl/+Ed13eegGP+G6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9829d062c3b84bbaa2b3471da9f3f36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1931735efe5843288341a40f6b945c8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0c8e8b61ff14adb81b6b79c4f621fb9",
              "IPY_MODEL_33703fb990954865a34ab98408b4b85e",
              "IPY_MODEL_626f6e512a014f1b82fcefd29b87311f"
            ]
          }
        },
        "1931735efe5843288341a40f6b945c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0c8e8b61ff14adb81b6b79c4f621fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2933f48be9c456a8b82a463ea8676f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef2e068ebbdf4a00ad4ef531037c6e0c"
          }
        },
        "33703fb990954865a34ab98408b4b85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e72465f9b894a11b67cd118c5443aff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4fd9644d325f4f4184d20aa4e12ac0d6"
          }
        },
        "626f6e512a014f1b82fcefd29b87311f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a60505305434ec38009f3b6c1219f77",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 958kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70ba3314cd7640988984b2406cac0e26"
          }
        },
        "c2933f48be9c456a8b82a463ea8676f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef2e068ebbdf4a00ad4ef531037c6e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e72465f9b894a11b67cd118c5443aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4fd9644d325f4f4184d20aa4e12ac0d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a60505305434ec38009f3b6c1219f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70ba3314cd7640988984b2406cac0e26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0269afdbadf4e02b86daf3f2796a255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a035c66dc4f844a185394f9f7dcc531f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e746d74a59f34720af1dc2330012a353",
              "IPY_MODEL_9de068a1541d4053846f3f96c3c00a5b",
              "IPY_MODEL_734e5278b0fd4e3db529ae6e52e8e9b1"
            ]
          }
        },
        "a035c66dc4f844a185394f9f7dcc531f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e746d74a59f34720af1dc2330012a353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa926f2082a04b4f8d76a00e6fede4e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_989e4790afbb45adbf4ce16fc7512899"
          }
        },
        "9de068a1541d4053846f3f96c3c00a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b627408859554120a2c77cdeb870c49d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f558867bf37b4ab69d1144d415dd3e15"
          }
        },
        "734e5278b0fd4e3db529ae6e52e8e9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7803e6256c8748ec866d7d82cfa4def0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 675kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1784e60cd2924b47a15c89f9ef8ea6b0"
          }
        },
        "aa926f2082a04b4f8d76a00e6fede4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "989e4790afbb45adbf4ce16fc7512899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b627408859554120a2c77cdeb870c49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f558867bf37b4ab69d1144d415dd3e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7803e6256c8748ec866d7d82cfa4def0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1784e60cd2924b47a15c89f9ef8ea6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d45454e349c414297e14719c14badc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b8361f037484354a6bad47a80953f9e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b613e19682904f37af70657ad59f1e3e",
              "IPY_MODEL_55df2dcc856846daabd92ac4217eb6e2",
              "IPY_MODEL_54447463db254cc5a809907db5cb853b"
            ]
          }
        },
        "0b8361f037484354a6bad47a80953f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b613e19682904f37af70657ad59f1e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2632d369c1064f51b0d7129bfe03892b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5187a17d20d4b248e8c5d5ca3ec0ef7"
          }
        },
        "55df2dcc856846daabd92ac4217eb6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4764a25807044393a6326776c234bb4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1af07911d2ab4b0f9353706c8ca16ae7"
          }
        },
        "54447463db254cc5a809907db5cb853b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_341bf6a13a364752b9819dbb6c96ce47",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.69MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ceb8ebe1b89459290f2fcb489b179b7"
          }
        },
        "2632d369c1064f51b0d7129bfe03892b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5187a17d20d4b248e8c5d5ca3ec0ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4764a25807044393a6326776c234bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1af07911d2ab4b0f9353706c8ca16ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "341bf6a13a364752b9819dbb6c96ce47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ceb8ebe1b89459290f2fcb489b179b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be7528d4ef3e4461971e5f06a9e293ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_22e8d9be5add4dd5894de803c0109a41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e28bac18e6564957a0a1b7b3958c884a",
              "IPY_MODEL_f5d81cac1e034b6183bf5a7d78f59a2a",
              "IPY_MODEL_ffea247c381f4a8e84cc70210442a719"
            ]
          }
        },
        "22e8d9be5add4dd5894de803c0109a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e28bac18e6564957a0a1b7b3958c884a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_280f5d7468264d78bac866b77dab08e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1655f64ed0e5406687d23e6a322c3454"
          }
        },
        "f5d81cac1e034b6183bf5a7d78f59a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5702ffbd74024ad28ce7e3e270d1ef33",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a394856270a44334b989beb7ca528532"
          }
        },
        "ffea247c381f4a8e84cc70210442a719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fd76bdd4b034b55b9f1cef279792e0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 10.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc9ceef1c87a437d91f4a5201f71cfec"
          }
        },
        "280f5d7468264d78bac866b77dab08e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1655f64ed0e5406687d23e6a322c3454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5702ffbd74024ad28ce7e3e270d1ef33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a394856270a44334b989beb7ca528532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fd76bdd4b034b55b9f1cef279792e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc9ceef1c87a437d91f4a5201f71cfec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83067b6c3ad84252a59e1eef398bb1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c47e30b2bc9d460faa350d74b578b71d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d863f48ed75644638bf9c675a2af1d4a",
              "IPY_MODEL_15284f8daefb41b7b0c6d7b85bbdb547",
              "IPY_MODEL_f829b9705d794a3ea4723fd01a139d74"
            ]
          }
        },
        "c47e30b2bc9d460faa350d74b578b71d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d863f48ed75644638bf9c675a2af1d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0af82becc6d045b6b7951f721bdce992",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a38c2a5e50f644a9a9b1959ae0b30c1f"
          }
        },
        "15284f8daefb41b7b0c6d7b85bbdb547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0ded275a11b4befb0affd5bd46810d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2a7a91967554fbdb8282aca4dce26c7"
          }
        },
        "f829b9705d794a3ea4723fd01a139d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cef36841f8b7495db54aca3993d9bbba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:12&lt;00:00, 40.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cae61a89271d44b4aadbbe29041d2fda"
          }
        },
        "0af82becc6d045b6b7951f721bdce992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a38c2a5e50f644a9a9b1959ae0b30c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0ded275a11b4befb0affd5bd46810d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2a7a91967554fbdb8282aca4dce26c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cef36841f8b7495db54aca3993d9bbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cae61a89271d44b4aadbbe29041d2fda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatthasitTobua/Identify_the_Sentiments/blob/main/Identify_the_Sentiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEWJiSiDU2I",
        "outputId": "d448b3a0-1f1b-4ddc-ce94-073d9fcce8c3"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rzWwOvDpc6",
        "outputId": "7d9b6666-a8fc-466e-8ee3-c3c7f7c699af"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 29 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.282 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk-82GQyDsxS",
        "outputId": "2a33693c-2572-4451-9e83-6f0a20673edb"
      },
      "source": [
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import warnings\n",
        "\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, roc_curve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5TAMeNQ5019"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNrTcpbBD1j5"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/test.csv\")\n",
        "ss = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/sample_submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sksxRsToGCnQ",
        "outputId": "34286e13-7450-4f59-b2c3-510e2625f67b"
      },
      "source": [
        "print('train data has', train.shape[0], 'rows')\n",
        "print('test data has', test.shape[0], 'rows')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data has 7920 rows\n",
            "test data has 1953 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "NW2FAQikGe4X",
        "outputId": "a45d4f7c-01a7-4d93-bb1d-3d84b3e708e9"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
              "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
              "2   3      0  We love this! Would you go? #talk #makememorie..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oeCb_NvGokC"
      },
      "source": [
        "**EXPLORING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "8VaizDhxHfnw",
        "outputId": "dc1f15ec-cf76-425d-a1ad-391f396c73e4"
      },
      "source": [
        "#see how many labels in each category\n",
        "sns.countplot(train['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6a441ae10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OUeXHepiGw0H",
        "outputId": "f90f189e-81d4-4be4-9361-996668dbcd0e"
      },
      "source": [
        "#plot lenght of tweet\n",
        "sns.histplot(train['tweet'].apply(lambda x: len(x.split())), bins = 30, color = 'blue')\n",
        "sns.histplot(test['tweet'].apply(lambda x: len(x.split())), bins = 30, color = 'red')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6a441a290>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekSWtpOTHMrf",
        "outputId": "dde9bce2-529a-4f8e-d96b-638fee04841d"
      },
      "source": [
        "#average length of text\n",
        "print('train average length :',train['tweet'].apply(lambda x: len(x.split())).mean())\n",
        "print('test average length :',test['tweet'].apply(lambda x: len(x.split())).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train average length : 16.843434343434343\n",
            "test average length : 16.872503840245777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Rv3yGeLKh2"
      },
      "source": [
        "def cleaned_tweet(doc):\n",
        "  # remove links and non-ASCII characters\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  doc = url.sub('',doc)\n",
        "\n",
        "  # lower case and remove special characters\\whitespaces\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "  \n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kuTQrK7LTqn"
      },
      "source": [
        "train['cleaned_tweet'] = train['tweet'].apply(lambda x : cleaned_tweet(x))\n",
        "test['cleaned_tweet'] = test['tweet'].apply(lambda x : cleaned_tweet(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "qbAqu6ZdLwmt",
        "outputId": "b9274f80-74af-4e06-f323-ce1b771db639"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
              "      <td>fingerprint pregnancy test  android apps beaut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
              "      <td>finally a transparant silicon case  thanks to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
              "      <td>we love this would you go talk makememories un...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                      cleaned_tweet\n",
              "0   1  ...  fingerprint pregnancy test  android apps beaut...\n",
              "1   2  ...  finally a transparant silicon case  thanks to ...\n",
              "2   3  ...  we love this would you go talk makememories un...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExzBfSMHyGqd",
        "outputId": "74a1ecdb-d6b7-4fa2-9a73-ca59e4a1884f"
      },
      "source": [
        "np.argmax(train['tweet'].apply(lambda x: len(x.split())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1794"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzOQDaA8ngJa",
        "outputId": "0d647ab0-5666-4611-806c-ee7a928e067a"
      },
      "source": [
        "print(train['tweet'][1794])\n",
        "print(train['cleaned_tweet'][1794])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Me lo dices o me lo preguntas #Apple ? Ya mi teléfono funciona solo conectado a la electricidad. Do you tell me or do yo ask me #apple ? Now my iPhone works only connected.Have you asked yourself if you were in a bad economic situation could you pay a new phone? #apple https://twitter.com/el_interes/status/944881090115899392 …\n",
            "me lo dices o me lo preguntas apple  ya mi telfono funciona solo conectado a la electricidad do you tell me or do yo ask me apple  now my iphone works only connectedhave you asked yourself if you were in a bad economic situation could you pay a new phone apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDLEvCLhPCxF"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXIkjmklPCZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "9829d062c3b84bbaa2b3471da9f3f36f",
            "1931735efe5843288341a40f6b945c8b",
            "c0c8e8b61ff14adb81b6b79c4f621fb9",
            "33703fb990954865a34ab98408b4b85e",
            "626f6e512a014f1b82fcefd29b87311f",
            "c2933f48be9c456a8b82a463ea8676f0",
            "ef2e068ebbdf4a00ad4ef531037c6e0c",
            "3e72465f9b894a11b67cd118c5443aff",
            "4fd9644d325f4f4184d20aa4e12ac0d6",
            "0a60505305434ec38009f3b6c1219f77",
            "70ba3314cd7640988984b2406cac0e26",
            "f0269afdbadf4e02b86daf3f2796a255",
            "a035c66dc4f844a185394f9f7dcc531f",
            "e746d74a59f34720af1dc2330012a353",
            "9de068a1541d4053846f3f96c3c00a5b",
            "734e5278b0fd4e3db529ae6e52e8e9b1",
            "aa926f2082a04b4f8d76a00e6fede4e1",
            "989e4790afbb45adbf4ce16fc7512899",
            "b627408859554120a2c77cdeb870c49d",
            "f558867bf37b4ab69d1144d415dd3e15",
            "7803e6256c8748ec866d7d82cfa4def0",
            "1784e60cd2924b47a15c89f9ef8ea6b0",
            "5d45454e349c414297e14719c14badc7",
            "0b8361f037484354a6bad47a80953f9e",
            "b613e19682904f37af70657ad59f1e3e",
            "55df2dcc856846daabd92ac4217eb6e2",
            "54447463db254cc5a809907db5cb853b",
            "2632d369c1064f51b0d7129bfe03892b",
            "f5187a17d20d4b248e8c5d5ca3ec0ef7",
            "4764a25807044393a6326776c234bb4a",
            "1af07911d2ab4b0f9353706c8ca16ae7",
            "341bf6a13a364752b9819dbb6c96ce47",
            "9ceb8ebe1b89459290f2fcb489b179b7",
            "be7528d4ef3e4461971e5f06a9e293ed",
            "22e8d9be5add4dd5894de803c0109a41",
            "e28bac18e6564957a0a1b7b3958c884a",
            "f5d81cac1e034b6183bf5a7d78f59a2a",
            "ffea247c381f4a8e84cc70210442a719",
            "280f5d7468264d78bac866b77dab08e4",
            "1655f64ed0e5406687d23e6a322c3454",
            "5702ffbd74024ad28ce7e3e270d1ef33",
            "a394856270a44334b989beb7ca528532",
            "1fd76bdd4b034b55b9f1cef279792e0a",
            "bc9ceef1c87a437d91f4a5201f71cfec"
          ]
        },
        "outputId": "02cc8a9e-5561-4d1c-ec0b-90c665274909"
      },
      "source": [
        "#use pretrained model \n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9829d062c3b84bbaa2b3471da9f3f36f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0269afdbadf4e02b86daf3f2796a255",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d45454e349c414297e14719c14badc7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be7528d4ef3e4461971e5f06a9e293ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "aEDOinsPPRC0",
        "outputId": "30e0c371-b057-4b51-8f0c-21a9b9079ab1"
      },
      "source": [
        "#plot the distribution after tokenize\n",
        "token_counts = []\n",
        "for _, row in train.iterrows():\n",
        "    token_count = len(tokenizer.encode(\n",
        "                                       row[\"cleaned_tweet\"],\n",
        "                                       max_length = 128,\n",
        "                                       truncation = True\n",
        "                                      )\n",
        "                     )\n",
        "    token_counts.append(token_count)\n",
        "sns.distplot(token_counts)\n",
        "plt.xlim([0, 128])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 128.0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aRvGsofPwKs"
      },
      "source": [
        "#most tweets contain less than 128 tokens, then stick with this number\n",
        "MAX_TOKEN_COUNT = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIn4WB48Pzis",
        "outputId": "34c4a3df-7a69-49e0-88f9-751eb55b9bb6"
      },
      "source": [
        "#split the data into train set and valid set\n",
        "train_df, valid_df = train_test_split(train, test_size = 0.2, random_state = 42)\n",
        "print('train df has', train_df.shape[0], 'rows')\n",
        "print('valid df has', valid_df.shape[0], 'rows')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train df has 6336 rows\n",
            "valid df has 1584 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3CEqyXGQNI0"
      },
      "source": [
        "class MakeDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "        self.labels = self.data['label']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        cleaned_tweet = data_row['cleaned_tweet']\n",
        "        labels = data_row['label']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                                              cleaned_tweet,\n",
        "                                              add_special_tokens = True,\n",
        "                                              max_length = self.max_token_len,\n",
        "                                              return_token_type_ids = False,\n",
        "                                              padding = \"max_length\",\n",
        "                                              truncation = True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_tensors = 'pt',\n",
        "                                             )\n",
        "        return dict(\n",
        "                    cleaned_tweet = cleaned_tweet,\n",
        "                    input_ids = encoding[\"input_ids\"].flatten(),\n",
        "                    attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "                    labels = labels\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgXxa5JkR5Vr"
      },
      "source": [
        "train_dataset = MakeDataset(\n",
        "                            train_df,\n",
        "                            tokenizer,\n",
        "                            max_token_len = MAX_TOKEN_COUNT\n",
        "                           )\n",
        "valid_dataset = MakeDataset(\n",
        "                            valid_df,\n",
        "                            tokenizer,\n",
        "                            max_token_len = MAX_TOKEN_COUNT\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jp2N-exHogm",
        "outputId": "90a9d362-8144-4853-d724-2b8601544389"
      },
      "source": [
        "#see how output look like\n",
        "train_dataset[987]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'cleaned_tweet': 'hey apple shopcrabtree you suck spent  to have a power cable last me  year and  months',\n",
              " 'input_ids': tensor([    0, 12229, 15162,  2792,  8344,   873, 21512,    47, 23829,  1240,\n",
              "          1437,     7,    33,    10,   476,  6129,    94,   162,  1437,    76,\n",
              "             8,  1437,   377,     2,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'labels': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "83067b6c3ad84252a59e1eef398bb1fc",
            "c47e30b2bc9d460faa350d74b578b71d",
            "d863f48ed75644638bf9c675a2af1d4a",
            "15284f8daefb41b7b0c6d7b85bbdb547",
            "f829b9705d794a3ea4723fd01a139d74",
            "0af82becc6d045b6b7951f721bdce992",
            "a38c2a5e50f644a9a9b1959ae0b30c1f",
            "f0ded275a11b4befb0affd5bd46810d2",
            "b2a7a91967554fbdb8282aca4dce26c7",
            "cef36841f8b7495db54aca3993d9bbba",
            "cae61a89271d44b4aadbbe29041d2fda"
          ]
        },
        "id": "0dpHuK6ESG4L",
        "outputId": "b06369d9-965f-4b0f-96da-c5935a6ba60d"
      },
      "source": [
        "#define model from pretrained model\n",
        "class RobertaClass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.bert_model = RobertaModel.from_pretrained(model_name, return_dict = True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert_model(input_ids, attention_mask)\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        output = torch.sigmoid(output)\n",
        "        return output\n",
        "    \n",
        "model = RobertaClass()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83067b6c3ad84252a59e1eef398bb1fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKU6cLvLkHIG"
      },
      "source": [
        "#calculate weight for imbalanced data\n",
        "train_label_count = train_df['label'].value_counts().values\n",
        "neg_label_count = [sum(train_label_count) - count for count in train_label_count]\n",
        "pos_weights = neg_label_count/train_label_count.min()\n",
        "pos_weights = torch.tensor(pos_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rBgJlDLhqKf"
      },
      "source": [
        "#define train and validation loop\n",
        "def _run(model):\n",
        "\n",
        "    def loss_fn(outputs, labels):\n",
        "        return nn.BCELoss(weight = pos_weights)(outputs, labels)\n",
        "    \n",
        "    def train_fn(train_dataloader, model, optimizer, scheduler, device):\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            \n",
        "            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = loss_fn(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            preds = (outputs > 0.5).float()\n",
        "            \n",
        "            correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            acc = correct_predictions.double() / len(train_df)\n",
        "\n",
        "            xm.master_print(f'Acc : {acc} train_loss : {np.mean(losses)}')\n",
        "\n",
        "    def valid_fn(valid_dataloader, model, device):\n",
        "\n",
        "        model.eval()\n",
        "        global y_pred \n",
        "        global y_valid \n",
        "        y_pred = []\n",
        "        y_valid = []\n",
        "\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(valid_dataloader):\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.float)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = loss_fn(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            preds = (outputs > 0.5).float()\n",
        "            \n",
        "            correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred.extend(preds.cpu().squeeze().numpy())\n",
        "            y_valid.extend(labels.cpu().numpy())\n",
        "\n",
        "            acc = correct_predictions.double() / len(valid_df)\n",
        "            \n",
        "            xm.master_print(f'Acc {acc} train_loss {np.mean(losses)}')\n",
        "    \n",
        "    EPOCHS = 10\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                    train_dataset,\n",
        "                                                                    num_replicas = xm.xrt_world_size(),\n",
        "                                                                    rank = xm.get_ordinal(),\n",
        "                                                                    shuffle = True\n",
        "                                                                   )\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                    valid_dataset,\n",
        "                                                                    num_replicas = xm.xrt_world_size(),\n",
        "                                                                    rank = xm.get_ordinal(),\n",
        "                                                                    shuffle = False\n",
        "                                                                   )\n",
        "    \n",
        "    train_data_loader = DataLoader(\n",
        "                                   train_dataset,\n",
        "                                   sampler = train_sampler,\n",
        "                                   batch_size = BATCH_SIZE,\n",
        "                                   num_workers = 2\n",
        "                                  )\n",
        "    valid_data_loader = DataLoader(\n",
        "                                   valid_dataset,\n",
        "                                   sampler = valid_sampler,\n",
        "                                   batch_size = BATCH_SIZE,\n",
        "                                   num_workers = 2\n",
        "                                  )\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "    model = model.to(device)\n",
        "\n",
        "    lr = 0.4 * 2e-05 * xm.xrt_world_size()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr)\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
        "    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE * EPOCHS)\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                                                   optimizer,\n",
        "                                                   num_warmup_steps = 0,\n",
        "                                                   num_training_steps = num_train_steps\n",
        "                                                  )\n",
        "\n",
        "    train_begin = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "\n",
        "        start = time.time()\n",
        "        print('*'*15)\n",
        "        print(f'EPOCH: {epoch+1}')\n",
        "        print('*'*15)\n",
        "\n",
        "        print('Training.....')\n",
        "        train_fn( \n",
        "                 train_dataloader = para_loader.per_device_loader(device),\n",
        "                 model = model, \n",
        "                 optimizer = optimizer,\n",
        "                 scheduler = lr_scheduler,\n",
        "                 device = device\n",
        "                )\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "          para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "            \n",
        "          print('Validating....')\n",
        "          valid_fn( \n",
        "                 valid_dataloader = para_loader.per_device_loader(device),\n",
        "                 model = model,\n",
        "                 device = device\n",
        "                )\n",
        "        \n",
        "        print(f'F1_score : {f1_score(y_valid, y_pred)}')\n",
        "\n",
        "        print(f'Epoch completed in {(time.time() - start)/60} minutes')\n",
        "    print(f'Training completed in {(time.time() - train_begin)/60} minutes')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5EVU37bm-Qs",
        "outputId": "87784bbd-80f9-4305-873f-60aa0308cac3"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = _run(model)\n",
        "\n",
        "FLAGS={}\n",
        "xmp.spawn(_mp_fn, args = (FLAGS, ), nprocs = 1, start_method = 'fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train_steps = 1980, world_size=1\n",
            "***************\n",
            "EPOCH: 1\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0018939394503831863 train_loss : 1.39670991897583\n",
            "Acc : 0.0045770201832056046 train_loss : 1.3904433846473694\n",
            "Acc : 0.007260100916028023 train_loss : 1.3914451996485393\n",
            "Acc : 0.010416666977107525 train_loss : 1.3855776190757751\n",
            "Acc : 0.013573232106864452 train_loss : 1.379532241821289\n",
            "Acc : 0.017045455053448677 train_loss : 1.3741677006085713\n",
            "Acc : 0.021148988977074623 train_loss : 1.3638942411967687\n",
            "Acc : 0.02509469725191593 train_loss : 1.356111153960228\n",
            "Acc : 0.028882576152682304 train_loss : 1.3470403618282742\n",
            "Acc : 0.032828282564878464 train_loss : 1.3337366819381713\n",
            "Acc : 0.0364583320915699 train_loss : 1.3238185969266025\n",
            "Acc : 0.039772726595401764 train_loss : 1.3186306059360504\n",
            "Acc : 0.0434027761220932 train_loss : 1.3056060259158795\n",
            "Acc : 0.04719065502285957 train_loss : 1.2863938297544206\n",
            "Acc : 0.050978533923625946 train_loss : 1.2694584767023722\n",
            "Acc : 0.054450757801532745 train_loss : 1.2629042640328407\n",
            "Acc : 0.05918560549616814 train_loss : 1.2224240934147554\n",
            "Acc : 0.062342170625925064 train_loss : 1.2277887066205342\n",
            "Acc : 0.06644570827484131 train_loss : 1.2087033140031915\n",
            "Acc : 0.07007575780153275 train_loss : 1.2001886397600174\n",
            "Acc : 0.07417929172515869 train_loss : 1.1852802634239197\n",
            "Acc : 0.07796717435121536 train_loss : 1.1785316386006095\n",
            "Acc : 0.08238636702299118 train_loss : 1.155204767766206\n",
            "Acc : 0.08633206784725189 train_loss : 1.1419740542769432\n",
            "Acc : 0.08980429172515869 train_loss : 1.1376630139350892\n",
            "Acc : 0.09438131004571915 train_loss : 1.1147524095498598\n",
            "Acc : 0.09785353392362595 train_loss : 1.1096990196793168\n",
            "Acc : 0.10148358345031738 train_loss : 1.1017694345542364\n",
            "Acc : 0.10432449728250504 train_loss : 1.1053005292497833\n",
            "Acc : 0.10874368995428085 train_loss : 1.0910280108451844\n",
            "Acc : 0.11300504952669144 train_loss : 1.0784229578510407\n",
            "Acc : 0.11679293215274811 train_loss : 1.070709239691496\n",
            "Acc : 0.12026514858007431 train_loss : 1.0611689939643398\n",
            "Acc : 0.12436868995428085 train_loss : 1.0564407373175901\n",
            "Acc : 0.12863005697727203 train_loss : 1.0419115015438625\n",
            "Acc : 0.13304924964904785 train_loss : 1.029622616039382\n",
            "Acc : 0.13636364042758942 train_loss : 1.0238123246141382\n",
            "Acc : 0.13967803120613098 train_loss : 1.0188454734651666\n",
            "Acc : 0.14330807328224182 train_loss : 1.0102117107464716\n",
            "Acc : 0.1463068127632141 train_loss : 1.0166213616728783\n",
            "Acc : 0.15072600543498993 train_loss : 1.003419569352778\n",
            "Acc : 0.1545138955116272 train_loss : 0.9928239257562728\n",
            "Acc : 0.1575126200914383 train_loss : 0.9920680883318879\n",
            "Acc : 0.16114267706871033 train_loss : 0.9848715473305095\n",
            "Acc : 0.1649305522441864 train_loss : 0.9780802660518222\n",
            "Acc : 0.16919191181659698 train_loss : 0.9674291429312333\n",
            "Acc : 0.1731376200914383 train_loss : 0.9582377862423024\n",
            "Acc : 0.1770833283662796 train_loss : 0.9506666138768196\n",
            "Acc : 0.18071338534355164 train_loss : 0.9441838969989699\n",
            "Acc : 0.18371212482452393 train_loss : 0.9415611577033997\n",
            "Acc : 0.1879734843969345 train_loss : 0.9334169392492256\n",
            "Acc : 0.19207702577114105 train_loss : 0.9294100243311662\n",
            "Acc : 0.19554924964904785 train_loss : 0.9292119977609167\n",
            "Acc : 0.1991792917251587 train_loss : 0.925846274252291\n",
            "Acc : 0.20375631749629974 train_loss : 0.9173794816840779\n",
            "Acc : 0.20817551016807556 train_loss : 0.9113136184002671\n",
            "Acc : 0.21259470283985138 train_loss : 0.9049814118627917\n",
            "Acc : 0.2170138955116272 train_loss : 0.8990656481734638\n",
            "Acc : 0.22111742198467255 train_loss : 0.8927631868144214\n",
            "Acc : 0.22585226595401764 train_loss : 0.8883633817235629\n",
            "Acc : 0.2304292917251587 train_loss : 0.8823820533322506\n",
            "Acc : 0.23500631749629974 train_loss : 0.877150630277972\n",
            "Acc : 0.23926767706871033 train_loss : 0.8724772149608249\n",
            "Acc : 0.24273990094661713 train_loss : 0.8714994289912283\n",
            "Acc : 0.24715909361839294 train_loss : 0.8702225451286022\n",
            "Acc : 0.25189393758773804 train_loss : 0.8636498022260088\n",
            "Acc : 0.2564709484577179 train_loss : 0.855412591304352\n",
            "Acc : 0.26057448983192444 train_loss : 0.859468009103747\n",
            "Acc : 0.26499369740486145 train_loss : 0.8565990808217422\n",
            "Acc : 0.2695707082748413 train_loss : 0.8519821920565196\n",
            "Acc : 0.27414771914482117 train_loss : 0.8469913093976571\n",
            "Acc : 0.27888256311416626 train_loss : 0.8439607442253165\n",
            "Acc : 0.28330177068710327 train_loss : 0.8415918819708367\n",
            "Acc : 0.28708964586257935 train_loss : 0.8438722970517906\n",
            "Acc : 0.2913510203361511 train_loss : 0.8421032027403513\n",
            "Acc : 0.2956123650074005 train_loss : 0.8394435208879019\n",
            "Acc : 0.2998737394809723 train_loss : 0.8361287554363152\n",
            "Acc : 0.30445075035095215 train_loss : 0.831449914819155\n",
            "Acc : 0.30918559432029724 train_loss : 0.8260731780076329\n",
            "Acc : 0.3139204680919647 train_loss : 0.821240308880806\n",
            "Acc : 0.3186553120613098 train_loss : 0.8165826738616566\n",
            "Acc : 0.3232323229312897 train_loss : 0.8146092004892302\n",
            "Acc : 0.32780933380126953 train_loss : 0.8094765170510992\n",
            "Acc : 0.3319128751754761 train_loss : 0.8077015465214139\n",
            "Acc : 0.33664771914482117 train_loss : 0.8022441239917979\n",
            "Acc : 0.3410669267177582 train_loss : 0.801772033752397\n",
            "Acc : 0.34580177068710327 train_loss : 0.798070034418983\n",
            "Acc : 0.35037878155708313 train_loss : 0.7948210175064477\n",
            "Acc : 0.35479798913002014 train_loss : 0.7924263561039828\n",
            "Acc : 0.35969066619873047 train_loss : 0.787656749619378\n",
            "Acc : 0.36395201086997986 train_loss : 0.7867102590236035\n",
            "Acc : 0.3685290515422821 train_loss : 0.7825280696801518\n",
            "Acc : 0.3727903962135315 train_loss : 0.7827541639087021\n",
            "Acc : 0.3776830732822418 train_loss : 0.7778801093710229\n",
            "Acc : 0.38178661465644836 train_loss : 0.7777697531800521\n",
            "Acc : 0.3862058222293854 train_loss : 0.7740359815458456\n",
            "Acc : 0.39030933380126953 train_loss : 0.7726705356971505\n",
            "Acc : 0.3948863744735718 train_loss : 0.7685950152119811\n",
            "Acc : 0.39962121844291687 train_loss : 0.7638578165059138\n",
            "Acc : 0.4045138955116272 train_loss : 0.7590867501497268\n",
            "Acc : 0.4092487394809723 train_loss : 0.7541271037984603\n",
            "Acc : 0.41351011395454407 train_loss : 0.7543725234036353\n",
            "Acc : 0.4180871248245239 train_loss : 0.7517755309354912\n",
            "Acc : 0.42297980189323425 train_loss : 0.7485586152626917\n",
            "Acc : 0.42771464586257935 train_loss : 0.7451037401244753\n",
            "Acc : 0.4319760203361511 train_loss : 0.7459716543836413\n",
            "Acc : 0.4360795319080353 train_loss : 0.7451272005232695\n",
            "Acc : 0.44113004207611084 train_loss : 0.7392104264624693\n",
            "Acc : 0.4453914165496826 train_loss : 0.7378912739660761\n",
            "Acc : 0.4499684274196625 train_loss : 0.7350108208304101\n",
            "Acc : 0.4548611044883728 train_loss : 0.73143765941128\n",
            "Acc : 0.45943814516067505 train_loss : 0.7291714178531298\n",
            "Acc : 0.4643308222293854 train_loss : 0.7253107892069142\n",
            "Acc : 0.46906566619873047 train_loss : 0.7213470689429525\n",
            "Acc : 0.4736426770687103 train_loss : 0.7210732531936273\n",
            "Acc : 0.4783775210380554 train_loss : 0.7174649610473164\n",
            "Acc : 0.48327019810676575 train_loss : 0.713578162730759\n",
            "Acc : 0.48768940567970276 train_loss : 0.7130787030998933\n",
            "Acc : 0.4921085834503174 train_loss : 0.7122437289657713\n",
            "Acc : 0.4962121248245239 train_loss : 0.7115522828573982\n",
            "Acc : 0.5004734992980957 train_loss : 0.7116865182587923\n",
            "Acc : 0.5045770406723022 train_loss : 0.7131184126387854\n",
            "Acc : 0.5089961886405945 train_loss : 0.712955032604012\n",
            "Acc : 0.5138888955116272 train_loss : 0.7095789966443854\n",
            "Acc : 0.5187815427780151 train_loss : 0.7056508690714837\n",
            "Acc : 0.5228850841522217 train_loss : 0.7050197093141457\n",
            "Acc : 0.5271464586257935 train_loss : 0.705064111983213\n",
            "Acc : 0.5318813323974609 train_loss : 0.7022367838653736\n",
            "Acc : 0.5363004803657532 train_loss : 0.7022947915649229\n",
            "Acc : 0.5407196879386902 train_loss : 0.7019398677234466\n",
            "Acc : 0.5448232293128967 train_loss : 0.7026948248043315\n",
            "Acc : 0.5490846037864685 train_loss : 0.7013234073352633\n",
            "Acc : 0.5536616444587708 train_loss : 0.6994293728157094\n",
            "Acc : 0.5577651262283325 train_loss : 0.6999718437332716\n",
            "Acc : 0.5621843338012695 train_loss : 0.6981448996398184\n",
            "Acc : 0.566919207572937 train_loss : 0.6963501602739972\n",
            "Acc : 0.5714961886405945 train_loss : 0.6940241757861889\n",
            "Acc : 0.576231062412262 train_loss : 0.6910882166222386\n",
            "Acc : 0.5809659361839294 train_loss : 0.687967646625831\n",
            "Acc : 0.5853850841522217 train_loss : 0.6864477836127792\n",
            "Acc : 0.5890151262283325 train_loss : 0.6899462276211021\n",
            "Acc : 0.5932765007019043 train_loss : 0.6893960969443892\n",
            "Acc : 0.5980113744735718 train_loss : 0.6871277729010249\n",
            "Acc : 0.6025883555412292 train_loss : 0.6847795241305398\n",
            "Acc : 0.6073232293128967 train_loss : 0.6826652898356832\n",
            "Acc : 0.6115846037864685 train_loss : 0.6816014751077515\n",
            "Acc : 0.6161616444587708 train_loss : 0.6793535833760184\n",
            "Acc : 0.6202651262283325 train_loss : 0.6788551669970557\n",
            "Acc : 0.625 train_loss : 0.6765645277880182\n",
            "Acc : 0.629419207572937 train_loss : 0.6757985011239847\n",
            "Acc : 0.6339961886405945 train_loss : 0.6743966585259564\n",
            "Acc : 0.6388888955116272 train_loss : 0.6712440651792445\n",
            "Acc : 0.6428346037864685 train_loss : 0.6713654569843236\n",
            "Acc : 0.6474116444587708 train_loss : 0.6699798633042094\n",
            "Acc : 0.651830792427063 train_loss : 0.6681830815249874\n",
            "Acc : 0.6565656661987305 train_loss : 0.6657613083624687\n",
            "Acc : 0.6613004803657532 train_loss : 0.6641290530942048\n",
            "Acc : 0.6657196879386902 train_loss : 0.6627004953407789\n",
            "Acc : 0.6702967286109924 train_loss : 0.6619474966102427\n",
            "Acc : 0.6748737096786499 train_loss : 0.6606501820962876\n",
            "Acc : 0.6791350841522217 train_loss : 0.6601542308001045\n",
            "Acc : 0.6840277910232544 train_loss : 0.6577505597637759\n",
            "Acc : 0.6882891654968262 train_loss : 0.6572227521450973\n",
            "Acc : 0.6922348737716675 train_loss : 0.6575636241766738\n",
            "Acc : 0.6971275210380554 train_loss : 0.6555151445847569\n",
            "Acc : 0.7020202279090881 train_loss : 0.6525325698396528\n",
            "Acc : 0.7069128751754761 train_loss : 0.6505030003225732\n",
            "Acc : 0.7116477489471436 train_loss : 0.6486895022736419\n",
            "Acc : 0.716224730014801 train_loss : 0.6465959651671218\n",
            "Acc : 0.7211174368858337 train_loss : 0.6442060791832559\n",
            "Acc : 0.7258522510528564 train_loss : 0.6420948803860541\n",
            "Acc : 0.7302714586257935 train_loss : 0.6428919745132674\n",
            "Acc : 0.7348484992980957 train_loss : 0.641580666291576\n",
            "Acc : 0.7392676472663879 train_loss : 0.6416070692926302\n",
            "Acc : 0.743686854839325 train_loss : 0.6402779099345207\n",
            "Acc : 0.7487373948097229 train_loss : 0.6381627877073531\n",
            "Acc : 0.7533143758773804 train_loss : 0.6369008740547013\n",
            "Acc : 0.7578914165496826 train_loss : 0.6358459592284111\n",
            "Acc : 0.7621527910232544 train_loss : 0.6354508603501586\n",
            "Acc : 0.7665719985961914 train_loss : 0.6346585014214118\n",
            "Acc : 0.7711489796638489 train_loss : 0.633308466503304\n",
            "Acc : 0.7755681872367859 train_loss : 0.6328358511430222\n",
            "Acc : 0.7803030014038086 train_loss : 0.6309290434249112\n",
            "Acc : 0.7848800420761108 train_loss : 0.6295740883392484\n",
            "Acc : 0.7894570827484131 train_loss : 0.6283076985865026\n",
            "Acc : 0.7940340638160706 train_loss : 0.6271089716383846\n",
            "Acc : 0.7986111044883728 train_loss : 0.625462206170202\n",
            "Acc : 0.8036616444587708 train_loss : 0.6230000122351215\n",
            "Acc : 0.8077651262283325 train_loss : 0.6234629451834336\n",
            "Acc : 0.8126578330993652 train_loss : 0.6209936530574371\n",
            "Acc : 0.8177083134651184 train_loss : 0.6181834614713778\n",
            "Acc : 0.8222853541374207 train_loss : 0.6171161734188596\n",
            "Acc : 0.8268623948097229 train_loss : 0.6156222173278196\n",
            "Acc : 0.8312815427780151 train_loss : 0.6160616658090317\n",
            "Acc : 0.8358585834503174 train_loss : 0.617031353712082\n",
            "Acc : 0.8402777910232544 train_loss : 0.6159513763019017\n",
            "Acc : 0.8448547720909119 train_loss : 0.614958146350638\n",
            "Acc : 0.8491161465644836 train_loss : 0.6154818581511275\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.3552757203578949\n",
            "Acc 0.03598484769463539 train_loss 0.40948039293289185\n",
            "Acc 0.056186869740486145 train_loss 0.2954947501420975\n",
            "Acc 0.07575757801532745 train_loss 0.2738913930952549\n",
            "Acc 0.09469696879386902 train_loss 0.2943180829286575\n",
            "Acc 0.11300504952669144 train_loss 0.3252975220481555\n",
            "Acc 0.12941919267177582 train_loss 0.3665392888443811\n",
            "Acc 0.14835858345031738 train_loss 0.37444829382002354\n",
            "Acc 0.16729797422885895 train_loss 0.36711303227477604\n",
            "Acc 0.18560606241226196 train_loss 0.36703975051641463\n",
            "Acc 0.20328283309936523 train_loss 0.39686374637213623\n",
            "Acc 0.2209595888853073 train_loss 0.4328223007420699\n",
            "Acc 0.23926767706871033 train_loss 0.4181419083705315\n",
            "Acc 0.25883838534355164 train_loss 0.39951661761317936\n",
            "Acc 0.2790403962135315 train_loss 0.3808436766266823\n",
            "Acc 0.29671716690063477 train_loss 0.3926073885522783\n",
            "Acc 0.3156565725803375 train_loss 0.3890962228178978\n",
            "Acc 0.33396464586257935 train_loss 0.3922488966749774\n",
            "Acc 0.35353535413742065 train_loss 0.3785113688362272\n",
            "Acc 0.3737373650074005 train_loss 0.3690456245094538\n",
            "Acc 0.39393940567970276 train_loss 0.35477740104709354\n",
            "Acc 0.41287878155708313 train_loss 0.36040314998139034\n",
            "Acc 0.4318181872367859 train_loss 0.3641749409877736\n",
            "Acc 0.4501262605190277 train_loss 0.36704125472654897\n",
            "Acc 0.46843433380126953 train_loss 0.3656259116530418\n",
            "Acc 0.48674243688583374 train_loss 0.37465303763747215\n",
            "Acc 0.5056818127632141 train_loss 0.3723867643210623\n",
            "Acc 0.5239899158477783 train_loss 0.3731928316078016\n",
            "Acc 0.5404040217399597 train_loss 0.38481986702516163\n",
            "Acc 0.558080792427063 train_loss 0.38872945283850036\n",
            "Acc 0.5763888955116272 train_loss 0.3921323185486178\n",
            "Acc 0.5959596037864685 train_loss 0.38479925761930645\n",
            "Acc 0.6155303120613098 train_loss 0.38176162924730417\n",
            "Acc 0.631313145160675 train_loss 0.3946993410149041\n",
            "\n",
            "Acc 0.6502525210380554 train_loss 0.39602434188127517\n",
            "Acc 0.6685606241226196 train_loss 0.39188880576855606\n",
            "Acc 0.6862373948097229 train_loss 0.3976131647422507\n",
            "Acc 0.7039141654968262 train_loss 0.39829614934952634\n",
            "Acc 0.7203282713890076 train_loss 0.4061635979093038\n",
            "Acc 0.7373737096786499 train_loss 0.4116037802770734\n",
            "Acc 0.7569444179534912 train_loss 0.41072730919936806\n",
            "Acc 0.7746211886405945 train_loss 0.4122916277320612\n",
            "Acc 0.7929292917251587 train_loss 0.41400563249061273\n",
            "Acc 0.809974730014801 train_loss 0.42223906466229394\n",
            "Acc 0.8289141654968262 train_loss 0.4178112280037668\n",
            "Acc 0.8478535413742065 train_loss 0.4162025741584923\n",
            "Acc 0.8667929172515869 train_loss 0.41344101045359954\n",
            "Acc 0.8863636255264282 train_loss 0.40971703253065545\n",
            "Acc 0.9059343338012695 train_loss 0.41199852298108897\n",
            "Acc 0.9147727489471436 train_loss 0.4154766909778118\n",
            "F1_score : 0.8511576626240352\n",
            "Epoch completed in 2.7825220425923667 minutes\n",
            "***************\n",
            "EPOCH: 2\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004419191740453243 train_loss : 0.8483525514602661\n",
            "Acc : 0.009154040366411209 train_loss : 0.5553512126207352\n",
            "Acc : 0.013573232106864452 train_loss : 0.5539149343967438\n",
            "Acc : 0.01815025322139263 train_loss : 0.4919721484184265\n",
            "Acc : 0.022885100916028023 train_loss : 0.4783891260623932\n",
            "Acc : 0.02777777798473835 train_loss : 0.4434151202440262\n",
            "Acc : 0.03219696879386902 train_loss : 0.44328148450170246\n",
            "Acc : 0.037089645862579346 train_loss : 0.42061980441212654\n",
            "Acc : 0.04182449355721474 train_loss : 0.40406326121754116\n",
            "Acc : 0.046243686228990555 train_loss : 0.4132294714450836\n",
            "Acc : 0.05082070827484131 train_loss : 0.4087529480457306\n",
            "Acc : 0.055397726595401764 train_loss : 0.4094332382082939\n",
            "Acc : 0.06044823303818703 train_loss : 0.389846879702348\n",
            "Acc : 0.06534090638160706 train_loss : 0.389663679259164\n",
            "Acc : 0.06960227340459824 train_loss : 0.39841436545054115\n",
            "Acc : 0.07433712482452393 train_loss : 0.40534187108278275\n",
            "Acc : 0.07844065874814987 train_loss : 0.4121551127994762\n",
            "Acc : 0.08317550271749496 train_loss : 0.4092068241702186\n",
            "Acc : 0.08743686974048615 train_loss : 0.4142894838985644\n",
            "Acc : 0.09185606241226196 train_loss : 0.4192023992538452\n",
            "Acc : 0.09643308073282242 train_loss : 0.4295863395645505\n",
            "Acc : 0.10101009905338287 train_loss : 0.4307746656916358\n",
            "Acc : 0.10542929172515869 train_loss : 0.43429942364278046\n",
            "Acc : 0.10969065874814987 train_loss : 0.43957993760704994\n",
            "Acc : 0.11426767706871033 train_loss : 0.4417674219608307\n",
            "Acc : 0.11852903664112091 train_loss : 0.4497790669019406\n",
            "Acc : 0.12263257801532745 train_loss : 0.4690694312254588\n",
            "Acc : 0.12705177068710327 train_loss : 0.4684894915137972\n",
            "Acc : 0.1314709633588791 train_loss : 0.4688896783466997\n",
            "Acc : 0.13557448983192444 train_loss : 0.4730589906374613\n",
            "Acc : 0.13952019810676575 train_loss : 0.47778166686334916\n",
            "Acc : 0.14378157258033752 train_loss : 0.4854473900049925\n",
            "Acc : 0.14851641654968262 train_loss : 0.48200544624617603\n",
            "Acc : 0.15293560922145844 train_loss : 0.4899253406945397\n",
            "Acc : 0.15767045319080353 train_loss : 0.48459150535719736\n",
            "Acc : 0.16224747896194458 train_loss : 0.48259880476527744\n",
            "Acc : 0.16682448983192444 train_loss : 0.481226842145662\n",
            "Acc : 0.171875 train_loss : 0.4765498198960957\n",
            "Acc : 0.17645202577114105 train_loss : 0.4765198146685576\n",
            "Acc : 0.18087121844291687 train_loss : 0.48419212326407435\n",
            "Acc : 0.18544822931289673 train_loss : 0.48150051948500844\n",
            "Acc : 0.19018307328224182 train_loss : 0.4771411915620168\n",
            "Acc : 0.19476009905338287 train_loss : 0.4770718674327052\n",
            "Acc : 0.1996527761220932 train_loss : 0.47451197423718194\n",
            "Acc : 0.2043876200914383 train_loss : 0.4725270324283176\n",
            "Acc : 0.20928029716014862 train_loss : 0.46731599873822666\n",
            "Acc : 0.21417297422885895 train_loss : 0.462959922057517\n",
            "Acc : 0.21875 train_loss : 0.45913117099553347\n",
            "Acc : 0.22364267706871033 train_loss : 0.4558408804694001\n",
            "Acc : 0.22821970283985138 train_loss : 0.45389044731855394\n",
            "Acc : 0.23279671370983124 train_loss : 0.45393870887803095\n",
            "Acc : 0.2373737394809723 train_loss : 0.45571375953463406\n",
            "Acc : 0.24195075035095215 train_loss : 0.45754237135626236\n",
            "Acc : 0.24684342741966248 train_loss : 0.4540774388997643\n",
            "Acc : 0.2514204680919647 train_loss : 0.45316011661832983\n",
            "Acc : 0.2561553120613098 train_loss : 0.45065032451280523\n",
            "Acc : 0.2607323229312897 train_loss : 0.4499181924681914\n",
            "Acc : 0.2651515007019043 train_loss : 0.44931061488801033\n",
            "Acc : 0.2700441777706146 train_loss : 0.4453194585897155\n",
            "Acc : 0.27446338534355164 train_loss : 0.444737849632899\n",
            "Acc : 0.2795138955116272 train_loss : 0.4407007589203412\n",
            "Acc : 0.2842487394809723 train_loss : 0.4407734334949524\n",
            "Acc : 0.2889835834503174 train_loss : 0.4383962776452776\n",
            "Acc : 0.2934027910232544 train_loss : 0.43919477448798716\n",
            "Acc : 0.2981376349925995 train_loss : 0.4405160585275063\n",
            "Acc : 0.30271464586257935 train_loss : 0.43864331981449417\n",
            "Acc : 0.30744948983192444 train_loss : 0.4349629632127819\n",
            "Acc : 0.31139519810676575 train_loss : 0.4430137717110269\n",
            "Acc : 0.31613004207611084 train_loss : 0.4394749837077182\n",
            "Acc : 0.3207070827484131 train_loss : 0.4401823735662869\n",
            "Acc : 0.3254419267177582 train_loss : 0.4399100015280952\n",
            "Acc : 0.3303346037864685 train_loss : 0.43712540922893417\n",
            "Acc : 0.33475378155708313 train_loss : 0.4382918709761476\n",
            "Acc : 0.3390151560306549 train_loss : 0.4411835537568943\n",
            "Acc : 0.34343433380126953 train_loss : 0.4420781179269155\n",
            "Acc : 0.3480113744735718 train_loss : 0.44153571717048945\n",
            "Acc : 0.35211488604545593 train_loss : 0.4455573291747601\n",
            "Acc : 0.3568497598171234 train_loss : 0.4440875840492738\n",
            "Acc : 0.36142677068710327 train_loss : 0.44333659319937985\n",
            "Acc : 0.36616161465644836 train_loss : 0.4413284074515104\n",
            "Acc : 0.3707386255264282 train_loss : 0.4399751674981765\n",
            "Acc : 0.37531566619873047 train_loss : 0.4424182765367555\n",
            "Acc : 0.38036614656448364 train_loss : 0.43935618953532485\n",
            "Acc : 0.3851010203361511 train_loss : 0.43815527785392033\n",
            "Acc : 0.3901515007019043 train_loss : 0.4352975012624965\n",
            "Acc : 0.39472854137420654 train_loss : 0.4383097154109977\n",
            "Acc : 0.3993055522441864 train_loss : 0.4394083620830514\n",
            "Acc : 0.4035669267177582 train_loss : 0.4406338261271065\n",
            "Acc : 0.4076704680919647 train_loss : 0.4411371583014392\n",
            "Acc : 0.4124053120613098 train_loss : 0.4395487399564849\n",
            "Acc : 0.4171401560306549 train_loss : 0.43853926511256247\n",
            "Acc : 0.42171716690063477 train_loss : 0.43719410653347557\n",
            "Acc : 0.4262941777706146 train_loss : 0.4385729549072122\n",
            "Acc : 0.43118685483932495 train_loss : 0.4357534781732458\n",
            "Acc : 0.43560606241226196 train_loss : 0.4366234900135743\n",
            "Acc : 0.4400252401828766 train_loss : 0.43674993437404436\n",
            "Acc : 0.4444444477558136 train_loss : 0.436243305040389\n",
            "Acc : 0.44949495792388916 train_loss : 0.4341456937242527\n",
            "Acc : 0.45422980189323425 train_loss : 0.4327171714317919\n",
            "Acc : 0.4592803120613098 train_loss : 0.42987453803420067\n",
            "Acc : 0.4640151560306549 train_loss : 0.4281130381444893\n",
            "Acc : 0.46875 train_loss : 0.4286879556728344\n",
            "Acc : 0.4734848439693451 train_loss : 0.4277272376331311\n",
            "Acc : 0.4783775210380554 train_loss : 0.4281678212663302\n",
            "Acc : 0.4831123650074005 train_loss : 0.4275673944325674\n",
            "Acc : 0.48768940567970276 train_loss : 0.42998605165279136\n",
            "Acc : 0.49195075035095215 train_loss : 0.43284160163357993\n",
            "Acc : 0.4968434274196625 train_loss : 0.42991697298431836\n",
            "Acc : 0.5015782713890076 train_loss : 0.42910409609385586\n",
            "Acc : 0.5059974789619446 train_loss : 0.42900360748171806\n",
            "Acc : 0.5105745196342468 train_loss : 0.4283666673141557\n",
            "Acc : 0.5151515007019043 train_loss : 0.4285298379005066\n",
            "Acc : 0.520044207572937 train_loss : 0.42695761445613034\n",
            "Acc : 0.5250946879386902 train_loss : 0.42436380939264046\n",
            "Acc : 0.5296717286109924 train_loss : 0.42661073538272276\n",
            "Acc : 0.5345643758773804 train_loss : 0.424697440310285\n",
            "Acc : 0.5396149158477783 train_loss : 0.4217836553724403\n",
            "Acc : 0.5438762903213501 train_loss : 0.4234262848809614\n",
            "Acc : 0.5484532713890076 train_loss : 0.42299333214759827\n",
            "Acc : 0.553188145160675 train_loss : 0.42291958183050155\n",
            "Acc : 0.5576072931289673 train_loss : 0.42227683160915847\n",
            "Acc : 0.5618686676025391 train_loss : 0.4250871956836982\n",
            "Acc : 0.5666035413742065 train_loss : 0.4252345494138516\n",
            "Acc : 0.5714961886405945 train_loss : 0.4238752160341509\n",
            "Acc : 0.5763888955116272 train_loss : 0.42206898486614225\n",
            "Acc : 0.5811237096786499 train_loss : 0.42200704261897104\n",
            "Acc : 0.5857007503509521 train_loss : 0.42351121825026716\n",
            "Acc : 0.5904356241226196 train_loss : 0.4233527692267671\n",
            "Acc : 0.5948547720909119 train_loss : 0.4244248955979828\n",
            "Acc : 0.5995896458625793 train_loss : 0.4240601932773223\n",
            "Acc : 0.6043245196342468 train_loss : 0.4242760192119438\n",
            "Acc : 0.6087436676025391 train_loss : 0.42470476268367335\n",
            "Acc : 0.6134785413742065 train_loss : 0.4235482712213258\n",
            "Acc : 0.6177399158477783 train_loss : 0.4262486744060445\n",
            "Acc : 0.6226325631141663 train_loss : 0.4249820101040381\n",
            "Acc : 0.627525269985199 train_loss : 0.42385595872559967\n",
            "Acc : 0.6322600841522217 train_loss : 0.42311393594654806\n",
            "Acc : 0.6369949579238892 train_loss : 0.4224254943538403\n",
            "Acc : 0.6420454382896423 train_loss : 0.4203541502892542\n",
            "Acc : 0.6466224789619446 train_loss : 0.4204194587256227\n",
            "Acc : 0.6507260203361511 train_loss : 0.42323814374757995\n",
            "Acc : 0.6551452279090881 train_loss : 0.42400256502376477\n",
            "Acc : 0.6598800420761108 train_loss : 0.42325439476049864\n",
            "Acc : 0.6646149158477783 train_loss : 0.42214086061964434\n",
            "Acc : 0.669349730014801 train_loss : 0.421576244563892\n",
            "Acc : 0.673768937587738 train_loss : 0.42142052142179176\n",
            "Acc : 0.6786616444587708 train_loss : 0.42066378161615253\n",
            "Acc : 0.683080792427063 train_loss : 0.42054843187734886\n",
            "Acc : 0.6878156661987305 train_loss : 0.4195724362694977\n",
            "Acc : 0.6923926472663879 train_loss : 0.4193247144420942\n",
            "Acc : 0.6971275210380554 train_loss : 0.41906682811430745\n",
            "Acc : 0.7021780014038086 train_loss : 0.4171419848540896\n",
            "Acc : 0.7065972089767456 train_loss : 0.4178277751783919\n",
            "Acc : 0.7114899158477783 train_loss : 0.4169303077188405\n",
            "Acc : 0.7163825631141663 train_loss : 0.4159611991336269\n",
            "Acc : 0.7211174368858337 train_loss : 0.4152061550472027\n",
            "Acc : 0.7260100841522217 train_loss : 0.4142965597521727\n",
            "Acc : 0.7304292917251587 train_loss : 0.4147671128186998\n",
            "Acc : 0.7351641654968262 train_loss : 0.41470162383040543\n",
            "Acc : 0.7398989796638489 train_loss : 0.4147670828737319\n",
            "Acc : 0.7441603541374207 train_loss : 0.4160371571779251\n",
            "Acc : 0.7490530014038086 train_loss : 0.41467017045727483\n",
            "Acc : 0.7534722089767456 train_loss : 0.4154900897499974\n",
            "Acc : 0.7575757503509521 train_loss : 0.41741988426301535\n",
            "Acc : 0.7624684572219849 train_loss : 0.4163706150921908\n",
            "Acc : 0.7673611044883728 train_loss : 0.41477098863527\n",
            "Acc : 0.7722538113594055 train_loss : 0.41366745843858777\n",
            "Acc : 0.7769886255264282 train_loss : 0.4133744997282823\n",
            "Acc : 0.7817234992980957 train_loss : 0.4121128827686141\n",
            "Acc : 0.7866161465644836 train_loss : 0.41099051806856607\n",
            "Acc : 0.7913510203361511 train_loss : 0.41049426785337995\n",
            "Acc : 0.7960858345031738 train_loss : 0.4116813019784384\n",
            "Acc : 0.8005050420761108 train_loss : 0.4115192165781308\n",
            "Acc : 0.8050820827484131 train_loss : 0.41182714777773827\n",
            "Acc : 0.8096590638160706 train_loss : 0.41167375947747914\n",
            "Acc : 0.8147096037864685 train_loss : 0.4106134675781835\n",
            "Acc : 0.8192866444587708 train_loss : 0.4102186602724474\n",
            "Acc : 0.8238636255264282 train_loss : 0.4101280297121305\n",
            "Acc : 0.8285984992980957 train_loss : 0.41001266041281503\n",
            "Acc : 0.8334911465644836 train_loss : 0.4092324473791652\n",
            "Acc : 0.8383838534355164 train_loss : 0.40822665516842793\n",
            "Acc : 0.8428030014038086 train_loss : 0.40921774883191664\n",
            "Acc : 0.8478535413742065 train_loss : 0.40789926434800927\n",
            "Acc : 0.8524305820465088 train_loss : 0.40764312778154144\n",
            "Acc : 0.8573232293128967 train_loss : 0.4070004107984337\n",
            "Acc : 0.861900269985199 train_loss : 0.4067933587457544\n",
            "Acc : 0.8667929172515869 train_loss : 0.4057783984882946\n",
            "Acc : 0.8718434572219849 train_loss : 0.4042269434542098\n",
            "Acc : 0.8762626051902771 train_loss : 0.4048868158349284\n",
            "Acc : 0.8811553120613098 train_loss : 0.40335954451247263\n",
            "Acc : 0.886205792427063 train_loss : 0.4014946097283775\n",
            "Acc : 0.8910984992980957 train_loss : 0.4003993842246321\n",
            "Acc : 0.8958333134651184 train_loss : 0.39961850154816797\n",
            "Acc : 0.9007260203361511 train_loss : 0.3987576628737536\n",
            "Acc : 0.9053030014038086 train_loss : 0.39951626450205463\n",
            "Acc : 0.9100378751754761 train_loss : 0.3988166742444951\n",
            "Acc : 0.9146149158477783 train_loss : 0.3991200478208549\n",
            "Acc : 0.9190340638160706 train_loss : 0.39968380623619365\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.5015987157821655\n",
            "Acc 0.03724747523665428 train_loss 0.46238814294338226\n",
            "Acc 0.05744949355721474 train_loss 0.3235449617107709\n",
            "Acc 0.07575757801532745 train_loss 0.3057690132409334\n",
            "Acc 0.09469696879386902 train_loss 0.32487893253564837\n",
            "Acc 0.11300504952669144 train_loss 0.34578248485922813\n",
            "Acc 0.12941919267177582 train_loss 0.39751348005873816\n",
            "Acc 0.14772726595401764 train_loss 0.412460801191628\n",
            "Acc 0.1666666716337204 train_loss 0.40296391066577697\n",
            "Acc 0.18497474491596222 train_loss 0.41448491290211675\n",
            "Acc 0.20328283309936523 train_loss 0.45881039513783023\n",
            "Acc 0.2209595888853073 train_loss 0.4941294100135565\n",
            "Acc 0.23926767706871033 train_loss 0.4904805954832297\n",
            "Acc 0.25883838534355164 train_loss 0.4668552667966911\n",
            "Acc 0.27840909361839294 train_loss 0.44548493772745135\n",
            "Acc 0.2960858643054962 train_loss 0.456140027847141\n",
            "Acc 0.31439393758773804 train_loss 0.44729608779444413\n",
            "Acc 0.33270201086997986 train_loss 0.45149680434001815\n",
            "Acc 0.35227271914482117 train_loss 0.4330578556186275\n",
            "Acc 0.3724747598171234 train_loss 0.41679268330335617\n",
            "Acc 0.39267677068710327 train_loss 0.3983632219689233\n",
            "Acc 0.41161614656448364 train_loss 0.40156105770306155\n",
            "Acc 0.4305555522441864 train_loss 0.40300928704116656\n",
            "Acc 0.4488636255264282 train_loss 0.4033241290599108\n",
            "Acc 0.4665403962135315 train_loss 0.40702787816524505\n",
            "Acc 0.4848484992980957 train_loss 0.4165762932254718\n",
            "Acc 0.504419207572937 train_loss 0.4113149493932724\n",
            "Acc 0.5227272510528564 train_loss 0.41144196848784176\n",
            "Acc 0.5397727489471436 train_loss 0.43073275120093907\n",
            "Acc 0.558080792427063 train_loss 0.4298789009451866\n",
            "Acc 0.5763888955116272 train_loss 0.4316053424150713\n",
            "Acc 0.5965909361839294 train_loss 0.4222349105402827\n",
            "Acc 0.6155303120613098 train_loss 0.4162646500450192\n",
            "Acc 0.6325757503509521 train_loss 0.427087644005523\n",
            "Acc 0.6515151262283325 train_loss 0.42980876863002776\n",
            "Acc 0.6704545617103577 train_loss 0.42627039758695495\n",
            "Acc 0.6875 train_loss 0.43781131868426865\n",
            "Acc 0.7051767706871033 train_loss 0.4410558237056983\n",
            "Acc 0.7228535413742065 train_loss 0.446222648024559\n",
            "Acc 0.7386363744735718 train_loss 0.45365995950996874\n",
            "Acc 0.7575757503509521 train_loss 0.4540668642375527\n",
            "Acc 0.7746211886405945 train_loss 0.45854737680582774\n",
            "Acc 0.7935606241226196 train_loss 0.45774945027606434\n",
            "Acc 0.809974730014801 train_loss 0.4663196195932952\n",
            "Acc 0.8289141654968262 train_loss 0.46036464075247446\n",
            "Acc 0.8465909361839294 train_loss 0.46333332133034\n",
            "Acc 0.8648989796638489 train_loss 0.45892854415355844\n",
            "Acc 0.8832070827484131 train_loss 0.4559623633200924\n",
            "Acc 0.9021464586257935 train_loss 0.4576219609197305\n",
            "Acc 0.9109848737716675 train_loss 0.46382999151945115\n",
            "F1_score : 0.8362369337979094\n",
            "Epoch completed in 2.0137945930163066 minutes\n",
            "***************\n",
            "EPOCH: 3\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.7551285028457642\n",
            "Acc : 0.009469697251915932 train_loss : 0.475642628967762\n",
            "Acc : 0.013731060549616814 train_loss : 0.5179842164119085\n",
            "Acc : 0.01862373761832714 train_loss : 0.443713054060936\n",
            "Acc : 0.023200757801532745 train_loss : 0.44128891825675964\n",
            "Acc : 0.02825126238167286 train_loss : 0.38878680268923443\n",
            "Acc : 0.03314393758773804 train_loss : 0.36713550559112\n",
            "Acc : 0.038036614656448364 train_loss : 0.34247491136193275\n",
            "Acc : 0.04292929172515869 train_loss : 0.3356196979681651\n",
            "Acc : 0.04766414314508438 train_loss : 0.3468351364135742\n",
            "Acc : 0.052241161465644836 train_loss : 0.3544745228507302\n",
            "Acc : 0.05681818351149559 train_loss : 0.36901194353898364\n",
            "Acc : 0.06171085685491562 train_loss : 0.3533628766353314\n",
            "Acc : 0.06660353392362595 train_loss : 0.33906118146010805\n",
            "Acc : 0.07086490094661713 train_loss : 0.3538115918636322\n",
            "Acc : 0.07544191926717758 train_loss : 0.35771916806697845\n",
            "Acc : 0.07970328629016876 train_loss : 0.36706412890378165\n",
            "Acc : 0.08475378900766373 train_loss : 0.35654302686452866\n",
            "Acc : 0.08948863297700882 train_loss : 0.36018455577524083\n",
            "Acc : 0.09406565874814987 train_loss : 0.36221566423773766\n",
            "Acc : 0.09864267706871033 train_loss : 0.362662469347318\n",
            "Acc : 0.10321969538927078 train_loss : 0.36415029046210373\n",
            "Acc : 0.10779671370983124 train_loss : 0.3663611159376476\n",
            "Acc : 0.11237373948097229 train_loss : 0.3656419000277917\n",
            "Acc : 0.11695075780153275 train_loss : 0.3675865143537521\n",
            "Acc : 0.12136995047330856 train_loss : 0.3708065559084599\n",
            "Acc : 0.1254734843969345 train_loss : 0.38786590927177006\n",
            "Acc : 0.1302083283662796 train_loss : 0.39370098018220495\n",
            "Acc : 0.13462752103805542 train_loss : 0.4006662528062689\n",
            "Acc : 0.13920454680919647 train_loss : 0.4009770448009173\n",
            "Acc : 0.14299242198467255 train_loss : 0.40511284480171816\n",
            "Acc : 0.14725378155708313 train_loss : 0.4144378020428121\n",
            "Acc : 0.15183080732822418 train_loss : 0.4125356534213731\n",
            "Acc : 0.15625 train_loss : 0.4214744098922786\n",
            "Acc : 0.16114267706871033 train_loss : 0.41427453543458664\n",
            "Acc : 0.16556186974048615 train_loss : 0.41425421461462975\n",
            "Acc : 0.17029671370983124 train_loss : 0.4135928198292449\n",
            "Acc : 0.1753472238779068 train_loss : 0.40904931920139415\n",
            "Acc : 0.17992424964904785 train_loss : 0.4099287577928641\n",
            "Acc : 0.18465909361839294 train_loss : 0.4105190936475992\n",
            "Acc : 0.18892045319080353 train_loss : 0.41146627540995434\n",
            "Acc : 0.19381313025951385 train_loss : 0.4074353923400243\n",
            "Acc : 0.19870580732822418 train_loss : 0.4074440480664719\n",
            "Acc : 0.2035984843969345 train_loss : 0.4044281061400067\n",
            "Acc : 0.20849116146564484 train_loss : 0.4020134899351332\n",
            "Acc : 0.21338383853435516 train_loss : 0.3982654835866845\n",
            "Acc : 0.2182765156030655 train_loss : 0.39349070857179924\n",
            "Acc : 0.22285354137420654 train_loss : 0.3917836071923375\n",
            "Acc : 0.22758838534355164 train_loss : 0.3894404814559586\n",
            "Acc : 0.23248106241226196 train_loss : 0.3872914269566536\n",
            "Acc : 0.23705807328224182 train_loss : 0.3888840181570427\n",
            "Acc : 0.24163509905338287 train_loss : 0.3905814728484704\n",
            "Acc : 0.24621212482452393 train_loss : 0.3923501605695149\n",
            "Acc : 0.250946968793869 train_loss : 0.38933026376697755\n",
            "Acc : 0.2556818127632141 train_loss : 0.38617572296749464\n",
            "Acc : 0.26057448983192444 train_loss : 0.3819692778800215\n",
            "Acc : 0.26530933380126953 train_loss : 0.3813719310258564\n",
            "Acc : 0.2700441777706146 train_loss : 0.37976277234225436\n",
            "Acc : 0.27493685483932495 train_loss : 0.37728712114237123\n",
            "Acc : 0.27935606241226196 train_loss : 0.37790885716676714\n",
            "Acc : 0.2844065725803375 train_loss : 0.3738090540053415\n",
            "Acc : 0.2891414165496826 train_loss : 0.3743591445588296\n",
            "Acc : 0.2938762605190277 train_loss : 0.3731894348821943\n",
            "Acc : 0.29845327138900757 train_loss : 0.37545636645518243\n",
            "Acc : 0.3030303120613098 train_loss : 0.378838040966254\n",
            "Acc : 0.30792298913002014 train_loss : 0.3759967258030718\n",
            "Acc : 0.3129734992980957 train_loss : 0.3719115763235448\n",
            "Acc : 0.3172348439693451 train_loss : 0.3752717032809468\n",
            "Acc : 0.3221275210380554 train_loss : 0.3713810732183249\n",
            "Acc : 0.3267045319080353 train_loss : 0.3718849398195744\n",
            "Acc : 0.3315972089767456 train_loss : 0.3718671684323902\n",
            "Acc : 0.33648988604545593 train_loss : 0.37022465291536516\n",
            "Acc : 0.3410669267177582 train_loss : 0.3700620918853642\n",
            "Acc : 0.3451704680919647 train_loss : 0.37741754214103157\n",
            "Acc : 0.3499053120613098 train_loss : 0.3749890304605166\n",
            "Acc : 0.3549558222293854 train_loss : 0.37176304575251906\n",
            "Acc : 0.35953283309936523 train_loss : 0.3727025609318312\n",
            "Acc : 0.3642676770687103 train_loss : 0.3720307960533179\n",
            "Acc : 0.3690025210380554 train_loss : 0.3703070895015439\n",
            "Acc : 0.3737373650074005 train_loss : 0.3686371917836368\n",
            "Acc : 0.3784722089767456 train_loss : 0.3671627700512792\n",
            "Acc : 0.38304924964904785 train_loss : 0.3709924127452257\n",
            "Acc : 0.3879419267177582 train_loss : 0.3681119975734906\n",
            "Acc : 0.39267677068710327 train_loss : 0.3676294365986472\n",
            "Acc : 0.39741161465644836 train_loss : 0.36565714662565907\n",
            "Acc : 0.4019886255264282 train_loss : 0.3674427368786446\n",
            "Acc : 0.40656566619873047 train_loss : 0.36808662587541274\n",
            "Acc : 0.4111426770687103 train_loss : 0.3695330773056908\n",
            "Acc : 0.4157196879386902 train_loss : 0.36960843381252184\n",
            "Acc : 0.4206123650074005 train_loss : 0.3674616059495343\n",
            "Acc : 0.4253472089767456 train_loss : 0.3674718433847794\n",
            "Acc : 0.4300820827484131 train_loss : 0.3659745468555585\n",
            "Acc : 0.43465909361839294 train_loss : 0.36813168251706707\n",
            "Acc : 0.43955177068710327 train_loss : 0.36596407289517685\n",
            "Acc : 0.4439709484577179 train_loss : 0.366401138195866\n",
            "Acc : 0.4487058222293854 train_loss : 0.3664016929299881\n",
            "Acc : 0.45328283309936523 train_loss : 0.3661999085668436\n",
            "Acc : 0.4580176770687103 train_loss : 0.3648543562359956\n",
            "Acc : 0.4627525210380554 train_loss : 0.3633231241444145\n",
            "Acc : 0.467803031206131 train_loss : 0.3603303150832653\n",
            "Acc : 0.4726957082748413 train_loss : 0.3590177108745764\n",
            "Acc : 0.4774305522441864 train_loss : 0.35899243226238325\n",
            "Acc : 0.48200756311416626 train_loss : 0.3585864910222952\n",
            "Acc : 0.48642677068710327 train_loss : 0.3603551442233416\n",
            "Acc : 0.49100378155708313 train_loss : 0.3598703421297528\n",
            "Acc : 0.4955808222293854 train_loss : 0.3615256820647222\n",
            "Acc : 0.49984216690063477 train_loss : 0.36440659822704635\n",
            "Acc : 0.5047348737716675 train_loss : 0.3624287962361618\n",
            "Acc : 0.5094696879386902 train_loss : 0.36233329171434453\n",
            "Acc : 0.5142045617103577 train_loss : 0.36135497201572764\n",
            "Acc : 0.5189393758773804 train_loss : 0.3613000332235216\n",
            "Acc : 0.5233585834503174 train_loss : 0.36198122932442595\n",
            "Acc : 0.5282512903213501 train_loss : 0.36118890437404666\n",
            "Acc : 0.533143937587738 train_loss : 0.35927112021467145\n",
            "Acc : 0.5378788113594055 train_loss : 0.3607433118250059\n",
            "Acc : 0.5426136255264282 train_loss : 0.359297057421043\n",
            "Acc : 0.5476641654968262 train_loss : 0.3567346120173605\n",
            "Acc : 0.5519254803657532 train_loss : 0.35754577180970526\n",
            "Acc : 0.5568181872367859 train_loss : 0.35684750157118844\n",
            "Acc : 0.5613952279090881 train_loss : 0.3573153798468411\n",
            "Acc : 0.5658143758773804 train_loss : 0.3568978220655406\n",
            "Acc : 0.5700757503509521 train_loss : 0.36050761378080143\n",
            "Acc : 0.5748106241226196 train_loss : 0.3611437508427515\n",
            "Acc : 0.5797032713890076 train_loss : 0.3596296402655782\n",
            "Acc : 0.5845959782600403 train_loss : 0.3577615843713284\n",
            "Acc : 0.5891729593276978 train_loss : 0.35808281041681767\n",
            "Acc : 0.59375 train_loss : 0.35823978363411635\n",
            "Acc : 0.5986426472663879 train_loss : 0.35739089813432656\n",
            "Acc : 0.603061854839325 train_loss : 0.35829732223421107\n",
            "Acc : 0.6079545617103577 train_loss : 0.35742455832660197\n",
            "Acc : 0.6126893758773804 train_loss : 0.35757380561860463\n",
            "Acc : 0.6171085834503174 train_loss : 0.359194585968825\n",
            "Acc : 0.6218434572219849 train_loss : 0.3597047395892161\n",
            "Acc : 0.6261047720909119 train_loss : 0.36299589718344494\n",
            "Acc : 0.6309974789619446 train_loss : 0.36136096072969615\n",
            "Acc : 0.6358901262283325 train_loss : 0.36069731104790287\n",
            "Acc : 0.6407828330993652 train_loss : 0.360306229442358\n",
            "Acc : 0.6455176472663879 train_loss : 0.3590994069919638\n",
            "Acc : 0.6504103541374207 train_loss : 0.35792832524120377\n",
            "Acc : 0.6551452279090881 train_loss : 0.35776560378393957\n",
            "Acc : 0.6592487096786499 train_loss : 0.3606861354981331\n",
            "Acc : 0.6638257503509521 train_loss : 0.36172450834911474\n",
            "Acc : 0.6685606241226196 train_loss : 0.3611949805977878\n",
            "Acc : 0.6732954382896423 train_loss : 0.3603754637297243\n",
            "Acc : 0.678188145160675 train_loss : 0.3596589696561468\n",
            "Acc : 0.6827651262283325 train_loss : 0.359660017750647\n",
            "Acc : 0.6878156661987305 train_loss : 0.3579514554011173\n",
            "Acc : 0.6923926472663879 train_loss : 0.35837679479673906\n",
            "Acc : 0.6972853541374207 train_loss : 0.357182010833249\n",
            "Acc : 0.7020202279090881 train_loss : 0.3571613309035699\n",
            "Acc : 0.7067550420761108 train_loss : 0.35695368952013007\n",
            "Acc : 0.7118055820465088 train_loss : 0.35504269107293923\n",
            "Acc : 0.7160668969154358 train_loss : 0.3559916278262154\n",
            "Acc : 0.7209596037864685 train_loss : 0.35488854646198936\n",
            "Acc : 0.7256944179534912 train_loss : 0.3545414425432682\n",
            "Acc : 0.7304292917251587 train_loss : 0.35439784086954135\n",
            "Acc : 0.7353219985961914 train_loss : 0.35360616091994723\n",
            "Acc : 0.7398989796638489 train_loss : 0.35359537174713007\n",
            "Acc : 0.7446338534355164 train_loss : 0.35363694526395706\n",
            "Acc : 0.7492108345031738 train_loss : 0.35388125001918525\n",
            "Acc : 0.7537878751754761 train_loss : 0.35411918725563873\n",
            "Acc : 0.7585227489471436 train_loss : 0.35324890063040787\n",
            "Acc : 0.7627840638160706 train_loss : 0.35480147213932195\n",
            "Acc : 0.7670454382896423 train_loss : 0.3562383286246076\n",
            "Acc : 0.771938145160675 train_loss : 0.3552085910556894\n",
            "Acc : 0.776830792427063 train_loss : 0.35405904941081284\n",
            "Acc : 0.7817234992980957 train_loss : 0.35313671200486\n",
            "Acc : 0.7864583134651184 train_loss : 0.35270820715509\n",
            "Acc : 0.7911931872367859 train_loss : 0.35153764783895225\n",
            "Acc : 0.7960858345031738 train_loss : 0.35034037615884756\n",
            "Acc : 0.8008207082748413 train_loss : 0.34976654876655305\n",
            "Acc : 0.8053977489471436 train_loss : 0.35116095717476553\n",
            "Acc : 0.8102903962135315 train_loss : 0.3506064270057775\n",
            "Acc : 0.8148674368858337 train_loss : 0.3511182585110267\n",
            "Acc : 0.8194444179534912 train_loss : 0.3507255368360451\n",
            "Acc : 0.8243371248245239 train_loss : 0.3500291373877024\n",
            "Acc : 0.8292297720909119 train_loss : 0.3496453052144603\n",
            "Acc : 0.8339646458625793 train_loss : 0.34971432695479204\n",
            "Acc : 0.8388572931289673 train_loss : 0.3493966642330146\n",
            "Acc : 0.84375 train_loss : 0.3485147345190247\n",
            "Acc : 0.8484848737716675 train_loss : 0.3476770984327925\n",
            "Acc : 0.8529040217399597 train_loss : 0.34842753948664273\n",
            "Acc : 0.8577967286109924 train_loss : 0.34740530139680115\n",
            "Acc : 0.8623737096786499 train_loss : 0.34754664578434563\n",
            "Acc : 0.8672664165496826 train_loss : 0.3470369636811115\n",
            "Acc : 0.8720012903213501 train_loss : 0.3466084572055968\n",
            "Acc : 0.8770517706871033 train_loss : 0.3453113428132738\n",
            "Acc : 0.8821022510528564 train_loss : 0.3440877794228653\n",
            "Acc : 0.8866792917251587 train_loss : 0.34482525365929756\n",
            "Acc : 0.8917297720909119 train_loss : 0.3432535335813698\n",
            "Acc : 0.8967803120613098 train_loss : 0.34162088508927385\n",
            "Acc : 0.9016729593276978 train_loss : 0.340823450727233\n",
            "Acc : 0.9064078330993652 train_loss : 0.34030768118914545\n",
            "Acc : 0.9113004803657532 train_loss : 0.34033067368892667\n",
            "Acc : 0.9160353541374207 train_loss : 0.34055505927938684\n",
            "Acc : 0.9209280014038086 train_loss : 0.3397870782040516\n",
            "Acc : 0.9255050420761108 train_loss : 0.34047611267660477\n",
            "Acc : 0.9299242496490479 train_loss : 0.34184866517104884\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.46533703804016113\n",
            "Acc 0.036616161465644836 train_loss 0.4502679705619812\n",
            "Acc 0.05681818351149559 train_loss 0.3101029520233472\n",
            "Acc 0.0763888880610466 train_loss 0.2842663247138262\n",
            "Acc 0.09532828629016876 train_loss 0.3120590642094612\n",
            "Acc 0.11363636702299118 train_loss 0.34622688715656597\n",
            "Acc 0.13005051016807556 train_loss 0.4107954725623131\n",
            "Acc 0.14898990094661713 train_loss 0.41153400857001543\n",
            "Acc 0.16856060922145844 train_loss 0.3987971718112628\n",
            "Acc 0.18686868250370026 train_loss 0.4122649647295475\n",
            "Acc 0.20454545319080353 train_loss 0.4526248662309213\n",
            "Acc 0.2222222238779068 train_loss 0.48963056318461895\n",
            "Acc 0.24053029716014862 train_loss 0.4800830099445123\n",
            "Acc 0.2594696879386902 train_loss 0.45943716753806385\n",
            "Acc 0.2790403962135315 train_loss 0.4389121726155281\n",
            "Acc 0.29671716690063477 train_loss 0.4556888076476753\n",
            "Acc 0.3162878751754761 train_loss 0.44275664450491176\n",
            "Acc 0.3345959484577179 train_loss 0.4468972240057256\n",
            "Acc 0.3541666567325592 train_loss 0.42794534838513326\n",
            "Acc 0.37436869740486145 train_loss 0.41105073429644107\n",
            "Acc 0.3945707082748413 train_loss 0.39266463617483777\n",
            "Acc 0.4141414165496826 train_loss 0.3951695717193864\n",
            "Acc 0.4330808222293854 train_loss 0.3984950398621352\n",
            "Acc 0.4513888955116272 train_loss 0.4011979755014181\n",
            "Acc 0.469696968793869 train_loss 0.40628447592258454\n",
            "Acc 0.4886363744735718 train_loss 0.41582890256093097\n",
            "Acc 0.5075757503509521 train_loss 0.41099406668433436\n",
            "Acc 0.5258838534355164 train_loss 0.41288696814860615\n",
            "Acc 0.5429292917251587 train_loss 0.4355510385899708\n",
            "Acc 0.560606062412262 train_loss 0.43748759776353835\n",
            "Acc 0.5789141654968262 train_loss 0.4410026164785508\n",
            "Acc 0.5984848737716675 train_loss 0.4315055557526648\n",
            "Acc 0.6180555820465088 train_loss 0.425060639778773\n",
            "Acc 0.6351010203361511 train_loss 0.43872446610647087\n",
            "Acc 0.6534090638160706 train_loss 0.44331913590431216\n",
            "Acc 0.6723484992980957 train_loss 0.43911675612131756\n",
            "Acc 0.689393937587738 train_loss 0.452278103377368\n",
            "Acc 0.7070707082748413 train_loss 0.45520000708730596\n",
            "Acc 0.7247474789619446 train_loss 0.46028962349280333\n",
            "Acc 0.7411616444587708 train_loss 0.47083715796470643\n",
            "Acc 0.7601010203361511 train_loss 0.47019329158271234\n",
            "Acc 0.7771464586257935 train_loss 0.4776352374326615\n",
            "Acc 0.7960858345031738 train_loss 0.47686004846595054\n",
            "Acc 0.8131313323974609 train_loss 0.48550103266130795\n",
            "Acc 0.8320707082748413 train_loss 0.4792294575108422\n",
            "Acc 0.8510100841522217 train_loss 0.4773941493552664\n",
            "Acc 0.868686854839325 train_loss 0.4745979962196756\n",
            "Acc 0.8876262903213501 train_loss 0.4710215013474226\n",
            "Acc 0.9065656661987305 train_loss 0.47210292731012615\n",
            "Acc 0.9154040217399597 train_loss 0.4763347798585892\n",
            "F1_score : 0.8473804100227791\n",
            "Epoch completed in 1.3923787117004394 minutes\n",
            "***************\n",
            "EPOCH: 4\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.602875292301178\n",
            "Acc : 0.009469697251915932 train_loss : 0.3730635568499565\n",
            "Acc : 0.014204545877873898 train_loss : 0.3482806334892909\n",
            "Acc : 0.018939394503831863 train_loss : 0.3161214217543602\n",
            "Acc : 0.02383207157254219 train_loss : 0.29961577951908114\n",
            "Acc : 0.028882576152682304 train_loss : 0.26256468271215755\n",
            "Acc : 0.033617425709962845 train_loss : 0.26287964305707384\n",
            "Acc : 0.03866792842745781 train_loss : 0.23614140134304762\n",
            "Acc : 0.04356060549616814 train_loss : 0.23214798751804563\n",
            "Acc : 0.04829545319080353 train_loss : 0.23862319216132163\n",
            "Acc : 0.05303030461072922 train_loss : 0.24383046342567963\n",
            "Acc : 0.05760732293128967 train_loss : 0.2570297885686159\n",
            "Acc : 0.06265782564878464 train_loss : 0.2425833808688017\n",
            "Acc : 0.06755050271749496 train_loss : 0.23404715902038983\n",
            "Acc : 0.07181186974048615 train_loss : 0.25342623541752496\n",
            "Acc : 0.07670454680919647 train_loss : 0.25513366563245654\n",
            "Acc : 0.08112373948097229 train_loss : 0.27195289161275416\n",
            "Acc : 0.08617424219846725 train_loss : 0.264177639865213\n",
            "Acc : 0.09090909361839294 train_loss : 0.27004144654462214\n",
            "Acc : 0.09564393758773804 train_loss : 0.27348425798118114\n",
            "Acc : 0.10053661465644836 train_loss : 0.27180950521003633\n",
            "Acc : 0.10527146607637405 train_loss : 0.2748870317908851\n",
            "Acc : 0.11000631004571915 train_loss : 0.27434933153183566\n",
            "Acc : 0.11474116146564484 train_loss : 0.2739904172097643\n",
            "Acc : 0.11963383853435516 train_loss : 0.2743793162703514\n",
            "Acc : 0.12421085685491562 train_loss : 0.28200211748480797\n",
            "Acc : 0.12815657258033752 train_loss : 0.3054718228953856\n",
            "Acc : 0.13304924964904785 train_loss : 0.30585029162466526\n",
            "Acc : 0.13731060922145844 train_loss : 0.31646824782264643\n",
            "Acc : 0.1418876200914383 train_loss : 0.31996887202064195\n",
            "Acc : 0.14599116146564484 train_loss : 0.3271484497574068\n",
            "Acc : 0.15072600543498993 train_loss : 0.33628625865094364\n",
            "Acc : 0.15546086430549622 train_loss : 0.3342689056739663\n",
            "Acc : 0.15988005697727203 train_loss : 0.34567540336181135\n",
            "Acc : 0.16477273404598236 train_loss : 0.341671773152692\n",
            "Acc : 0.16919191181659698 train_loss : 0.34360862088700134\n",
            "Acc : 0.17392677068710327 train_loss : 0.34192392290443985\n",
            "Acc : 0.17897726595401764 train_loss : 0.3389609305090026\n",
            "Acc : 0.1835542917251587 train_loss : 0.3383522711885281\n",
            "Acc : 0.18828913569450378 train_loss : 0.3404345503076911\n",
            "Acc : 0.1927083283662796 train_loss : 0.34211074851635026\n",
            "Acc : 0.19760100543498993 train_loss : 0.3387299518854845\n",
            "Acc : 0.20249368250370026 train_loss : 0.3389989316116932\n",
            "Acc : 0.20738635957241058 train_loss : 0.3369881116192449\n",
            "Acc : 0.2122790366411209 train_loss : 0.3351576214035352\n",
            "Acc : 0.21717171370983124 train_loss : 0.3326167342779429\n",
            "Acc : 0.2222222238779068 train_loss : 0.3285944977022232\n",
            "Acc : 0.22711490094661713 train_loss : 0.3256612269518276\n",
            "Acc : 0.23200757801532745 train_loss : 0.32326366661154493\n",
            "Acc : 0.23674242198467255 train_loss : 0.322938746958971\n",
            "Acc : 0.2413194477558136 train_loss : 0.3227234232951613\n",
            "Acc : 0.24573864042758942 train_loss : 0.32537234474260074\n",
            "Acc : 0.25031566619873047 train_loss : 0.32705797971981876\n",
            "Acc : 0.25505051016807556 train_loss : 0.3249717058130988\n",
            "Acc : 0.2596275210380554 train_loss : 0.3252976884896105\n",
            "Acc : 0.26452019810676575 train_loss : 0.32232176499175175\n",
            "Acc : 0.26925504207611084 train_loss : 0.32180048067841616\n",
            "Acc : 0.27414771914482117 train_loss : 0.3206955511765233\n",
            "Acc : 0.2790403962135315 train_loss : 0.3183151290325795\n",
            "Acc : 0.2837752401828766 train_loss : 0.31786099411547186\n",
            "Acc : 0.2886679172515869 train_loss : 0.3158644900703039\n",
            "Acc : 0.29324495792388916 train_loss : 0.316635852259013\n",
            "Acc : 0.2981376349925995 train_loss : 0.3147021757941397\n",
            "Acc : 0.30271464586257935 train_loss : 0.31534404295962304\n",
            "Acc : 0.30744948983192444 train_loss : 0.3179064997113668\n",
            "Acc : 0.31234216690063477 train_loss : 0.3158939111639153\n",
            "Acc : 0.3172348439693451 train_loss : 0.31285927244531575\n",
            "Acc : 0.3216540515422821 train_loss : 0.3183582816272974\n",
            "Acc : 0.3267045319080353 train_loss : 0.3154188746559447\n",
            "Acc : 0.3312815725803375 train_loss : 0.3191917936716761\n",
            "Acc : 0.3360164165496826 train_loss : 0.3206540579107446\n",
            "Acc : 0.3407512605190277 train_loss : 0.31986822332772946\n",
            "Acc : 0.34564393758773804 train_loss : 0.3184420499083114\n",
            "Acc : 0.35006314516067505 train_loss : 0.3231570664289835\n",
            "Acc : 0.3551136255264282 train_loss : 0.32074241995811464\n",
            "Acc : 0.3601641356945038 train_loss : 0.3178995428116698\n",
            "Acc : 0.36474114656448364 train_loss : 0.319112851248159\n",
            "Acc : 0.3694760203361511 train_loss : 0.3190803772363907\n",
            "Acc : 0.37436869740486145 train_loss : 0.3171713769058638\n",
            "Acc : 0.3792613744735718 train_loss : 0.31497035957872865\n",
            "Acc : 0.38399621844291687 train_loss : 0.3150490430402167\n",
            "Acc : 0.38857322931289673 train_loss : 0.3191186578535452\n",
            "Acc : 0.3936237394809723 train_loss : 0.31664677610598413\n",
            "Acc : 0.3983585834503174 train_loss : 0.31711268123416675\n",
            "Acc : 0.4030934274196625 train_loss : 0.31602370160467486\n",
            "Acc : 0.4076704680919647 train_loss : 0.3188805710091147\n",
            "Acc : 0.41256314516067505 train_loss : 0.3176707460962493\n",
            "Acc : 0.4169823229312897 train_loss : 0.3207942728291858\n",
            "Acc : 0.42203283309936523 train_loss : 0.3188938150245152\n",
            "Acc : 0.4270833432674408 train_loss : 0.31622085430555874\n",
            "Acc : 0.4318181872367859 train_loss : 0.31589221487660984\n",
            "Acc : 0.436553031206131 train_loss : 0.3151860476026069\n",
            "Acc : 0.4412878751754761 train_loss : 0.31737787228438163\n",
            "Acc : 0.4461805522441864 train_loss : 0.31545413039783216\n",
            "Acc : 0.45107322931289673 train_loss : 0.3145961908133406\n",
            "Acc : 0.4558080732822418 train_loss : 0.31496828817762434\n",
            "Acc : 0.4605429172515869 train_loss : 0.3145126481492495\n",
            "Acc : 0.46543559432029724 train_loss : 0.3133203515744939\n",
            "Acc : 0.47032827138900757 train_loss : 0.3119502694468306\n",
            "Acc : 0.47537878155708313 train_loss : 0.30937378857284786\n",
            "Acc : 0.48027145862579346 train_loss : 0.308242013880817\n",
            "Acc : 0.4848484992980957 train_loss : 0.30871654904502277\n",
            "Acc : 0.4895833432674408 train_loss : 0.3083079987142271\n",
            "Acc : 0.4943181872367859 train_loss : 0.3104546296840104\n",
            "Acc : 0.4992108643054962 train_loss : 0.309533352262917\n",
            "Acc : 0.5037878751754761 train_loss : 0.3115839477769046\n",
            "Acc : 0.5080492496490479 train_loss : 0.315338653695082\n",
            "Acc : 0.5129418969154358 train_loss : 0.31393164123787926\n",
            "Acc : 0.517518937587738 train_loss : 0.3152133090028522\n",
            "Acc : 0.5222538113594055 train_loss : 0.3152711080217903\n",
            "Acc : 0.5269886255264282 train_loss : 0.31477610139293716\n",
            "Acc : 0.5315656661987305 train_loss : 0.3162322667360838\n",
            "Acc : 0.5364583134651184 train_loss : 0.3155813708044259\n",
            "Acc : 0.5413510203361511 train_loss : 0.31419941498652887\n",
            "Acc : 0.5460858345031738 train_loss : 0.31383007197924284\n",
            "Acc : 0.5509785413742065 train_loss : 0.3122805502458379\n",
            "Acc : 0.5558711886405945 train_loss : 0.3105664249095652\n",
            "Acc : 0.560606062412262 train_loss : 0.3107276004186626\n",
            "Acc : 0.5656565427780151 train_loss : 0.3098075894184974\n",
            "Acc : 0.5702335834503174 train_loss : 0.31001013253505033\n",
            "Acc : 0.5751262903213501 train_loss : 0.3089177870491812\n",
            "Acc : 0.5797032713890076 train_loss : 0.31205790608999184\n",
            "Acc : 0.5845959782600403 train_loss : 0.3122147925924964\n",
            "Acc : 0.5896464586257935 train_loss : 0.3109072607551371\n",
            "Acc : 0.5946969985961914 train_loss : 0.3091694705784321\n",
            "Acc : 0.5992739796638489 train_loss : 0.30933321731549407\n",
            "Acc : 0.6040088534355164 train_loss : 0.30868379645577565\n",
            "Acc : 0.6089015007019043 train_loss : 0.3083869163237978\n",
            "Acc : 0.6134785413742065 train_loss : 0.30959710735798807\n",
            "Acc : 0.6183711886405945 train_loss : 0.3093841413179269\n",
            "Acc : 0.6232638955116272 train_loss : 0.30926371715564765\n",
            "Acc : 0.6278409361839294 train_loss : 0.3105260906176585\n",
            "Acc : 0.6327335834503174 train_loss : 0.3095136306395656\n",
            "Acc : 0.6369949579238892 train_loss : 0.3128556078994897\n",
            "Acc : 0.6420454382896423 train_loss : 0.31106434003622446\n",
            "Acc : 0.646938145160675 train_loss : 0.3105372408788432\n",
            "Acc : 0.651830792427063 train_loss : 0.31014880616843266\n",
            "Acc : 0.6567234992980957 train_loss : 0.30896158918630384\n",
            "Acc : 0.6617739796638489 train_loss : 0.3074187108670636\n",
            "Acc : 0.6661931872367859 train_loss : 0.30825603346207314\n",
            "Acc : 0.6704545617103577 train_loss : 0.3114897181537557\n",
            "Acc : 0.6750315427780151 train_loss : 0.31267123866144203\n",
            "Acc : 0.6799242496490479 train_loss : 0.3120482628318397\n",
            "Acc : 0.6848168969154358 train_loss : 0.31126894312910736\n",
            "Acc : 0.6897096037864685 train_loss : 0.31103269362757946\n",
            "Acc : 0.6946022510528564 train_loss : 0.31058877387581624\n",
            "Acc : 0.6994949579238892 train_loss : 0.3098274509669567\n",
            "Acc : 0.7040719985961914 train_loss : 0.30972101838906874\n",
            "Acc : 0.7089646458625793 train_loss : 0.30883624787198616\n",
            "Acc : 0.7138572931289673 train_loss : 0.30865973092615606\n",
            "Acc : 0.71875 train_loss : 0.3082907491715144\n",
            "Acc : 0.7236426472663879 train_loss : 0.30687383579482375\n",
            "Acc : 0.7283775210380554 train_loss : 0.3068181024803445\n",
            "Acc : 0.7332702279090881 train_loss : 0.3060491687630291\n",
            "Acc : 0.7381628751754761 train_loss : 0.3056931654051427\n",
            "Acc : 0.7428977489471436 train_loss : 0.3057967872143938\n",
            "Acc : 0.7477903962135315 train_loss : 0.3052931086984789\n",
            "Acc : 0.7523674368858337 train_loss : 0.3054289096094008\n",
            "Acc : 0.7571022510528564 train_loss : 0.3056355059474894\n",
            "Acc : 0.7618371248245239 train_loss : 0.30573885447811333\n",
            "Acc : 0.7665719985961914 train_loss : 0.30586777939837173\n",
            "Acc : 0.7716224789619446 train_loss : 0.304730779065946\n",
            "Acc : 0.7760416865348816 train_loss : 0.3057361603743094\n",
            "Acc : 0.7804608345031738 train_loss : 0.30718197403248493\n",
            "Acc : 0.7855113744735718 train_loss : 0.3058648439293558\n",
            "Acc : 0.7904040217399597 train_loss : 0.30517911715769624\n",
            "Acc : 0.7952967286109924 train_loss : 0.3045323536826108\n",
            "Acc : 0.8001893758773804 train_loss : 0.3043648383491451\n",
            "Acc : 0.8050820827484131 train_loss : 0.3032338387117936\n",
            "Acc : 0.8101325631141663 train_loss : 0.3018702327109435\n",
            "Acc : 0.8148674368858337 train_loss : 0.3017989277447525\n",
            "Acc : 0.8194444179534912 train_loss : 0.30329611193561967\n",
            "Acc : 0.8240214586257935 train_loss : 0.303077244926567\n",
            "Acc : 0.8287563323974609 train_loss : 0.3033509649036602\n",
            "Acc : 0.8334911465644836 train_loss : 0.3030913716341768\n",
            "Acc : 0.8385416865348816 train_loss : 0.3022217992481522\n",
            "Acc : 0.8432765007019043 train_loss : 0.3017752177087264\n",
            "Acc : 0.848169207572937 train_loss : 0.301259317989932\n",
            "Acc : 0.853061854839325 train_loss : 0.3008214863610334\n",
            "Acc : 0.8581123948097229 train_loss : 0.299696982672645\n",
            "Acc : 0.8628472089767456 train_loss : 0.2993204935520723\n",
            "Acc : 0.8674242496490479 train_loss : 0.2999052909090296\n",
            "Acc : 0.872474730014801 train_loss : 0.2986952794990579\n",
            "Acc : 0.8770517706871033 train_loss : 0.2989807004797394\n",
            "Acc : 0.8817866444587708 train_loss : 0.29875300049379067\n",
            "Acc : 0.8865214586257935 train_loss : 0.29864801905087884\n",
            "Acc : 0.8915719985961914 train_loss : 0.29749930448790285\n",
            "Acc : 0.8966224789619446 train_loss : 0.2963783653215208\n",
            "Acc : 0.9011995196342468 train_loss : 0.29722212966551226\n",
            "Acc : 0.90625 train_loss : 0.2958332825844225\n",
            "Acc : 0.9113004803657532 train_loss : 0.29439399672272315\n",
            "Acc : 0.9160353541374207 train_loss : 0.2941209734029447\n",
            "Acc : 0.9209280014038086 train_loss : 0.2935867665113563\n",
            "Acc : 0.9259785413742065 train_loss : 0.29249245953774944\n",
            "Acc : 0.9308711886405945 train_loss : 0.2933455523390036\n",
            "Acc : 0.9357638955116272 train_loss : 0.2928045993784861\n",
            "Acc : 0.9403409361839294 train_loss : 0.29376497119665146\n",
            "Acc : 0.9447600841522217 train_loss : 0.29453676032147025\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.6804409623146057\n",
            "Acc 0.03724747523665428 train_loss 0.6156460344791412\n",
            "Acc 0.05744949355721474 train_loss 0.4215082215766112\n",
            "Acc 0.0763888880610466 train_loss 0.37594932224601507\n",
            "Acc 0.09595959633588791 train_loss 0.3923775009810925\n",
            "Acc 0.11300504952669144 train_loss 0.423589442546169\n",
            "Acc 0.12941919267177582 train_loss 0.49685295511569294\n",
            "Acc 0.14835858345031738 train_loss 0.49308401672169566\n",
            "Acc 0.1666666716337204 train_loss 0.4784572948184278\n",
            "Acc 0.18560606241226196 train_loss 0.4868783500045538\n",
            "Acc 0.20391413569450378 train_loss 0.5339664894748818\n",
            "Acc 0.22159090638160706 train_loss 0.5657551260665059\n",
            "Acc 0.23989899456501007 train_loss 0.5619369444365685\n",
            "Acc 0.2594696879386902 train_loss 0.5338899863085577\n",
            "Acc 0.27840909361839294 train_loss 0.5144285437961419\n",
            "Acc 0.2960858643054962 train_loss 0.528168219840154\n",
            "Acc 0.3150252401828766 train_loss 0.5163845435223159\n",
            "Acc 0.33270201086997986 train_loss 0.5193976675056748\n",
            "Acc 0.35227271914482117 train_loss 0.4970035747085747\n",
            "Acc 0.3724747598171234 train_loss 0.4756488660350442\n",
            "Acc 0.39267677068710327 train_loss 0.4538467725118001\n",
            "Acc 0.4122474789619446 train_loss 0.453220470385118\n",
            "Acc 0.4318181872367859 train_loss 0.4535041710604792\n",
            "Acc 0.44949495792388916 train_loss 0.45354512085517246\n",
            "Acc 0.46717172861099243 train_loss 0.462262008190155\n",
            "Acc 0.4848484992980957 train_loss 0.4733143471754514\n",
            "Acc 0.504419207572937 train_loss 0.4663569993442959\n",
            "Acc 0.5227272510528564 train_loss 0.4676543836082731\n",
            "Acc 0.5404040217399597 train_loss 0.49030584507975083\n",
            "Acc 0.5587121248245239 train_loss 0.4895141412814458\n",
            "Acc 0.5763888955116272 train_loss 0.4932941400235699\n",
            "Acc 0.5959596037864685 train_loss 0.48357454035431147\n",
            "Acc 0.6148989796638489 train_loss 0.47663505149610114\n",
            "Acc 0.6319444179534912 train_loss 0.49035224143196554\n",
            "Acc 0.6502525210380554 train_loss 0.49391004358019147\n",
            "Acc 0.6691918969154358 train_loss 0.4915894427233272\n",
            "Acc 0.6862373948097229 train_loss 0.5063911668352179\n",
            "Acc 0.7039141654968262 train_loss 0.5115843125079808\n",
            "Acc 0.7203282713890076 train_loss 0.5186299429490016\n",
            "Acc 0.7367424368858337 train_loss 0.5312503285706043\n",
            "Acc 0.7556818127632141 train_loss 0.5307188099477349\n",
            "Acc 0.7727272510528564 train_loss 0.5393730096873783\n",
            "Acc 0.7910353541374207 train_loss 0.5396640889866408\n",
            "Acc 0.808080792427063 train_loss 0.5463979413563554\n",
            "Acc 0.8270202279090881 train_loss 0.5391804416974385\n",
            "Acc 0.8446969985961914 train_loss 0.5388846565847811\n",
            "Acc 0.8630050420761108 train_loss 0.5343441557376942\n",
            "Acc 0.8819444179534912 train_loss 0.5305831183989843\n",
            "Acc 0.9015151262283325 train_loss 0.5302672361841008\n",
            "Acc 0.9097222089767456 train_loss 0.5378580904006958\n",
            "F1_score : 0.8295589988081049\n",
            "Epoch completed in 1.395912778377533 minutes\n",
            "***************\n",
            "EPOCH: 5\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.37273865938186646\n",
            "Acc : 0.009627525694668293 train_loss : 0.23369748145341873\n",
            "Acc : 0.014520201832056046 train_loss : 0.20897541443506876\n",
            "Acc : 0.019255051389336586 train_loss : 0.23554128408432007\n",
            "Acc : 0.02383207157254219 train_loss : 0.2688490688800812\n",
            "Acc : 0.028882576152682304 train_loss : 0.24414252986510596\n",
            "Acc : 0.03377525135874748 train_loss : 0.22786642823900496\n",
            "Acc : 0.038825757801532745 train_loss : 0.20386230573058128\n",
            "Acc : 0.04387626424431801 train_loss : 0.18899966445234087\n",
            "Acc : 0.04876893758773804 train_loss : 0.19325933009386062\n",
            "Acc : 0.05350378900766373 train_loss : 0.19667366688901727\n",
            "Acc : 0.05823863670229912 train_loss : 0.2046272779504458\n",
            "Acc : 0.06313131004571915 train_loss : 0.19701917412189338\n",
            "Acc : 0.06802398711442947 train_loss : 0.1913870216480323\n",
            "Acc : 0.07244317978620529 train_loss : 0.2177662874261538\n",
            "Acc : 0.07733585685491562 train_loss : 0.21949133137241006\n",
            "Acc : 0.08143939077854156 train_loss : 0.24625292464214213\n",
            "Acc : 0.08601641654968262 train_loss : 0.24620099572671783\n",
            "Acc : 0.09075126051902771 train_loss : 0.2529397469602133\n",
            "Acc : 0.0954861119389534 train_loss : 0.2530442658811808\n",
            "Acc : 0.10037878900766373 train_loss : 0.25064271227234886\n",
            "Acc : 0.10511363297700882 train_loss : 0.24824583902955055\n",
            "Acc : 0.10984848439693451 train_loss : 0.24773818934741226\n",
            "Acc : 0.11442550271749496 train_loss : 0.250683075748384\n",
            "Acc : 0.11900252848863602 train_loss : 0.2586484661698341\n",
            "Acc : 0.12357954680919647 train_loss : 0.2647784332243296\n",
            "Acc : 0.1279987394809723 train_loss : 0.2843670119289999\n",
            "Acc : 0.13257575035095215 train_loss : 0.2885470233325447\n",
            "Acc : 0.13731060922145844 train_loss : 0.2944951234706517\n",
            "Acc : 0.14204545319080353 train_loss : 0.29398942018548646\n",
            "Acc : 0.14662247896194458 train_loss : 0.2962627547883218\n",
            "Acc : 0.15072600543498993 train_loss : 0.31324296654202044\n",
            "Acc : 0.15561868250370026 train_loss : 0.30878468467430636\n",
            "Acc : 0.16003787517547607 train_loss : 0.31905707475893635\n",
            "Acc : 0.16508838534355164 train_loss : 0.3122170974101339\n",
            "Acc : 0.1696654111146927 train_loss : 0.31418837006721234\n",
            "Acc : 0.17455807328224182 train_loss : 0.31231784478232666\n",
            "Acc : 0.17960858345031738 train_loss : 0.3076066082637561\n",
            "Acc : 0.18418560922145844 train_loss : 0.30813914976823026\n",
            "Acc : 0.18892045319080353 train_loss : 0.30849672239273784\n",
            "Acc : 0.1931818127632141 train_loss : 0.3131396637820616\n",
            "Acc : 0.19807448983192444 train_loss : 0.30923002763163476\n",
            "Acc : 0.203125 train_loss : 0.3068510583320329\n",
            "Acc : 0.20817551016807556 train_loss : 0.3030961790884083\n",
            "Acc : 0.2130681872367859 train_loss : 0.2999937628706296\n",
            "Acc : 0.21796086430549622 train_loss : 0.29795739459602727\n",
            "Acc : 0.2226957082748413 train_loss : 0.2958303545700743\n",
            "Acc : 0.22758838534355164 train_loss : 0.29385191031421226\n",
            "Acc : 0.23248106241226196 train_loss : 0.29140075752321554\n",
            "Acc : 0.23721590638160706 train_loss : 0.29109366729855535\n",
            "Acc : 0.24210858345031738 train_loss : 0.29108744406817005\n",
            "Acc : 0.2465277761220932 train_loss : 0.29503219646330064\n",
            "Acc : 0.25110480189323425 train_loss : 0.29601195243731987\n",
            "Acc : 0.2561553120613098 train_loss : 0.293533476828425\n",
            "Acc : 0.26104798913002014 train_loss : 0.2907799849456007\n",
            "Acc : 0.26594066619873047 train_loss : 0.28846624134374516\n",
            "Acc : 0.27067551016807556 train_loss : 0.2887100385207879\n",
            "Acc : 0.2755681872367859 train_loss : 0.2870730088445647\n",
            "Acc : 0.2804608643054962 train_loss : 0.28449922332824285\n",
            "Acc : 0.28535354137420654 train_loss : 0.2841451728095611\n",
            "Acc : 0.2904040515422821 train_loss : 0.2813107859892923\n",
            "Acc : 0.29529672861099243 train_loss : 0.281740645247121\n",
            "Acc : 0.30018940567970276 train_loss : 0.2807920624812444\n",
            "Acc : 0.30492424964904785 train_loss : 0.2797368033789098\n",
            "Acc : 0.30965909361839294 train_loss : 0.2805963328251472\n",
            "Acc : 0.31455177068710327 train_loss : 0.2790629423477433\n",
            "Acc : 0.31960228085517883 train_loss : 0.27605110317913456\n",
            "Acc : 0.3243371248245239 train_loss : 0.2783585620277068\n",
            "Acc : 0.32922980189323425 train_loss : 0.27628906373528467\n",
            "Acc : 0.3338068127632141 train_loss : 0.2771271375673158\n",
            "Acc : 0.33869948983192444 train_loss : 0.27742524654932427\n",
            "Acc : 0.34359216690063477 train_loss : 0.2770983607818683\n",
            "Acc : 0.3480113744735718 train_loss : 0.27730088711601414\n",
            "Acc : 0.35274621844291687 train_loss : 0.27890379626203227\n",
            "Acc : 0.35779672861099243 train_loss : 0.2763329463203748\n",
            "Acc : 0.3628472089767456 train_loss : 0.2744443784222791\n",
            "Acc : 0.3675820827484131 train_loss : 0.2746181444494755\n",
            "Acc : 0.3723169267177582 train_loss : 0.2750657100517016\n",
            "Acc : 0.3772096037864685 train_loss : 0.2726466286031506\n",
            "Acc : 0.38226011395454407 train_loss : 0.2697822252288461\n",
            "Acc : 0.38699495792388916 train_loss : 0.2699626170926624\n",
            "Acc : 0.391571968793869 train_loss : 0.27555245760737396\n",
            "Acc : 0.3966224789619446 train_loss : 0.27303558103291387\n",
            "Acc : 0.4013573229312897 train_loss : 0.2742676208061831\n",
            "Acc : 0.40640783309936523 train_loss : 0.2722187332370702\n",
            "Acc : 0.41130051016807556 train_loss : 0.27172356558053995\n",
            "Acc : 0.4161931872367859 train_loss : 0.27055219989055873\n",
            "Acc : 0.42077019810676575 train_loss : 0.27319332999600604\n",
            "Acc : 0.42550504207611084 train_loss : 0.2726221932621484\n",
            "Acc : 0.4305555522441864 train_loss : 0.2705333350433244\n",
            "Acc : 0.4352903962135315 train_loss : 0.2699537693144201\n",
            "Acc : 0.4400252401828766 train_loss : 0.26933479357672774\n",
            "Acc : 0.44476011395454407 train_loss : 0.27249245089228435\n",
            "Acc : 0.44981059432029724 train_loss : 0.270883838388514\n",
            "Acc : 0.45470327138900757 train_loss : 0.26973415252409483\n",
            "Acc : 0.4592803120613098 train_loss : 0.2706258345084886\n",
            "Acc : 0.4640151560306549 train_loss : 0.26952602184310404\n",
            "Acc : 0.46890783309936523 train_loss : 0.26834513231807827\n",
            "Acc : 0.47380051016807556 train_loss : 0.26696418030093416\n",
            "Acc : 0.4788510203361511 train_loss : 0.2647348777577281\n",
            "Acc : 0.48374369740486145 train_loss : 0.2636325388482892\n",
            "Acc : 0.48847854137420654 train_loss : 0.2645209919223014\n",
            "Acc : 0.49337121844291687 train_loss : 0.26336908033027234\n",
            "Acc : 0.49810606241226196 train_loss : 0.2655585864917017\n",
            "Acc : 0.5029987096786499 train_loss : 0.2642826143829595\n",
            "Acc : 0.5074179172515869 train_loss : 0.26900014010662177\n",
            "Acc : 0.5116792917251587 train_loss : 0.2724724084347765\n",
            "Acc : 0.5165719985961914 train_loss : 0.2709491465723625\n",
            "Acc : 0.5213068127632141 train_loss : 0.2703308589209657\n",
            "Acc : 0.5263572931289673 train_loss : 0.26885822351005945\n",
            "Acc : 0.53125 train_loss : 0.2683189847246484\n",
            "Acc : 0.5359848737716675 train_loss : 0.26961797213048805\n",
            "Acc : 0.5408775210380554 train_loss : 0.2697097255922524\n",
            "Acc : 0.5459280014038086 train_loss : 0.26798300703235883\n",
            "Acc : 0.5506628751754761 train_loss : 0.26885606254572453\n",
            "Acc : 0.5557133555412292 train_loss : 0.2669805160177679\n",
            "Acc : 0.5607638955116272 train_loss : 0.26514031623418516\n",
            "Acc : 0.5653409361839294 train_loss : 0.26600489809604017\n",
            "Acc : 0.5703914165496826 train_loss : 0.26485864611483423\n",
            "Acc : 0.5751262903213501 train_loss : 0.26558790386964876\n",
            "Acc : 0.5801767706871033 train_loss : 0.2640954700629573\n",
            "Acc : 0.5849116444587708 train_loss : 0.26708621678293726\n",
            "Acc : 0.5896464586257935 train_loss : 0.2676690873818669\n",
            "Acc : 0.5945391654968262 train_loss : 0.26749702014269366\n",
            "Acc : 0.5994318127632141 train_loss : 0.266542307972908\n",
            "Acc : 0.6041666865348816 train_loss : 0.26641519521436996\n",
            "Acc : 0.6089015007019043 train_loss : 0.2663806816489678\n",
            "Acc : 0.613794207572937 train_loss : 0.26593518909066916\n",
            "Acc : 0.6183711886405945 train_loss : 0.26643536234086795\n",
            "Acc : 0.6232638955116272 train_loss : 0.26610122781533463\n",
            "Acc : 0.6281565427780151 train_loss : 0.2657391662133559\n",
            "Acc : 0.6327335834503174 train_loss : 0.2671315066064849\n",
            "Acc : 0.6377840638160706 train_loss : 0.26629285099811123\n",
            "Acc : 0.642518937587738 train_loss : 0.26830803302686607\n",
            "Acc : 0.6475694179534912 train_loss : 0.26671875201993517\n",
            "Acc : 0.6524621248245239 train_loss : 0.266577991328257\n",
            "Acc : 0.6573547720909119 train_loss : 0.26669519818829795\n",
            "Acc : 0.6622474789619446 train_loss : 0.2660131012292012\n",
            "Acc : 0.6672979593276978 train_loss : 0.2645352961455318\n",
            "Acc : 0.6720328330993652 train_loss : 0.263856633699366\n",
            "Acc : 0.6761363744735718 train_loss : 0.2676229794503104\n",
            "Acc : 0.6808711886405945 train_loss : 0.26891716165651736\n",
            "Acc : 0.6859217286109924 train_loss : 0.26789040920826107\n",
            "Acc : 0.6908143758773804 train_loss : 0.26747900573536754\n",
            "Acc : 0.6957070827484131 train_loss : 0.26771030338673757\n",
            "Acc : 0.700599730014801 train_loss : 0.26734258760124036\n",
            "Acc : 0.7054924368858337 train_loss : 0.266411259922446\n",
            "Acc : 0.7100694179534912 train_loss : 0.26643871500886773\n",
            "Acc : 0.7149621248245239 train_loss : 0.2657526965009286\n",
            "Acc : 0.7196969985961914 train_loss : 0.2659051062166691\n",
            "Acc : 0.7244318127632141 train_loss : 0.2664771265048065\n",
            "Acc : 0.7294822931289673 train_loss : 0.26510385938576964\n",
            "Acc : 0.734375 train_loss : 0.2650436872458146\n",
            "Acc : 0.7394254803657532 train_loss : 0.2641195399904406\n",
            "Acc : 0.7443181872367859 train_loss : 0.2636287345520912\n",
            "Acc : 0.7490530014038086 train_loss : 0.26395425014197826\n",
            "Acc : 0.7537878751754761 train_loss : 0.2644901000864946\n",
            "Acc : 0.7583649158477783 train_loss : 0.2644988668587389\n",
            "Acc : 0.7632575631141663 train_loss : 0.2647354279018048\n",
            "Acc : 0.7679924368858337 train_loss : 0.2645782785024494\n",
            "Acc : 0.7727272510528564 train_loss : 0.2645259173297734\n",
            "Acc : 0.7776199579238892 train_loss : 0.2639730403912656\n",
            "Acc : 0.7823547720909119 train_loss : 0.2641601968853752\n",
            "Acc : 0.7869318127632141 train_loss : 0.2651186037081771\n",
            "Acc : 0.7919822931289673 train_loss : 0.26409480526591794\n",
            "Acc : 0.796875 train_loss : 0.26310995324368935\n",
            "Acc : 0.8017676472663879 train_loss : 0.26244547289467146\n",
            "Acc : 0.8066603541374207 train_loss : 0.2623544997491297\n",
            "Acc : 0.8115530014038086 train_loss : 0.26164757045944764\n",
            "Acc : 0.8164457082748413 train_loss : 0.26083181763396546\n",
            "Acc : 0.8213383555412292 train_loss : 0.2607616245398047\n",
            "Acc : 0.826231062412262 train_loss : 0.26099811979504517\n",
            "Acc : 0.8311237096786499 train_loss : 0.26033950153458324\n",
            "Acc : 0.8358585834503174 train_loss : 0.2603488942158633\n",
            "Acc : 0.8407512903213501 train_loss : 0.259805030141558\n",
            "Acc : 0.8458017706871033 train_loss : 0.2590783021666787\n",
            "Acc : 0.8505366444587708 train_loss : 0.25917328346920554\n",
            "Acc : 0.8554292917251587 train_loss : 0.25854505280430395\n",
            "Acc : 0.8604797720909119 train_loss : 0.2580456495618021\n",
            "Acc : 0.8652146458625793 train_loss : 0.25777249799834356\n",
            "Acc : 0.8699495196342468 train_loss : 0.2573963573295108\n",
            "Acc : 0.8745265007019043 train_loss : 0.25794184060542136\n",
            "Acc : 0.8795770406723022 train_loss : 0.2571177875083652\n",
            "Acc : 0.884311854839325 train_loss : 0.25740807228114293\n",
            "Acc : 0.8892045617103577 train_loss : 0.25714522777376947\n",
            "Acc : 0.8937815427780151 train_loss : 0.25700895784683125\n",
            "Acc : 0.8988320827484131 train_loss : 0.25600277863562426\n",
            "Acc : 0.9038825631141663 train_loss : 0.2550843768218096\n",
            "Acc : 0.9084596037864685 train_loss : 0.2561437395751161\n",
            "Acc : 0.9135100841522217 train_loss : 0.25499491871971836\n",
            "Acc : 0.9185606241226196 train_loss : 0.2537689670744835\n",
            "Acc : 0.9234532713890076 train_loss : 0.25347583483865793\n",
            "Acc : 0.9283459782600403 train_loss : 0.2527897869347754\n",
            "Acc : 0.9333964586257935 train_loss : 0.2518075848113477\n",
            "Acc : 0.9384469985961914 train_loss : 0.2509626439271065\n",
            "Acc : 0.9433396458625793 train_loss : 0.25033872311326616\n",
            "Acc : 0.9480745196342468 train_loss : 0.25070934605590883\n",
            "Acc : 0.9526515007019043 train_loss : 0.2512925605411933\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.6769858002662659\n",
            "Acc 0.036616161465644836 train_loss 0.6313264071941376\n",
            "Acc 0.05681818351149559 train_loss 0.4325820629795392\n",
            "Acc 0.07386363297700882 train_loss 0.4623626936227083\n",
            "Acc 0.09280303120613098 train_loss 0.4690145328640938\n",
            "Acc 0.10984848439693451 train_loss 0.5054289363324642\n",
            "Acc 0.1262626200914383 train_loss 0.586146990103381\n",
            "Acc 0.14520202577114105 train_loss 0.5776422815397382\n",
            "Acc 0.16287878155708313 train_loss 0.5759724486205313\n",
            "Acc 0.1818181872367859 train_loss 0.5779854334890843\n",
            "Acc 0.2001262605190277 train_loss 0.6219607083634897\n",
            "Acc 0.21780303120613098 train_loss 0.6576250822593769\n",
            "Acc 0.23674242198467255 train_loss 0.6521238805009768\n",
            "Acc 0.2556818127632141 train_loss 0.6239857891840594\n",
            "Acc 0.27462121844291687 train_loss 0.607284377515316\n",
            "Acc 0.29229798913002014 train_loss 0.6159147867001593\n",
            "Acc 0.3112373650074005 train_loss 0.6017024801934466\n",
            "Acc 0.3295454680919647 train_loss 0.6036930551959409\n",
            "Acc 0.34911614656448364 train_loss 0.5780088383900491\n",
            "Acc 0.3693181872367859 train_loss 0.5527831960469485\n",
            "Acc 0.38952019810676575 train_loss 0.5276362012539592\n",
            "Acc 0.40909090638160706 train_loss 0.5240567187693986\n",
            "Acc 0.42866161465644836 train_loss 0.5218412963592488\n",
            "Acc 0.4457070827484131 train_loss 0.5239004526908199\n",
            "Acc 0.46338382363319397 train_loss 0.533206168115139\n",
            "Acc 0.48106059432029724 train_loss 0.5465352131197085\n",
            "Acc 0.5006313323974609 train_loss 0.5372881820356404\n",
            "Acc 0.5189393758773804 train_loss 0.5375491995364428\n",
            "Acc 0.5366161465644836 train_loss 0.5575056957273647\n",
            "Acc 0.5542929172515869 train_loss 0.5548986929158369\n",
            "Acc 0.5719696879386902 train_loss 0.5613699129992916\n",
            "Acc 0.5915403962135315 train_loss 0.5523929640185088\n",
            "Acc 0.6104797720909119 train_loss 0.5436148794763016\n",
            "Acc 0.626893937587738 train_loss 0.5585506214376759\n",
            "Validating....\n",
            "Acc 0.6452020406723022 train_loss 0.5612732467906816\n",
            "Acc 0.6641414165496826 train_loss 0.5587995306899151\n",
            "Acc 0.681186854839325 train_loss 0.5735162518717147\n",
            "Acc 0.6988636255264282 train_loss 0.57724793549431\n",
            "Acc 0.7152777910232544 train_loss 0.5851034772319671\n",
            "Acc 0.7310606241226196 train_loss 0.601948999427259\n",
            "Acc 0.7493686676025391 train_loss 0.6013858062101574\n",
            "Acc 0.7664141654968262 train_loss 0.6110816773559365\n",
            "Acc 0.7847222089767456 train_loss 0.6128238666542741\n",
            "Acc 0.8017676472663879 train_loss 0.6197875756770372\n",
            "Acc 0.8207070827484131 train_loss 0.6117855037252108\n",
            "Acc 0.8383838534355164 train_loss 0.6104396832701953\n",
            "Acc 0.8566918969154358 train_loss 0.6051818936112079\n",
            "Acc 0.875 train_loss 0.602201905567199\n",
            "Acc 0.8939393758773804 train_loss 0.6019124794675379\n",
            "Acc 0.9021464586257935 train_loss 0.6121489767730236\n",
            "F1_score : 0.812121212121212\n",
            "Epoch completed in 1.394418485959371 minutes\n",
            "***************\n",
            "EPOCH: 6\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.4026743173599243\n",
            "Acc : 0.009469697251915932 train_loss : 0.34955698251724243\n",
            "Acc : 0.014362373389303684 train_loss : 0.2887535293896993\n",
            "Acc : 0.019255051389336586 train_loss : 0.24637955985963345\n",
            "Acc : 0.024147726595401764 train_loss : 0.22017990052700043\n",
            "Acc : 0.02904040366411209 train_loss : 0.20610103011131287\n",
            "Acc : 0.03393308073282242 train_loss : 0.19930910425526754\n",
            "Acc : 0.03898358717560768 train_loss : 0.17784866713918746\n",
            "Acc : 0.044034089893102646 train_loss : 0.16463038635750613\n",
            "Acc : 0.04892676696181297 train_loss : 0.16410565208643674\n",
            "Acc : 0.053661614656448364 train_loss : 0.16676674359901386\n",
            "Acc : 0.058396466076374054 train_loss : 0.17206124076619744\n",
            "Acc : 0.06344696879386902 train_loss : 0.16278823680029467\n",
            "Acc : 0.06833964586257935 train_loss : 0.15838428839508975\n",
            "Acc : 0.0729166641831398 train_loss : 0.18518858291208745\n",
            "Acc : 0.07780934125185013 train_loss : 0.18341255595441908\n",
            "Acc : 0.08238636702299118 train_loss : 0.20153880853425055\n",
            "Acc : 0.08743686974048615 train_loss : 0.19227486693610749\n",
            "Acc : 0.09217171370983124 train_loss : 0.20545229325561146\n",
            "Acc : 0.09690656512975693 train_loss : 0.21346919843927026\n",
            "Acc : 0.10179924219846725 train_loss : 0.2085091309355838\n",
            "Acc : 0.10669191926717758 train_loss : 0.2031737363643267\n",
            "Acc : 0.11158459633588791 train_loss : 0.20164851917196874\n",
            "Acc : 0.1163194477558136 train_loss : 0.20069732959382236\n",
            "Acc : 0.12121212482452393 train_loss : 0.19891556032001972\n",
            "Acc : 0.12578913569450378 train_loss : 0.21244751726492092\n",
            "Acc : 0.13036616146564484 train_loss : 0.22295013849657994\n",
            "Acc : 0.1349431872367859 train_loss : 0.22774122662043997\n",
            "Acc : 0.13967803120613098 train_loss : 0.23092846013605595\n",
            "Acc : 0.1445707082748413 train_loss : 0.2298055922612548\n",
            "Acc : 0.1493055522441864 train_loss : 0.23218372037574167\n",
            "Acc : 0.15372474491596222 train_loss : 0.24592512898379937\n",
            "Acc : 0.15877525508403778 train_loss : 0.2400957029312849\n",
            "Acc : 0.1631944477558136 train_loss : 0.25856246399309707\n",
            "Acc : 0.16824494302272797 train_loss : 0.25269220508635043\n",
            "Acc : 0.1731376200914383 train_loss : 0.25090570328757167\n",
            "Acc : 0.17803029716014862 train_loss : 0.24938104800074487\n",
            "Acc : 0.18308080732822418 train_loss : 0.24499198867890395\n",
            "Acc : 0.18781565129756927 train_loss : 0.2459527139002696\n",
            "Acc : 0.1927083283662796 train_loss : 0.24579889024607837\n",
            "Acc : 0.19712752103805542 train_loss : 0.25090551735242694\n",
            "Acc : 0.20202019810676575 train_loss : 0.24935383948364429\n",
            "Acc : 0.2070707082748413 train_loss : 0.24627427457896775\n",
            "Acc : 0.21196338534355164 train_loss : 0.24373924372379074\n",
            "Acc : 0.21685606241226196 train_loss : 0.24094532243907452\n",
            "Acc : 0.22159090638160706 train_loss : 0.24031482227956472\n",
            "Acc : 0.22664141654968262 train_loss : 0.23693358593006084\n",
            "Acc : 0.23153409361839294 train_loss : 0.23542020202148706\n",
            "Acc : 0.23642677068710327 train_loss : 0.23442503425995914\n",
            "Acc : 0.2413194477558136 train_loss : 0.23209066715091467\n",
            "Acc : 0.2460542917251587 train_loss : 0.2332791332155466\n",
            "Acc : 0.250946968793869 train_loss : 0.23510033146549875\n",
            "Acc : 0.2556818127632141 train_loss : 0.23679708053340326\n",
            "Acc : 0.2607323229312897 train_loss : 0.23446436926584552\n",
            "Acc : 0.265625 train_loss : 0.23238742077215152\n",
            "Acc : 0.2705176770687103 train_loss : 0.23057213874666818\n",
            "Acc : 0.2752525210380554 train_loss : 0.23071497701631302\n",
            "Acc : 0.28014519810676575 train_loss : 0.22981779865021335\n",
            "Acc : 0.2851957082748413 train_loss : 0.2272224721451432\n",
            "Acc : 0.29008838534355164 train_loss : 0.22826276989653707\n",
            "Acc : 0.29498106241226196 train_loss : 0.22720515163096247\n",
            "Acc : 0.2998737394809723 train_loss : 0.22907509699824355\n",
            "Acc : 0.3047664165496826 train_loss : 0.22768586232430404\n",
            "Acc : 0.3098169267177582 train_loss : 0.22692999683204107\n",
            "Acc : 0.3147096037864685 train_loss : 0.22730888194189622\n",
            "Acc : 0.31960228085517883 train_loss : 0.22562879783976258\n",
            "Acc : 0.32449495792388916 train_loss : 0.2237161329822309\n",
            "Acc : 0.3293876349925995 train_loss : 0.2267411551438272\n",
            "Acc : 0.33443814516067505 train_loss : 0.22461538381226684\n",
            "Acc : 0.3390151560306549 train_loss : 0.22645123598298855\n",
            "Acc : 0.34375 train_loss : 0.22852126181020704\n",
            "Acc : 0.3486426770687103 train_loss : 0.2287620543760972\n",
            "Acc : 0.3536931872367859 train_loss : 0.22705802227025979\n",
            "Acc : 0.358428031206131 train_loss : 0.22886387226045937\n",
            "Acc : 0.36347854137420654 train_loss : 0.22673431235055128\n",
            "Acc : 0.3685290515422821 train_loss : 0.22482631245235862\n",
            "Acc : 0.37310606241226196 train_loss : 0.2266059297971524\n",
            "Acc : 0.3779987394809723 train_loss : 0.22758986674344692\n",
            "Acc : 0.38304924964904785 train_loss : 0.22577127095170413\n",
            "Acc : 0.3880997598171234 train_loss : 0.2232931026024744\n",
            "Acc : 0.3928346037864685 train_loss : 0.22340145106944773\n",
            "Acc : 0.3975694477558136 train_loss : 0.22878015429780976\n",
            "Acc : 0.40261995792388916 train_loss : 0.22632998351501413\n",
            "Acc : 0.40735480189323425 train_loss : 0.22852150543725916\n",
            "Acc : 0.4124053120613098 train_loss : 0.22631036040975766\n",
            "Acc : 0.4174558222293854 train_loss : 0.22453683439295652\n",
            "Acc : 0.4223484992980957 train_loss : 0.22375908988560067\n",
            "Acc : 0.42692551016807556 train_loss : 0.22755771720866588\n",
            "Acc : 0.4319760203361511 train_loss : 0.22611951047366255\n",
            "Acc : 0.4370265007019043 train_loss : 0.2239032101093067\n",
            "Acc : 0.4419191777706146 train_loss : 0.22354245560427943\n",
            "Acc : 0.44681185483932495 train_loss : 0.2224026398287843\n",
            "Acc : 0.4513888955116272 train_loss : 0.2262899803338192\n",
            "Acc : 0.45643940567970276 train_loss : 0.2246033255050474\n",
            "Acc : 0.4613320827484131 train_loss : 0.2236119659911645\n",
            "Acc : 0.4660669267177582 train_loss : 0.2245570561499335\n",
            "Acc : 0.47111743688583374 train_loss : 0.2231857721914643\n",
            "Acc : 0.4761679172515869 train_loss : 0.2218962134413269\n",
            "Acc : 0.4812184274196625 train_loss : 0.22010943848602096\n",
            "Acc : 0.48626893758773804 train_loss : 0.21822010789066554\n",
            "Acc : 0.49116161465644836 train_loss : 0.2174233356398521\n",
            "Acc : 0.4960542917251587 train_loss : 0.21745916430418397\n",
            "Acc : 0.5009469985961914 train_loss : 0.21755209800253794\n",
            "Acc : 0.5056818127632141 train_loss : 0.22050779402399293\n",
            "Acc : 0.5101010203361511 train_loss : 0.22335469187015577\n",
            "Acc : 0.5146780014038086 train_loss : 0.22519138065289776\n",
            "Acc : 0.5189393758773804 train_loss : 0.228641063288272\n",
            "Acc : 0.5238320827484131 train_loss : 0.2279622940070651\n",
            "Acc : 0.528724730014801 train_loss : 0.22738832402803483\n",
            "Acc : 0.533775269985199 train_loss : 0.22610285312614659\n",
            "Acc : 0.5385100841522217 train_loss : 0.22542918161363215\n",
            "Acc : 0.5432449579238892 train_loss : 0.2264969288038888\n",
            "Acc : 0.5481376051902771 train_loss : 0.2264066271615767\n",
            "Acc : 0.553188145160675 train_loss : 0.22507283049063725\n",
            "Acc : 0.5579229593276978 train_loss : 0.224753114030413\n",
            "Acc : 0.5629734992980957 train_loss : 0.2231626034682167\n",
            "Acc : 0.5680239796638489 train_loss : 0.22151857262684238\n",
            "Acc : 0.5729166865348816 train_loss : 0.22168618457991693\n",
            "Acc : 0.5779671669006348 train_loss : 0.22054187989472843\n",
            "Acc : 0.5828598737716675 train_loss : 0.2209043073002249\n",
            "Acc : 0.5879103541374207 train_loss : 0.21963044568465268\n",
            "Acc : 0.5926452279090881 train_loss : 0.22280926475698343\n",
            "Acc : 0.5975378751754761 train_loss : 0.22334477388701304\n",
            "Acc : 0.6025883555412292 train_loss : 0.22182264204527583\n",
            "Acc : 0.6076388955116272 train_loss : 0.22073436637222768\n",
            "Acc : 0.6125315427780151 train_loss : 0.22012353069075044\n",
            "Acc : 0.6174242496490479 train_loss : 0.220283639785929\n",
            "Acc : 0.6223168969154358 train_loss : 0.2198408635595115\n",
            "Acc : 0.6272096037864685 train_loss : 0.2195539861022271\n",
            "Acc : 0.6321022510528564 train_loss : 0.21907658538279626\n",
            "Acc : 0.6368371248245239 train_loss : 0.21938332558417592\n",
            "Acc : 0.6415719985961914 train_loss : 0.22059875485402616\n",
            "Acc : 0.6464646458625793 train_loss : 0.22002541208009524\n",
            "Acc : 0.6510416865348816 train_loss : 0.2229409030596934\n",
            "Acc : 0.6560921669006348 train_loss : 0.2219460938953691\n",
            "Acc : 0.6608270406723022 train_loss : 0.22235551795593517\n",
            "Acc : 0.6657196879386902 train_loss : 0.22266359867895172\n",
            "Acc : 0.6707702279090881 train_loss : 0.22155197419604097\n",
            "Acc : 0.6755050420761108 train_loss : 0.2212707305045651\n",
            "Acc : 0.6803977489471436 train_loss : 0.2210503393118935\n",
            "Acc : 0.6846590638160706 train_loss : 0.22446252671169473\n",
            "Acc : 0.689393937587738 train_loss : 0.22672039712451295\n",
            "Acc : 0.6942866444587708 train_loss : 0.2261476633562283\n",
            "Acc : 0.6990214586257935 train_loss : 0.2261800472236549\n",
            "Acc : 0.7039141654968262 train_loss : 0.22619119626951628\n",
            "Acc : 0.7088068127632141 train_loss : 0.22592748723260753\n",
            "Acc : 0.7138572931289673 train_loss : 0.22474643842754316\n",
            "Acc : 0.71875 train_loss : 0.22422419016476017\n",
            "Acc : 0.7236426472663879 train_loss : 0.22393811140334446\n",
            "Acc : 0.7285353541374207 train_loss : 0.22401112249741953\n",
            "Acc : 0.7334280014038086 train_loss : 0.2242117752228549\n",
            "Acc : 0.7384785413742065 train_loss : 0.22298754336263396\n",
            "Acc : 0.7432133555412292 train_loss : 0.22297807053656749\n",
            "Acc : 0.748106062412262 train_loss : 0.2222297467992871\n",
            "Acc : 0.7529987096786499 train_loss : 0.22239817434981946\n",
            "Acc : 0.7577335834503174 train_loss : 0.22295579310649863\n",
            "Acc : 0.7626262903213501 train_loss : 0.2228394909672866\n",
            "Acc : 0.7673611044883728 train_loss : 0.22249974731261593\n",
            "Acc : 0.7722538113594055 train_loss : 0.22258988900238988\n",
            "Acc : 0.7773042917251587 train_loss : 0.2217667336226441\n",
            "Acc : 0.7821969985961914 train_loss : 0.22166839886387313\n",
            "Acc : 0.7870896458625793 train_loss : 0.22100019644669913\n",
            "Acc : 0.7919822931289673 train_loss : 0.22091595048050575\n",
            "Acc : 0.7965593338012695 train_loss : 0.22113474037089362\n",
            "Acc : 0.8016098737716675 train_loss : 0.2203395592105208\n",
            "Acc : 0.8065025210380554 train_loss : 0.21962300783404745\n",
            "Acc : 0.8113952279090881 train_loss : 0.21896664625379497\n",
            "Acc : 0.8162878751754761 train_loss : 0.21907732552582665\n",
            "Acc : 0.8211805820465088 train_loss : 0.21854600068953264\n",
            "Acc : 0.826231062412262 train_loss : 0.21755918311503003\n",
            "Acc : 0.8309659361839294 train_loss : 0.2183590264152191\n",
            "Acc : 0.8357007503509521 train_loss : 0.21942105520040142\n",
            "Acc : 0.8407512903213501 train_loss : 0.2186846103777603\n",
            "Acc : 0.845643937587738 train_loss : 0.21832230750300075\n",
            "Acc : 0.8505366444587708 train_loss : 0.21755081056484155\n",
            "Acc : 0.8555871248245239 train_loss : 0.21673032852016727\n",
            "Acc : 0.8604797720909119 train_loss : 0.21641542622556456\n",
            "Acc : 0.8652146458625793 train_loss : 0.21653638578156073\n",
            "Acc : 0.8701072931289673 train_loss : 0.21600264505425978\n",
            "Acc : 0.8751578330993652 train_loss : 0.21505045949791868\n",
            "Acc : 0.8800504803657532 train_loss : 0.21433869145234316\n",
            "Acc : 0.8847853541374207 train_loss : 0.21504818616373525\n",
            "Acc : 0.8898358345031738 train_loss : 0.21414600761576755\n",
            "Acc : 0.8947285413742065 train_loss : 0.21464606245939175\n",
            "Acc : 0.8996211886405945 train_loss : 0.21469970523706963\n",
            "Acc : 0.9041982293128967 train_loss : 0.2151969679360909\n",
            "Acc : 0.9090909361839294 train_loss : 0.21467614840257931\n",
            "Acc : 0.9141414165496826 train_loss : 0.2138596824766632\n",
            "Acc : 0.9187184572219849 train_loss : 0.21499234005296355\n",
            "Acc : 0.923768937587738 train_loss : 0.21406902307154316\n",
            "Acc : 0.9288194179534912 train_loss : 0.21304169460077874\n",
            "Acc : 0.9337121248245239 train_loss : 0.21257819086895324\n",
            "Acc : 0.9386047720909119 train_loss : 0.2119175129198503\n",
            "Acc : 0.9436553120613098 train_loss : 0.21116941033372866\n",
            "Acc : 0.948705792427063 train_loss : 0.21046704461750312\n",
            "Acc : 0.9537563323974609 train_loss : 0.21010052354778258\n",
            "Acc : 0.9584911465644836 train_loss : 0.21020198430922735\n",
            "Acc : 0.9632260203361511 train_loss : 0.21035614063154268\n",
            "Validating....\n",
            "Acc 0.017045455053448677 train_loss 0.7777582406997681\n",
            "Acc 0.03598484769463539 train_loss 0.6832567453384399\n",
            "Acc 0.056186869740486145 train_loss 0.4754674608508746\n",
            "Acc 0.07386363297700882 train_loss 0.4776793848723173\n",
            "Acc 0.09343434125185013 train_loss 0.47884062975645064\n",
            "Acc 0.1111111119389534 train_loss 0.5279205578068892\n",
            "Acc 0.12752525508403778 train_loss 0.6213863864541054\n",
            "Acc 0.1458333283662796 train_loss 0.6045056348666549\n",
            "Acc 0.16351009905338287 train_loss 0.6014245740241475\n",
            "Acc 0.18244948983192444 train_loss 0.6030347041785717\n",
            "Acc 0.20075757801532745 train_loss 0.6436904282732443\n",
            "Acc 0.21780303120613098 train_loss 0.6764809532711903\n",
            "Acc 0.2361111044883728 train_loss 0.6678517776039931\n",
            "Acc 0.25505051016807556 train_loss 0.6346258015504905\n",
            "Acc 0.27398988604545593 train_loss 0.6178776388367017\n",
            "Acc 0.2916666567325592 train_loss 0.6273049530573189\n",
            "Acc 0.31060606241226196 train_loss 0.6155631651773172\n",
            "Acc 0.3289141356945038 train_loss 0.6162037754224406\n",
            "Acc 0.34911614656448364 train_loss 0.5861370512529424\n",
            "Acc 0.36868685483932495 train_loss 0.5606892913579941\n",
            "Acc 0.3888888955116272 train_loss 0.5349577475516569\n",
            "Acc 0.4084596037864685 train_loss 0.5307171001014385\n",
            "Acc 0.4273989796638489 train_loss 0.5291526571887991\n",
            "Acc 0.4444444477558136 train_loss 0.5315690110437572\n",
            "Acc 0.4627525210380554 train_loss 0.5446170063316822\n",
            "Acc 0.4804292917251587 train_loss 0.5586785463472972\n",
            "Acc 0.49936869740486145 train_loss 0.5503699929901847\n",
            "Acc 0.5176767706871033 train_loss 0.5503697753218668\n",
            "Acc 0.5353535413742065 train_loss 0.5674498673399975\n",
            "Acc 0.5530303120613098 train_loss 0.5641865885506073\n",
            "Acc 0.5700757503509521 train_loss 0.5688942282670929\n",
            "Acc 0.5896464586257935 train_loss 0.5596797807374969\n",
            "Acc 0.6092171669006348 train_loss 0.5495113194214575\n",
            "Acc 0.6256313323974609 train_loss 0.5672447003205033\n",
            "Acc 0.6439393758773804 train_loss 0.570851530879736\n",
            "Acc 0.6628788113594055 train_loss 0.5695318882871006\n",
            "Acc 0.6799242496490479 train_loss 0.5854375335211689\n",
            "Acc 0.6976010203361511 train_loss 0.5915137706814628\n",
            "Acc 0.7146464586257935 train_loss 0.5984093261261781\n",
            "Acc 0.7310606241226196 train_loss 0.6141372655518353\n",
            "Acc 0.7493686676025391 train_loss 0.6151228682478753\n",
            "Acc 0.7670454382896423 train_loss 0.6219249655980439\n",
            "Acc 0.7853535413742065 train_loss 0.6232599939197995\n",
            "Acc 0.8030303120613098 train_loss 0.6271785691549833\n",
            "Acc 0.8219696879386902 train_loss 0.6203385330736637\n",
            "Acc 0.8390151262283325 train_loss 0.6222534183736729\n",
            "Acc 0.8579545617103577 train_loss 0.6178494232924695\n",
            "Acc 0.8762626051902771 train_loss 0.6161897391236076\n",
            "Acc 0.8945707082748413 train_loss 0.6171767210625873\n",
            "Acc 0.9027777910232544 train_loss 0.6282380018383265\n",
            "F1_score : 0.8098765432098766\n",
            "Epoch completed in 1.3938557465871175 minutes\n",
            "***************\n",
            "EPOCH: 7\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004734848625957966 train_loss : 0.37243348360061646\n",
            "Acc : 0.009627525694668293 train_loss : 0.23164737597107887\n",
            "Acc : 0.014362373389303684 train_loss : 0.2060685083270073\n",
            "Acc : 0.019255051389336586 train_loss : 0.18102875724434853\n",
            "Acc : 0.024147726595401764 train_loss : 0.17569872736930847\n",
            "Acc : 0.02872474677860737 train_loss : 0.19634826481342316\n",
            "Acc : 0.03377525135874748 train_loss : 0.18216629219906671\n",
            "Acc : 0.038825757801532745 train_loss : 0.16777650453150272\n",
            "Acc : 0.04371843487024307 train_loss : 0.1625755743847953\n",
            "Acc : 0.04876893758773804 train_loss : 0.15491234585642816\n",
            "Acc : 0.0538194440305233 train_loss : 0.1450674913146279\n",
            "Acc : 0.05886995047330856 train_loss : 0.14175107205907503\n",
            "Acc : 0.06392045319080353 train_loss : 0.13285236719709176\n",
            "Acc : 0.06897095590829849 train_loss : 0.1257444710603782\n",
            "Acc : 0.07354798167943954 train_loss : 0.15330712050199508\n",
            "Acc : 0.07844065874814987 train_loss : 0.15125110419467092\n",
            "Acc : 0.08301767706871033 train_loss : 0.17264624597395167\n",
            "Acc : 0.08806817978620529 train_loss : 0.16492825146350595\n",
            "Acc : 0.09280303120613098 train_loss : 0.1804484025036034\n",
            "Acc : 0.09753787517547607 train_loss : 0.19426781591027975\n",
            "Acc : 0.10258838534355164 train_loss : 0.18755708918685005\n",
            "Acc : 0.1076388880610466 train_loss : 0.17999675844542004\n",
            "Acc : 0.11205808073282242 train_loss : 0.1908683813298526\n",
            "Acc : 0.11695075780153275 train_loss : 0.19208327781719467\n",
            "Acc : 0.12168560922145844 train_loss : 0.19665279127657415\n",
            "Acc : 0.12610480189323425 train_loss : 0.2168552942144183\n",
            "Acc : 0.1306818127632141 train_loss : 0.23007987363746873\n",
            "Acc : 0.13573232293128967 train_loss : 0.22354534966871142\n",
            "Acc : 0.140625 train_loss : 0.2202516157693904\n",
            "Acc : 0.1453598439693451 train_loss : 0.2195953657850623\n",
            "Acc : 0.15025252103805542 train_loss : 0.22185835440553003\n",
            "Acc : 0.15514519810676575 train_loss : 0.2300152630195953\n",
            "Acc : 0.1601957082748413 train_loss : 0.22579795489031257\n",
            "Acc : 0.16461490094661713 train_loss : 0.24129811460700104\n",
            "Acc : 0.1696654111146927 train_loss : 0.23565820439585616\n",
            "Acc : 0.17455807328224182 train_loss : 0.2344855540432036\n",
            "Acc : 0.17945075035095215 train_loss : 0.2324115213610836\n",
            "Acc : 0.18418560922145844 train_loss : 0.23051587752017536\n",
            "Acc : 0.18892045319080353 train_loss : 0.232230261207009\n",
            "Acc : 0.19365529716014862 train_loss : 0.2364254264626652\n",
            "Acc : 0.1983901560306549 train_loss : 0.23836304197405897\n",
            "Acc : 0.20328283309936523 train_loss : 0.23555819960754543\n",
            "Acc : 0.2083333283662796 train_loss : 0.23318415278092372\n",
            "Acc : 0.21291035413742065 train_loss : 0.2326435841704634\n",
            "Acc : 0.21796086430549622 train_loss : 0.2290887448936701\n",
            "Acc : 0.22285354137420654 train_loss : 0.2278538644718735\n",
            "Acc : 0.2279040366411209 train_loss : 0.2244277788682821\n",
            "Acc : 0.23279671370983124 train_loss : 0.2221597652339066\n",
            "Acc : 0.23768939077854156 train_loss : 0.21965195409649488\n",
            "Acc : 0.2425820678472519 train_loss : 0.21814069706946612\n",
            "Acc : 0.24747474491596222 train_loss : 0.21857003885887416\n",
            "Acc : 0.2522096037864685 train_loss : 0.22077552103795683\n",
            "Acc : 0.2569444477558136 train_loss : 0.22178419795379323\n",
            "Acc : 0.26199495792388916 train_loss : 0.21910493727773428\n",
            "Acc : 0.2670454680919647 train_loss : 0.2162836052138697\n",
            "Acc : 0.27193814516067505 train_loss : 0.21430748784249382\n",
            "Acc : 0.27667298913002014 train_loss : 0.21421815928790652\n",
            "Acc : 0.2817234992980957 train_loss : 0.21276275148807927\n",
            "Acc : 0.28661614656448364 train_loss : 0.21235851012170315\n",
            "Acc : 0.29103535413742065 train_loss : 0.21581895801549156\n",
            "Acc : 0.2960858643054962 train_loss : 0.2140722998158365\n",
            "Acc : 0.3006628751754761 train_loss : 0.21601414683485223\n",
            "Acc : 0.3055555522441864 train_loss : 0.21570665214861195\n",
            "Acc : 0.31060606241226196 train_loss : 0.21542485200916417\n",
            "Acc : 0.31534090638160706 train_loss : 0.21425969099196104\n",
            "Acc : 0.3203914165496826 train_loss : 0.21182148341992588\n",
            "Acc : 0.3254419267177582 train_loss : 0.20963997174221188\n",
            "Acc : 0.3303346037864685 train_loss : 0.21205206075683236\n",
            "Acc : 0.33522728085517883 train_loss : 0.21070817558337812\n",
            "Acc : 0.3398042917251587 train_loss : 0.21205461932612316\n",
            "Acc : 0.3445391356945038 train_loss : 0.21393407831414485\n",
            "Acc : 0.3492739796638489 train_loss : 0.21562629454355273\n",
            "Acc : 0.35432448983192444 train_loss : 0.21475561057561882\n",
            "Acc : 0.359375 train_loss : 0.21472415642661824\n",
            "Acc : 0.3642676770687103 train_loss : 0.21641353401045005\n",
            "Acc : 0.36916035413742065 train_loss : 0.21525116081006432\n",
            "Acc : 0.374053031206131 train_loss : 0.21568878123222232\n",
            "Acc : 0.3787878751754761 train_loss : 0.21691368589512047\n",
            "Acc : 0.38383838534355164 train_loss : 0.21503358270642878\n",
            "Acc : 0.3888888955116272 train_loss : 0.21283682666253298\n",
            "Acc : 0.3937815725803375 train_loss : 0.21178689605935855\n",
            "Acc : 0.3985164165496826 train_loss : 0.21746397425034425\n",
            "Acc : 0.4035669267177582 train_loss : 0.215353960485523\n",
            "Acc : 0.40830177068710327 train_loss : 0.21633627099384153\n",
            "Acc : 0.41335228085517883 train_loss : 0.21459626312203267\n",
            "Acc : 0.41824495792388916 train_loss : 0.2142233505715118\n",
            "Acc : 0.4232954680919647 train_loss : 0.21232811213824257\n",
            "Acc : 0.4278724789619446 train_loss : 0.21519975853152573\n",
            "Acc : 0.43292298913002014 train_loss : 0.21362786897029098\n",
            "Acc : 0.43781566619873047 train_loss : 0.2127337836350004\n",
            "Acc : 0.4427083432674408 train_loss : 0.21219367507312978\n",
            "Acc : 0.4476010203361511 train_loss : 0.21075880841311553\n",
            "Acc : 0.4523358643054962 train_loss : 0.21363496257653158\n",
            "Acc : 0.4573863744735718 train_loss : 0.2118104545478808\n",
            "Acc : 0.46243685483932495 train_loss : 0.21055796050319547\n",
            "Acc : 0.46717172861099243 train_loss : 0.21216757562554753\n",
            "Acc : 0.47206440567970276 train_loss : 0.2119329109135055\n",
            "Acc : 0.47711488604545593 train_loss : 0.21023669069138717\n",
            "Acc : 0.48200756311416626 train_loss : 0.2100414044289577\n",
            "Acc : 0.4870580732822418 train_loss : 0.20812372164800763\n",
            "Acc : 0.49195075035095215 train_loss : 0.20815399704076867\n",
            "Acc : 0.4968434274196625 train_loss : 0.20792091698111856\n",
            "Acc : 0.501893937587738 train_loss : 0.2066847190297055\n",
            "Acc : 0.5066288113594055 train_loss : 0.2092289357410314\n",
            "Acc : 0.5116792917251587 train_loss : 0.2076433137591396\n",
            "Acc : 0.5162563323974609 train_loss : 0.209315186243434\n",
            "Acc : 0.5206754803657532 train_loss : 0.21340338187727415\n",
            "Acc : 0.5257260203361511 train_loss : 0.21164386876410357\n",
            "Acc : 0.5304608345031738 train_loss : 0.21166981515739489\n",
            "Acc : 0.5353535413742065 train_loss : 0.2106994927437468\n",
            "Acc : 0.5404040217399597 train_loss : 0.2093465976115014\n",
            "Acc : 0.5451388955116272 train_loss : 0.21032423827065422\n",
            "Acc : 0.5500315427780151 train_loss : 0.2105234834505657\n",
            "Acc : 0.5550820827484131 train_loss : 0.20901276156502335\n",
            "Acc : 0.559974730014801 train_loss : 0.20860623015981652\n",
            "Acc : 0.565025269985199 train_loss : 0.20757130101515814\n",
            "Acc : 0.5700757503509521 train_loss : 0.2059726196094456\n",
            "Acc : 0.5749684572219849 train_loss : 0.2054298260580685\n",
            "Acc : 0.580018937587738 train_loss : 0.20421998866465912\n",
            "Acc : 0.5849116444587708 train_loss : 0.2047439619898796\n",
            "Acc : 0.5898042917251587 train_loss : 0.203981173494138\n",
            "Acc : 0.5945391654968262 train_loss : 0.2075568555442036\n",
            "Acc : 0.5994318127632141 train_loss : 0.20793815290297918\n",
            "Acc : 0.6044822931289673 train_loss : 0.20662048879650333\n",
            "Acc : 0.6095328330993652 train_loss : 0.205298326253891\n",
            "Acc : 0.6144254803657532 train_loss : 0.2046149397889773\n",
            "Acc : 0.6193181872367859 train_loss : 0.20439180825638958\n",
            "Acc : 0.6242108345031738 train_loss : 0.20382717670872808\n",
            "Acc : 0.6291035413742065 train_loss : 0.20366131289060727\n",
            "Acc : 0.6339961886405945 train_loss : 0.20291504836999452\n",
            "Acc : 0.638731062412262 train_loss : 0.2031050218653133\n",
            "Acc : 0.6434659361839294 train_loss : 0.2044693695550615\n",
            "Acc : 0.6485164165496826 train_loss : 0.20334851918251892\n",
            "Acc : 0.6532512903213501 train_loss : 0.2062992640117656\n",
            "Acc : 0.6583017706871033 train_loss : 0.204991824855959\n",
            "Acc : 0.6631944179534912 train_loss : 0.20501128582776906\n",
            "Acc : 0.6680871248245239 train_loss : 0.20534866807615235\n",
            "Acc : 0.6731376051902771 train_loss : 0.2044000859250841\n",
            "Acc : 0.678188145160675 train_loss : 0.20320887366483967\n",
            "Acc : 0.6832386255264282 train_loss : 0.20241450993344187\n",
            "Acc : 0.6876578330993652 train_loss : 0.20504661090672016\n",
            "Acc : 0.6922348737716675 train_loss : 0.20617029056425246\n",
            "Acc : 0.6972853541374207 train_loss : 0.20545221548657733\n",
            "Acc : 0.7023358345031738 train_loss : 0.2047561813896108\n",
            "Acc : 0.7072285413742065 train_loss : 0.20487438297734178\n",
            "Acc : 0.7122790217399597 train_loss : 0.20410703312707681\n",
            "Acc : 0.7173295617103577 train_loss : 0.20307877339219965\n",
            "Acc : 0.7223800420761108 train_loss : 0.20220726369754286\n",
            "Acc : 0.7272727489471436 train_loss : 0.2023272307702159\n",
            "Acc : 0.7321653962135315 train_loss : 0.20255900505930186\n",
            "Acc : 0.7370581030845642 train_loss : 0.20287829424125076\n",
            "Acc : 0.7421085834503174 train_loss : 0.20177738977547147\n",
            "Acc : 0.7470012903213501 train_loss : 0.20183835915149817\n",
            "Acc : 0.7520517706871033 train_loss : 0.20097484760131543\n",
            "Acc : 0.7569444179534912 train_loss : 0.20114604860784546\n",
            "Acc : 0.7616792917251587 train_loss : 0.20182187598532972\n",
            "Acc : 0.7665719985961914 train_loss : 0.20171783846702165\n",
            "Acc : 0.7714646458625793 train_loss : 0.20135621086376\n",
            "Acc : 0.7763572931289673 train_loss : 0.20171034637159146\n",
            "Acc : 0.7810921669006348 train_loss : 0.20125433738576248\n",
            "Acc : 0.7859848737716675 train_loss : 0.2009878363228918\n",
            "Acc : 0.7908775210380554 train_loss : 0.20038797243003859\n",
            "Acc : 0.7957702279090881 train_loss : 0.20050777080097448\n",
            "Acc : 0.8005050420761108 train_loss : 0.20102950472884426\n",
            "Acc : 0.8055555820465088 train_loss : 0.2000836647030982\n",
            "Acc : 0.8104482293128967 train_loss : 0.1994235926386283\n",
            "Acc : 0.8154987096786499 train_loss : 0.19888600070185647\n",
            "Acc : 0.8202335834503174 train_loss : 0.1988700839934782\n",
            "Acc : 0.8252840638160706 train_loss : 0.19847796968674872\n",
            "Acc : 0.8303346037864685 train_loss : 0.1976460213718169\n",
            "Acc : 0.8353850841522217 train_loss : 0.19693342710665443\n",
            "Acc : 0.8404356241226196 train_loss : 0.19670324648059037\n",
            "Acc : 0.8454861044883728 train_loss : 0.19592807796175424\n",
            "Acc : 0.8502209782600403 train_loss : 0.19580367915118221\n",
            "Acc : 0.8551136255264282 train_loss : 0.19539520724543502\n",
            "Acc : 0.8601641654968262 train_loss : 0.19503039808478206\n",
            "Acc : 0.8650568127632141 train_loss : 0.19479990343294912\n",
            "Acc : 0.8699495196342468 train_loss : 0.1945363933722792\n",
            "Acc : 0.875 train_loss : 0.1937468502379163\n",
            "Acc : 0.8800504803657532 train_loss : 0.19287412504975995\n",
            "Acc : 0.8851010203361511 train_loss : 0.1921312272733911\n",
            "Acc : 0.8899936676025391 train_loss : 0.19272970257756802\n",
            "Acc : 0.895044207572937 train_loss : 0.19194647540526638\n",
            "Acc : 0.899936854839325 train_loss : 0.19223884315958814\n",
            "Acc : 0.9048295617103577 train_loss : 0.19218904393146166\n",
            "Acc : 0.9094065427780151 train_loss : 0.19279991828345805\n",
            "Acc : 0.9144570827484131 train_loss : 0.19203150042277925\n",
            "Acc : 0.9195075631141663 train_loss : 0.19130987246976572\n",
            "Acc : 0.9239267706871033 train_loss : 0.19296317473645247\n",
            "Acc : 0.9289772510528564 train_loss : 0.1920627803296635\n",
            "Acc : 0.9340277910232544 train_loss : 0.19112520602517102\n",
            "Acc : 0.9389204382896423 train_loss : 0.19084019745544842\n",
            "Acc : 0.9439709782600403 train_loss : 0.19020039387471935\n",
            "Acc : 0.9490214586257935 train_loss : 0.18936037783962242\n",
            "Acc : 0.9540719985961914 train_loss : 0.18866233490407466\n",
            "Acc : 0.9591224789619446 train_loss : 0.18801111466612438\n",
            "Acc : 0.9636995196342468 train_loss : 0.18939303153851614\n",
            "Acc : 0.9684343338012695 train_loss : 0.1900338682280195\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.5917189121246338\n",
            "Acc 0.036616161465644836 train_loss 0.6142615675926208\n",
            "Acc 0.05681818351149559 train_loss 0.42532818267742795\n",
            "Acc 0.07386363297700882 train_loss 0.4366184528917074\n",
            "Acc 0.09343434125185013 train_loss 0.4397893652319908\n",
            "Acc 0.1111111119389534 train_loss 0.5073461818198363\n",
            "Acc 0.12752525508403778 train_loss 0.5942823620779174\n",
            "Acc 0.14646464586257935 train_loss 0.5699524534866214\n",
            "Acc 0.1654040366411209 train_loss 0.5579361344377199\n",
            "Acc 0.18434342741966248 train_loss 0.5678341530263424\n",
            "Acc 0.20202019810676575 train_loss 0.6084527664563872\n",
            "Acc 0.21969696879386902 train_loss 0.6403984694431225\n",
            "Acc 0.23863635957241058 train_loss 0.6163110154179426\n",
            "Acc 0.25757575035095215 train_loss 0.5893934001880032\n",
            "Acc 0.2765151560306549 train_loss 0.5660587921738625\n",
            "Acc 0.2941919267177582 train_loss 0.5838752328418195\n",
            "Acc 0.31313130259513855 train_loss 0.5652079630423995\n",
            "Acc 0.3308080732822418 train_loss 0.5724086244073179\n",
            "Acc 0.35101011395454407 train_loss 0.5434947829497488\n",
            "Acc 0.3712121248245239 train_loss 0.5191047038882971\n",
            "Acc 0.3914141356945038 train_loss 0.49502717215745223\n",
            "Acc 0.4109848439693451 train_loss 0.49144924813034857\n",
            "Acc 0.42992424964904785 train_loss 0.4912993108610744\n",
            "Acc 0.4476010203361511 train_loss 0.4945111775305122\n",
            "Acc 0.4652777910232544 train_loss 0.509092698469758\n",
            "Acc 0.4829545319080353 train_loss 0.5215712193256388\n",
            "Acc 0.502525269985199 train_loss 0.512350369727722\n",
            "Acc 0.5208333134651184 train_loss 0.5142016648314893\n",
            "Acc 0.5385100841522217 train_loss 0.5321857048648184\n",
            "Acc 0.5555555820465088 train_loss 0.5351514203473926\n",
            "Acc 0.5732322931289673 train_loss 0.5402189861502378\n",
            "Acc 0.5928030014038086 train_loss 0.5309742449899204\n",
            "Acc 0.6123737096786499 train_loss 0.5215244065744408\n",
            "Acc 0.6287878751754761 train_loss 0.5403805084197837\n",
            "Acc 0.6470959782600403 train_loss 0.5464381258934736\n",
            "Acc 0.6660353541374207 train_loss 0.5440587879986398\n",
            "Acc 0.683080792427063 train_loss 0.5576946621106283\n",
            "Acc 0.7007575631141663 train_loss 0.5628867006713623\n",
            "Acc 0.7178030014038086 train_loss 0.5711683369695376\n",
            "Acc 0.7342171669006348 train_loss 0.5871967850718647\n",
            "Acc 0.752525269985199 train_loss 0.5882018163527657\n",
            "Acc 0.7695707082748413 train_loss 0.5989674583875707\n",
            "Acc 0.7878788113594055 train_loss 0.6006736712486938\n",
            "Acc 0.8055555820465088 train_loss 0.6065063272094862\n",
            "Acc 0.8244949579238892 train_loss 0.5989712462243106\n",
            "Acc 0.8428030014038086 train_loss 0.5972652317470183\n",
            "Acc 0.8611111044883728 train_loss 0.5945741659862563\n",
            "Acc 0.8800504803657532 train_loss 0.594691131147556\n",
            "Acc 0.8983585834503174 train_loss 0.5945488079546057\n",
            "Acc 0.9065656661987305 train_loss 0.5985976253822446\n",
            "F1_score : 0.8233890214797137\n",
            "Epoch completed in 1.3942636251449585 minutes\n",
            "***************\n",
            "EPOCH: 8\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.00489267660304904 train_loss : 0.2530818283557892\n",
            "Acc : 0.009943181648850441 train_loss : 0.15759413689374924\n",
            "Acc : 0.01499368716031313 train_loss : 0.12799902260303497\n",
            "Acc : 0.020044192671775818 train_loss : 0.10709432605654001\n",
            "Acc : 0.02509469725191593 train_loss : 0.10136106982827187\n",
            "Acc : 0.02998737432062626 train_loss : 0.11644641620417436\n",
            "Acc : 0.03488004952669144 train_loss : 0.11410741774099213\n",
            "Acc : 0.0399305559694767 train_loss : 0.10539553686976433\n",
            "Acc : 0.04498106241226196 train_loss : 0.09924591746595171\n",
            "Acc : 0.049715910106897354 train_loss : 0.10896087437868118\n",
            "Acc : 0.05476641282439232 train_loss : 0.10266518457369371\n",
            "Acc : 0.05981691926717758 train_loss : 0.10072389928003152\n",
            "Acc : 0.06486742198467255 train_loss : 0.09505709977104114\n",
            "Acc : 0.06991793215274811 train_loss : 0.09234711926962648\n",
            "Acc : 0.07449495047330856 train_loss : 0.11715545033415159\n",
            "Acc : 0.07938762754201889 train_loss : 0.12915267585776746\n",
            "Acc : 0.08428030461072922 train_loss : 0.13420451892649427\n",
            "Acc : 0.08933080732822418 train_loss : 0.12841896961132684\n",
            "Acc : 0.09406565874814987 train_loss : 0.13828140732489133\n",
            "Acc : 0.09880050271749496 train_loss : 0.145774494856596\n",
            "Acc : 0.10385101288557053 train_loss : 0.14066902423898378\n",
            "Acc : 0.10874368995428085 train_loss : 0.13946906426413494\n",
            "Acc : 0.11363636702299118 train_loss : 0.13811145155974056\n",
            "Acc : 0.11821338534355164 train_loss : 0.14292098969841996\n",
            "Acc : 0.12310606241226196 train_loss : 0.14119767501950264\n",
            "Acc : 0.12768307328224182 train_loss : 0.15865230861191565\n",
            "Acc : 0.1324179321527481 train_loss : 0.16529399446315235\n",
            "Acc : 0.13746842741966248 train_loss : 0.16093242846961534\n",
            "Acc : 0.14220328629016876 train_loss : 0.16278996023124662\n",
            "Acc : 0.14725378155708313 train_loss : 0.16053433604538442\n",
            "Acc : 0.15198864042758942 train_loss : 0.16120115343120792\n",
            "Acc : 0.15688131749629974 train_loss : 0.1694273775210604\n",
            "Acc : 0.1619318127632141 train_loss : 0.16625542724222847\n",
            "Acc : 0.1666666716337204 train_loss : 0.1783648099312011\n",
            "Acc : 0.17155934870243073 train_loss : 0.17839013221008437\n",
            "Acc : 0.17645202577114105 train_loss : 0.1763837719336152\n",
            "Acc : 0.18134470283985138 train_loss : 0.17565492812443423\n",
            "Acc : 0.18639519810676575 train_loss : 0.17289900377784906\n",
            "Acc : 0.19128787517547607 train_loss : 0.17185817229060027\n",
            "Acc : 0.19633838534355164 train_loss : 0.17031717775389552\n",
            "Acc : 0.20107322931289673 train_loss : 0.17355533952756627\n",
            "Acc : 0.2061237394809723 train_loss : 0.17038337567022868\n",
            "Acc : 0.21117424964904785 train_loss : 0.16828023070512815\n",
            "Acc : 0.21622474491596222 train_loss : 0.16579302671280774\n",
            "Acc : 0.22111742198467255 train_loss : 0.1645990894900428\n",
            "Acc : 0.22601009905338287 train_loss : 0.16378704620444257\n",
            "Acc : 0.23106060922145844 train_loss : 0.16217793841311273\n",
            "Acc : 0.2361111044883728 train_loss : 0.16013071686029434\n",
            "Acc : 0.24100378155708313 train_loss : 0.15876948392512846\n",
            "Acc : 0.24589645862579346 train_loss : 0.15887685909867286\n",
            "Acc : 0.2507891356945038 train_loss : 0.15858815800325543\n",
            "Acc : 0.2556818127632141 train_loss : 0.159998539978495\n",
            "Acc : 0.26057448983192444 train_loss : 0.16133977655532225\n",
            "Acc : 0.265625 train_loss : 0.16037742310652026\n",
            "Acc : 0.27067551016807556 train_loss : 0.15882892568003049\n",
            "Acc : 0.2757260203361511 train_loss : 0.15665606229699083\n",
            "Acc : 0.28061869740486145 train_loss : 0.1560551514358897\n",
            "Acc : 0.2856691777706146 train_loss : 0.1550121663844791\n",
            "Acc : 0.29056185483932495 train_loss : 0.1538951496072745\n",
            "Acc : 0.2954545319080353 train_loss : 0.15515905103335778\n",
            "Acc : 0.3003472089767456 train_loss : 0.15542195377046944\n",
            "Acc : 0.30539771914482117 train_loss : 0.15444828383624554\n",
            "Acc : 0.31044822931289673 train_loss : 0.15263816933073696\n",
            "Acc : 0.31534090638160706 train_loss : 0.15282481495523825\n",
            "Acc : 0.32007575035095215 train_loss : 0.1526866890490055\n",
            "Acc : 0.3251262605190277 train_loss : 0.15067149475781302\n",
            "Acc : 0.33017677068710327 train_loss : 0.14902726863858415\n",
            "Acc : 0.33491161465644836 train_loss : 0.1527376238749746\n",
            "Acc : 0.3399621248245239 train_loss : 0.15099467517997045\n",
            "Acc : 0.34485480189323425 train_loss : 0.15078406937952551\n",
            "Acc : 0.34958964586257935 train_loss : 0.1520924533284466\n",
            "Acc : 0.3544823229312897 train_loss : 0.15291434968821704\n",
            "Acc : 0.35953283309936523 train_loss : 0.15179731490167037\n",
            "Acc : 0.3641098439693451 train_loss : 0.1548540075435429\n",
            "Acc : 0.36868685483932495 train_loss : 0.15689312366147837\n",
            "Acc : 0.3737373650074005 train_loss : 0.15547655000792523\n",
            "Acc : 0.37863004207611084 train_loss : 0.15541208707183213\n",
            "Acc : 0.38336488604545593 train_loss : 0.15735227545389\n",
            "Acc : 0.3884153962135315 train_loss : 0.1561637503578316\n",
            "Acc : 0.3933080732822418 train_loss : 0.15538603418972344\n",
            "Acc : 0.39820075035095215 train_loss : 0.15477241453841145\n",
            "Acc : 0.40293559432029724 train_loss : 0.16027200919371554\n",
            "Acc : 0.4079861044883728 train_loss : 0.15857467160795827\n",
            "Acc : 0.41287878155708313 train_loss : 0.16014402747775117\n",
            "Acc : 0.4179292917251587 train_loss : 0.1593762188054183\n",
            "Acc : 0.42297980189323425 train_loss : 0.15816037571274263\n",
            "Acc : 0.4280303120613098 train_loss : 0.1567167543174549\n",
            "Acc : 0.4326073229312897 train_loss : 0.16029486764984374\n",
            "Acc : 0.43765783309936523 train_loss : 0.15926552942713326\n",
            "Acc : 0.4427083432674408 train_loss : 0.15784732084721326\n",
            "Acc : 0.4476010203361511 train_loss : 0.1573737798115382\n",
            "Acc : 0.4523358643054962 train_loss : 0.1568128693970325\n",
            "Acc : 0.4570707082748413 train_loss : 0.1601496224081324\n",
            "Acc : 0.46196338534355164 train_loss : 0.16022921946058247\n",
            "Acc : 0.46685606241226196 train_loss : 0.16059482417216425\n",
            "Acc : 0.47159090638160706 train_loss : 0.16203402454266325\n",
            "Acc : 0.4766414165496826 train_loss : 0.1609205686362441\n",
            "Acc : 0.4816919267177582 train_loss : 0.1599304166489414\n",
            "Acc : 0.48674243688583374 train_loss : 0.15857623376403795\n",
            "Acc : 0.4917929172515869 train_loss : 0.15723163690418004\n",
            "Acc : 0.49668559432029724 train_loss : 0.15730958966778055\n",
            "Acc : 0.5015782713890076 train_loss : 0.15704474463036247\n",
            "Acc : 0.5066288113594055 train_loss : 0.15604862440558312\n",
            "Acc : 0.5115214586257935 train_loss : 0.15875340066850185\n",
            "Acc : 0.5165719985961914 train_loss : 0.15755361389546169\n",
            "Acc : 0.5213068127632141 train_loss : 0.15993373869162686\n",
            "Acc : 0.5257260203361511 train_loss : 0.16411790349216104\n",
            "Acc : 0.5307765007019043 train_loss : 0.16319299689321606\n",
            "Acc : 0.5355113744735718 train_loss : 0.16369729037951986\n",
            "Acc : 0.5404040217399597 train_loss : 0.16318165355108\n",
            "Acc : 0.5454545617103577 train_loss : 0.1620903660518092\n",
            "Acc : 0.5501893758773804 train_loss : 0.16458704731693224\n",
            "Acc : 0.5549242496490479 train_loss : 0.16699637416585356\n",
            "Acc : 0.559974730014801 train_loss : 0.16608912867020098\n",
            "Acc : 0.5648674368858337 train_loss : 0.1665391221318556\n",
            "Acc : 0.5699179172515869 train_loss : 0.1653706148138334\n",
            "Acc : 0.5749684572219849 train_loss : 0.16410268924366206\n",
            "Acc : 0.5798611044883728 train_loss : 0.16400264498730333\n",
            "Acc : 0.5849116444587708 train_loss : 0.16289553608150542\n",
            "Acc : 0.5898042917251587 train_loss : 0.1639816739751647\n",
            "Acc : 0.5948547720909119 train_loss : 0.16306229365204483\n",
            "Acc : 0.5995896458625793 train_loss : 0.1666100440302589\n",
            "Acc : 0.6044822931289673 train_loss : 0.16754417568142335\n",
            "Acc : 0.6095328330993652 train_loss : 0.16636235139242583\n",
            "Acc : 0.6145833134651184 train_loss : 0.16526332560181617\n",
            "Acc : 0.6194760203361511 train_loss : 0.16496013273440657\n",
            "Acc : 0.6242108345031738 train_loss : 0.16495043863579045\n",
            "Acc : 0.6291035413742065 train_loss : 0.16432546087889932\n",
            "Acc : 0.6339961886405945 train_loss : 0.16421763572928516\n",
            "Acc : 0.6390467286109924 train_loss : 0.1632681692735507\n",
            "Acc : 0.6440972089767456 train_loss : 0.1623932250867818\n",
            "Acc : 0.6486742496490479 train_loss : 0.16433294255738007\n",
            "Acc : 0.653724730014801 train_loss : 0.16371946224480644\n",
            "Acc : 0.6584596037864685 train_loss : 0.16633258201181889\n",
            "Acc : 0.6635100841522217 train_loss : 0.16525999558744608\n",
            "Acc : 0.6684027910232544 train_loss : 0.16544326271533089\n",
            "Acc : 0.6732954382896423 train_loss : 0.16614704831999583\n",
            "Acc : 0.6783459782600403 train_loss : 0.16523322867958443\n",
            "Acc : 0.6833964586257935 train_loss : 0.16437251796182112\n",
            "Acc : 0.6882891654968262 train_loss : 0.16399424725345202\n",
            "Acc : 0.6927083134651184 train_loss : 0.1667445015611378\n",
            "Acc : 0.6974431872367859 train_loss : 0.16825578655575363\n",
            "Acc : 0.7024936676025391 train_loss : 0.16742206416972033\n",
            "Acc : 0.7072285413742065 train_loss : 0.16741743337156045\n",
            "Acc : 0.7121211886405945 train_loss : 0.16773618382626565\n",
            "Acc : 0.7171717286109924 train_loss : 0.1672367820490713\n",
            "Acc : 0.7222222089767456 train_loss : 0.1663508116945523\n",
            "Acc : 0.7272727489471436 train_loss : 0.16597123265367103\n",
            "Acc : 0.7321653962135315 train_loss : 0.16544380746531806\n",
            "Acc : 0.7370581030845642 train_loss : 0.1662264244010051\n",
            "Acc : 0.7419507503509521 train_loss : 0.1666807479268273\n",
            "Acc : 0.7470012903213501 train_loss : 0.16587401727998727\n",
            "Acc : 0.7520517706871033 train_loss : 0.1654353103070867\n",
            "Acc : 0.7571022510528564 train_loss : 0.16491561225772677\n",
            "Acc : 0.7619949579238892 train_loss : 0.16536651501251806\n",
            "Acc : 0.7667297720909119 train_loss : 0.1654576661829383\n",
            "Acc : 0.7716224789619446 train_loss : 0.16516649597294772\n",
            "Acc : 0.7766729593276978 train_loss : 0.16476588241190096\n",
            "Acc : 0.7814078330993652 train_loss : 0.16565482578468774\n",
            "Acc : 0.7864583134651184 train_loss : 0.16504068442154676\n",
            "Acc : 0.7915088534355164 train_loss : 0.16442546889371012\n",
            "Acc : 0.7965593338012695 train_loss : 0.16391432940683984\n",
            "Acc : 0.8014520406723022 train_loss : 0.1643158678491057\n",
            "Acc : 0.8063446879386902 train_loss : 0.1648059309482938\n",
            "Acc : 0.8113952279090881 train_loss : 0.16400057632814755\n",
            "Acc : 0.8164457082748413 train_loss : 0.1632635700909129\n",
            "Acc : 0.8214961886405945 train_loss : 0.1625273153631987\n",
            "Acc : 0.826231062412262 train_loss : 0.1637938447474014\n",
            "Acc : 0.8311237096786499 train_loss : 0.16320943894118248\n",
            "Acc : 0.8361742496490479 train_loss : 0.1624282375416335\n",
            "Acc : 0.841224730014801 train_loss : 0.16190823455128753\n",
            "Acc : 0.846275269985199 train_loss : 0.16148502124083597\n",
            "Acc : 0.8511679172515869 train_loss : 0.16104652280407833\n",
            "Acc : 0.8559027910232544 train_loss : 0.1612100937756999\n",
            "Acc : 0.8607954382896423 train_loss : 0.16111139612538475\n",
            "Acc : 0.8658459782600403 train_loss : 0.16054696115580472\n",
            "Acc : 0.8707386255264282 train_loss : 0.16020006926383001\n",
            "Acc : 0.8756313323974609 train_loss : 0.15985514068703974\n",
            "Acc : 0.8806818127632141 train_loss : 0.15928401107062173\n",
            "Acc : 0.8857322931289673 train_loss : 0.158547890983108\n",
            "Acc : 0.8907828330993652 train_loss : 0.15802126107171424\n",
            "Acc : 0.8955176472663879 train_loss : 0.15872034681244537\n",
            "***************\n",
            "EPOCH: 8\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.9005681872367859 train_loss : 0.1580886036625814\n",
            "Acc : 0.9054608345031738 train_loss : 0.1585101386754895\n",
            "Acc : 0.9103535413742065 train_loss : 0.15880248968464297\n",
            "Acc : 0.9152461886405945 train_loss : 0.15856553108421384\n",
            "Acc : 0.9201388955116272 train_loss : 0.15828400586338284\n",
            "Acc : 0.9250315427780151 train_loss : 0.1579605460939731\n",
            "Acc : 0.9297664165496826 train_loss : 0.15901927148341818\n",
            "Acc : 0.9346590638160706 train_loss : 0.15958163341213213\n",
            "Acc : 0.9397096037864685 train_loss : 0.158823411346106\n",
            "Acc : 0.9447600841522217 train_loss : 0.1581556249972588\n",
            "Acc : 0.9496527910232544 train_loss : 0.15771853608209543\n",
            "Acc : 0.9547032713890076 train_loss : 0.15709419082850218\n",
            "Acc : 0.9597538113594055 train_loss : 0.15643881414181146\n",
            "Acc : 0.9648042917251587 train_loss : 0.15583357325165856\n",
            "Acc : 0.9693813323974609 train_loss : 0.15705471190855588\n",
            "Acc : 0.9742739796638489 train_loss : 0.15716617618395826\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.5833876132965088\n",
            "Acc 0.03598484769463539 train_loss 0.6399550437927246\n",
            "Acc 0.056186869740486145 train_loss 0.4476889247695605\n",
            "Acc 0.07386363297700882 train_loss 0.4627630915492773\n",
            "Acc 0.09343434125185013 train_loss 0.4616305574774742\n",
            "Acc 0.1111111119389534 train_loss 0.5310558366278807\n",
            "Acc 0.12752525508403778 train_loss 0.6165683854903493\n",
            "Acc 0.14646464586257935 train_loss 0.5826990371569991\n",
            "Acc 0.1654040366411209 train_loss 0.5663337931036949\n",
            "Acc 0.18371212482452393 train_loss 0.5784999392926693\n",
            "Acc 0.2013888955116272 train_loss 0.6135224124247377\n",
            "Acc 0.21843434870243073 train_loss 0.645358777915438\n",
            "Acc 0.2373737394809723 train_loss 0.6232804719072121\n",
            "Acc 0.25631314516067505 train_loss 0.5950673717473235\n",
            "Acc 0.2752525210380554 train_loss 0.574817240734895\n",
            "Acc 0.2929292917251587 train_loss 0.5943209459073842\n",
            "Acc 0.31186869740486145 train_loss 0.5772098833147217\n",
            "Acc 0.3295454680919647 train_loss 0.5848876407576932\n",
            "Acc 0.3497474789619446 train_loss 0.5551917303941751\n",
            "Acc 0.36994948983192444 train_loss 0.5302417024970054\n",
            "Acc 0.3901515007019043 train_loss 0.5055862641228097\n",
            "Acc 0.4097222089767456 train_loss 0.5017087030309167\n",
            "Acc 0.42866161465644836 train_loss 0.49983068174966006\n",
            "Acc 0.4469696879386902 train_loss 0.5030592534846315\n",
            "Acc 0.4640151560306549 train_loss 0.5210016111284494\n",
            "Acc 0.4816919267177582 train_loss 0.5337951622473506\n",
            "Acc 0.5012626051902771 train_loss 0.5247252443460403\n",
            "Acc 0.5195707082748413 train_loss 0.5250846991714623\n",
            "Acc 0.5372474789619446 train_loss 0.5423880890654079\n",
            "Acc 0.5549242496490479 train_loss 0.5442971341932813\n",
            "Acc 0.5719696879386902 train_loss 0.5509837455446681\n",
            "Acc 0.5915403962135315 train_loss 0.5418983253766783\n",
            "Acc 0.6111111044883728 train_loss 0.5320397518913854\n",
            "Acc 0.627525269985199 train_loss 0.5526985116850804\n",
            "Acc 0.6458333134651184 train_loss 0.5589609821460076\n",
            "Acc 0.6647727489471436 train_loss 0.5574725281136731\n",
            "Acc 0.6818181872367859 train_loss 0.5708200684371026\n",
            "Acc 0.6994949579238892 train_loss 0.5728883533983639\n",
            "Acc 0.7171717286109924 train_loss 0.5826967368618801\n",
            "Acc 0.7335858345031738 train_loss 0.5986439401749521\n",
            "Acc 0.751893937587738 train_loss 0.5998533871933455\n",
            "Acc 0.7689393758773804 train_loss 0.6104061059387667\n",
            "Acc 0.7872474789619446 train_loss 0.6134095048575208\n",
            "Acc 0.8049242496490479 train_loss 0.6181410438042473\n",
            "Acc 0.8238636255264282 train_loss 0.6116091140856346\n",
            "Acc 0.8421717286109924 train_loss 0.6092523526319343\n",
            "Acc 0.8604797720909119 train_loss 0.6078514870890277\n",
            "Acc 0.8787878751754761 train_loss 0.6069854679905499\n",
            "Acc 0.8970959782600403 train_loss 0.6070082876831293\n",
            "Acc 0.9053030014038086 train_loss 0.6106883390620351\n",
            "F1_score : 0.8205741626794258\n",
            "Epoch completed in 1.407227921485901 minutes\n",
            "***************\n",
            "EPOCH: 9\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.00489267660304904 train_loss : 0.26382964849472046\n",
            "Acc : 0.009943181648850441 train_loss : 0.1588491890579462\n",
            "Acc : 0.01499368716031313 train_loss : 0.12159029891093572\n",
            "Acc : 0.020044192671775818 train_loss : 0.09734114725142717\n",
            "Acc : 0.02509469725191593 train_loss : 0.08822939321398734\n",
            "Acc : 0.030145201832056046 train_loss : 0.08252628830571969\n",
            "Acc : 0.03519570827484131 train_loss : 0.08288980860795293\n",
            "Acc : 0.04024621099233627 train_loss : 0.077077257912606\n",
            "Acc : 0.0451388880610466 train_loss : 0.08442532478107347\n",
            "Acc : 0.05018939450383186 train_loss : 0.08261248357594013\n",
            "Acc : 0.05523989722132683 train_loss : 0.08097060735930096\n",
            "Acc : 0.06029040366411209 train_loss : 0.08334463058660428\n",
            "Acc : 0.06534090638160706 train_loss : 0.0800273226430783\n",
            "Acc : 0.07039141654968262 train_loss : 0.07770105132034846\n",
            "Acc : 0.07496843487024307 train_loss : 0.09010583559672038\n",
            "Acc : 0.08001893758773804 train_loss : 0.08955579530447721\n",
            "Acc : 0.08491161465644836 train_loss : 0.09224776222425349\n",
            "Acc : 0.08996212482452393 train_loss : 0.08922015937666099\n",
            "Acc : 0.09485479444265366 train_loss : 0.09789543853778589\n",
            "Acc : 0.09958964586257935 train_loss : 0.10963461976498365\n",
            "Acc : 0.10464014858007431 train_loss : 0.10830889056835856\n",
            "Acc : 0.10953282564878464 train_loss : 0.10760325921530073\n",
            "Acc : 0.11426767706871033 train_loss : 0.10914492526132127\n",
            "Acc : 0.11931817978620529 train_loss : 0.10642964548120896\n",
            "Acc : 0.12436868995428085 train_loss : 0.10520521074533462\n",
            "Acc : 0.1289457082748413 train_loss : 0.11891083218730412\n",
            "Acc : 0.1336805522441864 train_loss : 0.1308743073432534\n",
            "Acc : 0.13873106241226196 train_loss : 0.1273311133097325\n",
            "Acc : 0.1436237394809723 train_loss : 0.1284367808237158\n",
            "Acc : 0.14867424964904785 train_loss : 0.12612955942749976\n",
            "Acc : 0.15356691181659698 train_loss : 0.12578869586990726\n",
            "Acc : 0.1584595888853073 train_loss : 0.13334628520533442\n",
            "Acc : 0.16351009905338287 train_loss : 0.13080349553263548\n",
            "Acc : 0.16824494302272797 train_loss : 0.14392838134046862\n",
            "Acc : 0.17329545319080353 train_loss : 0.14046221833143915\n",
            "Acc : 0.17787247896194458 train_loss : 0.1425822318221132\n",
            "Acc : 0.1827651560306549 train_loss : 0.1423326355178614\n",
            "Acc : 0.18781565129756927 train_loss : 0.14063763726306588\n",
            "Acc : 0.19255051016807556 train_loss : 0.14048594379654297\n",
            "Acc : 0.1974431872367859 train_loss : 0.1398730249144137\n",
            "Acc : 0.2018623799085617 train_loss : 0.1474498496731607\n",
            "Acc : 0.20691287517547607 train_loss : 0.14560215556550593\n",
            "Acc : 0.2118055522441864 train_loss : 0.14429111735418784\n",
            "Acc : 0.21685606241226196 train_loss : 0.14184787005863406\n",
            "Acc : 0.22190657258033752 train_loss : 0.14148386600944732\n",
            "Acc : 0.22679924964904785 train_loss : 0.1408470302172329\n",
            "Acc : 0.23184974491596222 train_loss : 0.13865213365630902\n",
            "Acc : 0.23690025508403778 train_loss : 0.1373485974036157\n",
            "Acc : 0.2417929321527481 train_loss : 0.13671870864167504\n",
            "Acc : 0.2465277761220932 train_loss : 0.13750679910182953\n",
            "Acc : 0.2514204680919647 train_loss : 0.1364115089470265\n",
            "Acc : 0.25631314516067505 train_loss : 0.1382089565293147\n",
            "Acc : 0.2612058222293854 train_loss : 0.13914961845807308\n",
            "Acc : 0.2660984992980957 train_loss : 0.13901332255314897\n",
            "Acc : 0.2711489796638489 train_loss : 0.13690892918543382\n",
            "Acc : 0.27619948983192444 train_loss : 0.1349788750854454\n",
            "Acc : 0.28109216690063477 train_loss : 0.1355609565385078\n",
            "Acc : 0.2861426770687103 train_loss : 0.1340853215098895\n",
            "Acc : 0.2911931872367859 train_loss : 0.13240158434767843\n",
            "Acc : 0.2960858643054962 train_loss : 0.1339609516474108\n",
            "Acc : 0.3011363744735718 train_loss : 0.13242486311641874\n",
            "Acc : 0.3060290515422821 train_loss : 0.133997994535152\n",
            "Acc : 0.3110795319080353 train_loss : 0.13293974630771174\n",
            "Acc : 0.31613004207611084 train_loss : 0.13254777665133588\n",
            "Acc : 0.32102271914482117 train_loss : 0.13233521921703448\n",
            "Acc : 0.32607322931289673 train_loss : 0.13065728559299852\n",
            "Acc : 0.3311237394809723 train_loss : 0.12899445828550787\n",
            "Acc : 0.3358585834503174 train_loss : 0.13287329331369085\n",
            "Acc : 0.34090909361839294 train_loss : 0.13125842167199522\n",
            "Acc : 0.34564393758773804 train_loss : 0.1328437720558473\n",
            "Acc : 0.3506944477558136 train_loss : 0.13177037590616186\n",
            "Acc : 0.3555871248245239 train_loss : 0.1328183772145874\n",
            "Acc : 0.36047980189323425 train_loss : 0.13315940694245573\n",
            "Acc : 0.3650568127632141 train_loss : 0.13506132258555373\n",
            "Acc : 0.3701073229312897 train_loss : 0.13385361358523368\n",
            "Acc : 0.37515783309936523 train_loss : 0.13264708946409978\n",
            "Acc : 0.38005051016807556 train_loss : 0.13296131634867037\n",
            "Acc : 0.38478535413742065 train_loss : 0.13496233312747416\n",
            "Acc : 0.389678031206131 train_loss : 0.13466969019250025\n",
            "Acc : 0.39472854137420654 train_loss : 0.13338139983825387\n",
            "Acc : 0.39962121844291687 train_loss : 0.13467668047473755\n",
            "Acc : 0.40435606241226196 train_loss : 0.13929986622093654\n",
            "Acc : 0.4094065725803375 train_loss : 0.1379675785057157\n",
            "Acc : 0.41429924964904785 train_loss : 0.13952703053309096\n",
            "Acc : 0.4193497598171234 train_loss : 0.1387184929979198\n",
            "Acc : 0.42424243688583374 train_loss : 0.13906659703528465\n",
            "Acc : 0.42913511395454407 train_loss : 0.1385888438243633\n",
            "Acc : 0.4337121248245239 train_loss : 0.14065891993232071\n",
            "Acc : 0.4387626349925995 train_loss : 0.1395251097345955\n",
            "Acc : 0.44381314516067505 train_loss : 0.13829302833312088\n",
            "Acc : 0.4487058222293854 train_loss : 0.13774498613489852\n",
            "Acc : 0.45375630259513855 train_loss : 0.13716014737830215\n",
            "Acc : 0.4586489796638489 train_loss : 0.14065513851982292\n",
            "Acc : 0.46369948983192444 train_loss : 0.13986738347151179\n",
            "Acc : 0.46875 train_loss : 0.138815186525646\n",
            "Acc : 0.4736426770687103 train_loss : 0.1392998929756383\n",
            "Acc : 0.4786931872367859 train_loss : 0.13831556381024035\n",
            "Acc : 0.48374369740486145 train_loss : 0.13725857786378082\n",
            "Acc : 0.4887941777706146 train_loss : 0.13608853349631483\n",
            "Acc : 0.4938446879386902 train_loss : 0.13497245233505964\n",
            "Acc : 0.49889519810676575 train_loss : 0.13385926725545733\n",
            "Acc : 0.5037878751754761 train_loss : 0.13355814271113453\n",
            "Acc : 0.5088383555412292 train_loss : 0.13280911530250483\n",
            "Acc : 0.513731062412262 train_loss : 0.13548398222057864\n",
            "Acc : 0.5187815427780151 train_loss : 0.1345413326507523\n",
            "Acc : 0.5232007503509521 train_loss : 0.13747383056665366\n",
            "Acc : 0.5276199579238892 train_loss : 0.14148634943728136\n",
            "Acc : 0.5326704382896423 train_loss : 0.14032159128260832\n",
            "Acc : 0.5374053120613098 train_loss : 0.14059733732714566\n",
            "Acc : 0.542455792427063 train_loss : 0.13955133950168436\n",
            "Acc : 0.5473484992980957 train_loss : 0.1391690990275091\n",
            "Acc : 0.5522411465644836 train_loss : 0.14020413606028473\n",
            "Acc : 0.5571338534355164 train_loss : 0.14099953046678443\n",
            "Acc : 0.5620265007019043 train_loss : 0.14044394389840595\n",
            "Acc : 0.5667613744735718 train_loss : 0.14147258044584937\n",
            "Acc : 0.5716540217399597 train_loss : 0.14094534856748991\n",
            "Acc : 0.5767045617103577 train_loss : 0.13994822923380595\n",
            "Acc : 0.5815972089767456 train_loss : 0.14032691829222238\n",
            "Acc : 0.5864899158477783 train_loss : 0.1398050297802987\n",
            "Acc : 0.591224730014801 train_loss : 0.14134096613464256\n",
            "Acc : 0.596275269985199 train_loss : 0.14051868460210395\n",
            "Acc : 0.6010100841522217 train_loss : 0.144623968032662\n",
            "Acc : 0.6059027910232544 train_loss : 0.1453273367379012\n",
            "Acc : 0.6107954382896423 train_loss : 0.146010126633149\n",
            "Acc : 0.6158459782600403 train_loss : 0.1451025329977274\n",
            "Acc : 0.6208964586257935 train_loss : 0.14449817712403953\n",
            "Acc : 0.6259469985961914 train_loss : 0.14391688704783992\n",
            "Acc : 0.6308396458625793 train_loss : 0.1433228728856193\n",
            "Acc : 0.6358901262283325 train_loss : 0.14301424474968005\n",
            "Acc : 0.6409406661987305 train_loss : 0.14215486896439242\n",
            "Acc : 0.6459911465644836 train_loss : 0.14147068891418343\n",
            "Acc : 0.6508838534355164 train_loss : 0.14239534759668238\n",
            "Acc : 0.6559343338012695 train_loss : 0.14155623918973415\n",
            "Acc : 0.6608270406723022 train_loss : 0.14330497536975056\n",
            "Acc : 0.6658775210380554 train_loss : 0.1423883174304609\n",
            "Acc : 0.6706123948097229 train_loss : 0.14329388156971512\n",
            "Acc : 0.6755050420761108 train_loss : 0.14407043594078425\n",
            "Acc : 0.6803977489471436 train_loss : 0.14439781334089197\n",
            "Acc : 0.6854482293128967 train_loss : 0.14352933003855267\n",
            "Acc : 0.6903409361839294 train_loss : 0.14306040967681577\n",
            "Acc : 0.6947600841522217 train_loss : 0.1458226925039545\n",
            "Acc : 0.6994949579238892 train_loss : 0.14716504752950768\n",
            "Acc : 0.7045454382896423 train_loss : 0.1464054429760346\n",
            "Acc : 0.7095959782600403 train_loss : 0.1461151216386093\n",
            "Acc : 0.7144886255264282 train_loss : 0.14669584765516477\n",
            "Acc : 0.7193813323974609 train_loss : 0.14639180898666382\n",
            "Acc : 0.7244318127632141 train_loss : 0.14567035010882787\n",
            "Acc : 0.7293245196342468 train_loss : 0.14539255866327802\n",
            "Acc : 0.7342171669006348 train_loss : 0.14489526731655902\n",
            "Acc : 0.7391098737716675 train_loss : 0.14558174446225167\n",
            "Acc : 0.7440025210380554 train_loss : 0.14602531093043208\n",
            "Acc : 0.7490530014038086 train_loss : 0.14516913553846902\n",
            "Acc : 0.7541035413742065 train_loss : 0.14470350534162102\n",
            "Acc : 0.7591540217399597 train_loss : 0.14406339767352713\n",
            "Acc : 0.7640467286109924 train_loss : 0.14460573255294754\n",
            "Acc : 0.7689393758773804 train_loss : 0.14479418081016496\n",
            "Acc : 0.7738320827484131 train_loss : 0.1448144972632835\n",
            "Acc : 0.7788825631141663 train_loss : 0.1445581235486684\n",
            "Acc : 0.783775269985199 train_loss : 0.14535045545290476\n",
            "Acc : 0.7886679172515869 train_loss : 0.14537259038770572\n",
            "Acc : 0.7934027910232544 train_loss : 0.14635541790992207\n",
            "Acc : 0.7984532713890076 train_loss : 0.14564450434696527\n",
            "Acc : 0.8033459782600403 train_loss : 0.14545777805743773\n",
            "Acc : 0.8083964586257935 train_loss : 0.1454307694987553\n",
            "Acc : 0.8134469985961914 train_loss : 0.14473974659587396\n",
            "Acc : 0.8184974789619446 train_loss : 0.14413895740746016\n",
            "Acc : 0.8235479593276978 train_loss : 0.14354850757800178\n",
            "Acc : 0.8282828330993652 train_loss : 0.14409538540279582\n",
            "Acc : 0.8333333134651184 train_loss : 0.143411705336539\n",
            "Acc : 0.8383838534355164 train_loss : 0.14285132217714014\n",
            "Acc : 0.8432765007019043 train_loss : 0.14259471860850415\n",
            "Acc : 0.8483270406723022 train_loss : 0.14209263920134238\n",
            "Acc : 0.8533775210380554 train_loss : 0.14150355378370408\n",
            "Acc : 0.8582702279090881 train_loss : 0.14127241975317398\n",
            "Acc : 0.8631628751754761 train_loss : 0.14114535775567805\n",
            "Acc : 0.8682133555412292 train_loss : 0.14061919080135835\n",
            "Acc : 0.873106062412262 train_loss : 0.14043473444963242\n",
            "Acc : 0.8781565427780151 train_loss : 0.13985433240075795\n",
            "Acc : 0.8830492496490479 train_loss : 0.1398667381275133\n",
            "Acc : 0.888099730014801 train_loss : 0.13920654587240683\n",
            "Acc : 0.8929924368858337 train_loss : 0.13969067478994968\n",
            "Acc : 0.8978850841522217 train_loss : 0.14084286741134558\n",
            "\n",
            "EPOCH: 9\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.9029356241226196 train_loss : 0.14024465802873745\n",
            "Acc : 0.9078282713890076 train_loss : 0.1403579868113055\n",
            "Acc : 0.9127209782600403 train_loss : 0.14068132194916944\n",
            "Acc : 0.9177714586257935 train_loss : 0.14038530207409333\n",
            "Acc : 0.9228219985961914 train_loss : 0.14005948614109964\n",
            "Acc : 0.9278724789619446 train_loss : 0.13945861709641016\n",
            "Acc : 0.9327651262283325 train_loss : 0.13977778425055837\n",
            "Acc : 0.9378156661987305 train_loss : 0.13914581095113565\n",
            "Acc : 0.9428661465644836 train_loss : 0.13848747596320213\n",
            "Acc : 0.9479166865348816 train_loss : 0.13795190939466315\n",
            "Acc : 0.9529671669006348 train_loss : 0.13752202499040206\n",
            "Acc : 0.9580176472663879 train_loss : 0.1370386924527432\n",
            "Acc : 0.9630681872367859 train_loss : 0.13647155633243996\n",
            "Acc : 0.9679608345031738 train_loss : 0.13644529728465998\n",
            "Acc : 0.9725378751754761 train_loss : 0.1378396994713221\n",
            "Acc : 0.9775883555412292 train_loss : 0.13776944488794965\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.4625324606895447\n",
            "Acc 0.036616161465644836 train_loss 0.6205938160419464\n",
            "Acc 0.05681818351149559 train_loss 0.42758094643553096\n",
            "Acc 0.07512626051902771 train_loss 0.4330924479290843\n",
            "Acc 0.09469696879386902 train_loss 0.43626963123679163\n",
            "Acc 0.11237373948097229 train_loss 0.5130003510663906\n",
            "Acc 0.12941919267177582 train_loss 0.5983915004347052\n",
            "Acc 0.14835858345031738 train_loss 0.5651879138313234\n",
            "Acc 0.16729797422885895 train_loss 0.5481836443973912\n",
            "Acc 0.18560606241226196 train_loss 0.5644824426621199\n",
            "Acc 0.20328283309936523 train_loss 0.6002688336778771\n",
            "Acc 0.2209595888853073 train_loss 0.6208349152778586\n",
            "Acc 0.23989899456501007 train_loss 0.5883843850057858\n",
            "Acc 0.25883838534355164 train_loss 0.5646077285387686\n",
            "Acc 0.2777777910232544 train_loss 0.537298389027516\n",
            "Acc 0.2954545319080353 train_loss 0.5617330854292959\n",
            "Acc 0.31439393758773804 train_loss 0.5397865866475245\n",
            "Acc 0.3320707082748413 train_loss 0.550256906905108\n",
            "Acc 0.35227271914482117 train_loss 0.522074704646672\n",
            "Acc 0.3718434274196625 train_loss 0.5015866400208324\n",
            "Acc 0.3920454680919647 train_loss 0.4781269392087346\n",
            "Acc 0.41161614656448364 train_loss 0.4751041399484331\n",
            "Acc 0.4305555522441864 train_loss 0.4734480837764947\n",
            "Acc 0.4488636255264282 train_loss 0.4784513528769215\n",
            "Acc 0.4665403962135315 train_loss 0.4941630545258522\n",
            "Acc 0.4848484992980957 train_loss 0.5049979334267286\n",
            "Acc 0.504419207572937 train_loss 0.49624537235056915\n",
            "Acc 0.5227272510528564 train_loss 0.498710018449596\n",
            "Acc 0.5410353541374207 train_loss 0.5153381981212517\n",
            "Acc 0.5587121248245239 train_loss 0.52060651704669\n",
            "Acc 0.5757575631141663 train_loss 0.5287813902381928\n",
            "Acc 0.5953282713890076 train_loss 0.5201086283195764\n",
            "Acc 0.6148989796638489 train_loss 0.5117674372864492\n",
            "Acc 0.6319444179534912 train_loss 0.5318029831875773\n",
            "Acc 0.6502525210380554 train_loss 0.5395118162035942\n",
            "Acc 0.6691918969154358 train_loss 0.5355008345925145\n",
            "Acc 0.6868686676025391 train_loss 0.54688384102003\n",
            "Acc 0.7051767706871033 train_loss 0.5458388989301104\n",
            "Acc 0.7222222089767456 train_loss 0.5564901173496858\n",
            "Acc 0.7386363744735718 train_loss 0.5740418868139386\n",
            "Acc 0.7569444179534912 train_loss 0.5748326024267731\n",
            "Acc 0.7733585834503174 train_loss 0.5874591329622836\n",
            "Acc 0.7916666865348816 train_loss 0.5906866028211838\n",
            "Acc 0.8093434572219849 train_loss 0.5975068064237182\n",
            "Acc 0.8289141654968262 train_loss 0.5907094221976068\n",
            "Acc 0.8478535413742065 train_loss 0.5877234192322129\n",
            "Acc 0.8661616444587708 train_loss 0.587766299380901\n",
            "Acc 0.8851010203361511 train_loss 0.5852255661350986\n",
            "Acc 0.9040403962135315 train_loss 0.5846072278764783\n",
            "Acc 0.9122474789619446 train_loss 0.5845584262907505\n",
            "F1_score : 0.8374269005847953\n",
            "Epoch completed in 1.398671042919159 minutes\n",
            "***************\n",
            "EPOCH: 10\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.00489267660304904 train_loss : 0.26041239500045776\n",
            "Acc : 0.009943181648850441 train_loss : 0.1515350192785263\n",
            "Acc : 0.01499368716031313 train_loss : 0.1166548269490401\n",
            "Acc : 0.020044192671775818 train_loss : 0.09172626491636038\n",
            "Acc : 0.02509469725191593 train_loss : 0.08394813761115075\n",
            "Acc : 0.030145201832056046 train_loss : 0.07572270681460698\n",
            "Acc : 0.03519570827484131 train_loss : 0.07062439354402679\n",
            "Acc : 0.040088385343551636 train_loss : 0.08088890230283141\n",
            "Acc : 0.0451388880610466 train_loss : 0.07460376951429579\n",
            "Acc : 0.05018939450383186 train_loss : 0.0725843820720911\n",
            "Acc : 0.05523989722132683 train_loss : 0.07024752856655554\n",
            "Acc : 0.06029040366411209 train_loss : 0.07082109122226636\n",
            "Acc : 0.06534090638160706 train_loss : 0.06836919773083466\n",
            "Acc : 0.07039141654968262 train_loss : 0.06567873912198204\n",
            "Acc : 0.07512626051902771 train_loss : 0.07958845496177673\n",
            "Acc : 0.08017677068710327 train_loss : 0.07838960736989975\n",
            "Acc : 0.0850694477558136 train_loss : 0.08587630268405466\n",
            "Acc : 0.09011995047330856 train_loss : 0.08355782926082611\n",
            "Acc : 0.09501262754201889 train_loss : 0.09005739579075261\n",
            "Acc : 0.09990530461072922 train_loss : 0.09540788754820824\n",
            "Acc : 0.10495580732822418 train_loss : 0.09296524098941258\n",
            "Acc : 0.11000631004571915 train_loss : 0.09005850223316388\n",
            "Acc : 0.11505682021379471 train_loss : 0.08796026241844115\n",
            "Acc : 0.12010732293128967 train_loss : 0.08644165152994294\n",
            "Acc : 0.12515783309936523 train_loss : 0.0859129162877798\n",
            "Acc : 0.1297348439693451 train_loss : 0.09986980567471339\n",
            "Acc : 0.13446970283985138 train_loss : 0.11116910608554328\n",
            "Acc : 0.13952019810676575 train_loss : 0.10904409556782671\n",
            "Acc : 0.1445707082748413 train_loss : 0.10861477776077287\n",
            "Acc : 0.14962121844291687 train_loss : 0.10682621492693821\n",
            "Acc : 0.1545138955116272 train_loss : 0.10672637069177243\n",
            "Acc : 0.15940657258033752 train_loss : 0.11610435618786141\n",
            "Acc : 0.1644570678472519 train_loss : 0.11360008500967965\n",
            "Acc : 0.16919191181659698 train_loss : 0.12793557154124274\n",
            "Acc : 0.17424242198467255 train_loss : 0.12510362065264158\n",
            "Acc : 0.1792929321527481 train_loss : 0.123034351091418\n",
            "Acc : 0.18418560922145844 train_loss : 0.12212510900320234\n",
            "Acc : 0.1892361044883728 train_loss : 0.12000986463145207\n",
            "Acc : 0.19428661465644836 train_loss : 0.1194125398611411\n",
            "Acc : 0.19933712482452393 train_loss : 0.11857961248606444\n",
            "Acc : 0.20391413569450378 train_loss : 0.12234610155588244\n",
            "Acc : 0.20896464586257935 train_loss : 0.1202575033087106\n",
            "Acc : 0.2140151560306549 train_loss : 0.11846289740398873\n",
            "Acc : 0.21906565129756927 train_loss : 0.11702975275164301\n",
            "Acc : 0.22411616146564484 train_loss : 0.11548719190888934\n",
            "Acc : 0.22900883853435516 train_loss : 0.11462946867813235\n",
            "Acc : 0.23405934870243073 train_loss : 0.11291513409703335\n",
            "Acc : 0.2391098439693451 train_loss : 0.11129803800334533\n",
            "Acc : 0.24416035413742065 train_loss : 0.1102743461271938\n",
            "Acc : 0.24905303120613098 train_loss : 0.11049652822315693\n",
            "Acc : 0.2539457082748413 train_loss : 0.10972820667951715\n",
            "Acc : 0.25883838534355164 train_loss : 0.11111314745190051\n",
            "Acc : 0.26373106241226196 train_loss : 0.11185675219825979\n",
            "Acc : 0.2687815725803375 train_loss : 0.11096552904281351\n",
            "Acc : 0.2738320827484131 train_loss : 0.10940792899240147\n",
            "Acc : 0.2787247598171234 train_loss : 0.10896754996584994\n",
            "Acc : 0.2837752401828766 train_loss : 0.10827829545004326\n",
            "Acc : 0.28882575035095215 train_loss : 0.10742506192162118\n",
            "Acc : 0.2938762605190277 train_loss : 0.10612680791419442\n",
            "Acc : 0.29876893758773804 train_loss : 0.10663638329133392\n",
            "Acc : 0.3038194477558136 train_loss : 0.10591724737867957\n",
            "Acc : 0.30886995792388916 train_loss : 0.10519901990529991\n",
            "Acc : 0.3139204680919647 train_loss : 0.104192688026362\n",
            "Acc : 0.3189709484577179 train_loss : 0.10346943393233232\n",
            "Acc : 0.3238636255264282 train_loss : 0.1032186357734295\n",
            "Acc : 0.3289141356945038 train_loss : 0.1019187102263624\n",
            "Acc : 0.33396464586257935 train_loss : 0.10058500050608792\n",
            "Acc : 0.3388573229312897 train_loss : 0.10459549264872775\n",
            "Acc : 0.34390783309936523 train_loss : 0.10338130531211694\n",
            "Acc : 0.3486426770687103 train_loss : 0.10438268676932369\n",
            "Acc : 0.35353535413742065 train_loss : 0.10474944516071971\n",
            "Acc : 0.358428031206131 train_loss : 0.10546685985496475\n",
            "Acc : 0.36347854137420654 train_loss : 0.1049369625900298\n",
            "Acc : 0.3685290515422821 train_loss : 0.10466252038305676\n",
            "Acc : 0.3735795319080353 train_loss : 0.10394853167235851\n",
            "Acc : 0.37863004207611084 train_loss : 0.1031114544082237\n",
            "Acc : 0.38352271914482117 train_loss : 0.10337039032443003\n",
            "Acc : 0.3884153962135315 train_loss : 0.10509405645709007\n",
            "Acc : 0.39346590638160706 train_loss : 0.10427687168592893\n",
            "Acc : 0.3985164165496826 train_loss : 0.10332177309319376\n",
            "Acc : 0.40340909361839294 train_loss : 0.10315011579681325\n",
            "Acc : 0.40814393758773804 train_loss : 0.10837459755016536\n",
            "Acc : 0.4131944477558136 train_loss : 0.10741924485528326\n",
            "Acc : 0.4180871248245239 train_loss : 0.10926355173190434\n",
            "Acc : 0.4231376349925995 train_loss : 0.10837704285102732\n",
            "Acc : 0.4278724789619446 train_loss : 0.10901957152541293\n",
            "Acc : 0.43292298913002014 train_loss : 0.1085680106590534\n",
            "Acc : 0.43734216690063477 train_loss : 0.11139669201590797\n",
            "Acc : 0.4422348439693451 train_loss : 0.11131178905789772\n",
            "Acc : 0.44728535413742065 train_loss : 0.1105587675753567\n",
            "Acc : 0.4523358643054962 train_loss : 0.10978584924896995\n",
            "Acc : 0.4573863744735718 train_loss : 0.10910724512422862\n",
            "Acc : 0.4622790515422821 train_loss : 0.11257177611352295\n",
            "Acc : 0.4673295319080353 train_loss : 0.11200568670446569\n",
            "Acc : 0.47238004207611084 train_loss : 0.11166569688602497\n",
            "Acc : 0.47727271914482117 train_loss : 0.11282220005523413\n",
            "Acc : 0.4821653962135315 train_loss : 0.11260341564865456\n",
            "Acc : 0.4870580732822418 train_loss : 0.11224034744105776\n",
            "Acc : 0.4921085834503174 train_loss : 0.11132890070703896\n",
            "Acc : 0.49715909361839294 train_loss : 0.11040801202878356\n",
            "Acc : 0.5022096037864685 train_loss : 0.1096472427949752\n",
            "Acc : 0.5071022510528564 train_loss : 0.10987021268217587\n",
            "Acc : 0.5121527910232544 train_loss : 0.10906685631975387\n",
            "Acc : 0.5170454382896423 train_loss : 0.112234359069799\n",
            "Acc : 0.5220959782600403 train_loss : 0.11153194201844079\n",
            "Acc : 0.526830792427063 train_loss : 0.11404939115328609\n",
            "Acc : 0.5314078330993652 train_loss : 0.11715742813371052\n",
            "Acc : 0.5364583134651184 train_loss : 0.11620578373333922\n",
            "Acc : 0.5415088534355164 train_loss : 0.11572598218234308\n",
            "Acc : 0.5465593338012695 train_loss : 0.11515546397052027\n",
            "Acc : 0.5516098737716675 train_loss : 0.11457068929532627\n",
            "Acc : 0.5565025210380554 train_loss : 0.11555370122992567\n",
            "Acc : 0.5613952279090881 train_loss : 0.11576962161116895\n",
            "Acc : 0.5664457082748413 train_loss : 0.11486466553290947\n",
            "Acc : 0.5713383555412292 train_loss : 0.11515974192677633\n",
            "Acc : 0.5763888955116272 train_loss : 0.11433976271253979\n",
            "Acc : 0.5814393758773804 train_loss : 0.11357062899818023\n",
            "Acc : 0.5864899158477783 train_loss : 0.11339570068107066\n",
            "Acc : 0.5915403962135315 train_loss : 0.11275387837888062\n",
            "Acc : 0.5964331030845642 train_loss : 0.11388604008437445\n",
            "Acc : 0.6014835834503174 train_loss : 0.11308180637011105\n",
            "Acc : 0.6062184572219849 train_loss : 0.11689155650340387\n",
            "Acc : 0.6111111044883728 train_loss : 0.1180173601213146\n",
            "Acc : 0.6161616444587708 train_loss : 0.11723491195739517\n",
            "Acc : 0.6212121248245239 train_loss : 0.11649228800088167\n",
            "Acc : 0.6261047720909119 train_loss : 0.11623822123787943\n",
            "Acc : 0.6311553120613098 train_loss : 0.1158576783775462\n",
            "Acc : 0.636205792427063 train_loss : 0.11528585959604243\n",
            "Acc : 0.6412563323974609 train_loss : 0.11550597006873797\n",
            "Acc : 0.6461489796638489 train_loss : 0.11544970827750288\n",
            "Acc : 0.6511995196342468 train_loss : 0.11512265938071121\n",
            "Acc : 0.6559343338012695 train_loss : 0.11681346875389642\n",
            "Acc : 0.6609848737716675 train_loss : 0.11608633263862896\n",
            "Acc : 0.6657196879386902 train_loss : 0.11909732021697199\n",
            "Acc : 0.6707702279090881 train_loss : 0.11835940308454963\n",
            "Acc : 0.6756628751754761 train_loss : 0.11925914209089516\n",
            "Acc : 0.6805555820465088 train_loss : 0.12031280346568266\n",
            "Acc : 0.685606062412262 train_loss : 0.119665645474595\n",
            "Acc : 0.6906565427780151 train_loss : 0.11894236762987838\n",
            "Acc : 0.6957070827484131 train_loss : 0.11851726465060243\n",
            "Acc : 0.7001262903213501 train_loss : 0.12140557183785007\n",
            "Acc : 0.7048611044883728 train_loss : 0.12290431824709538\n",
            "Acc : 0.7099116444587708 train_loss : 0.12236421102644889\n",
            "Acc : 0.7148042917251587 train_loss : 0.12250757694063294\n",
            "Acc : 0.7198547720909119 train_loss : 0.12210853860938344\n",
            "Acc : 0.7249053120613098 train_loss : 0.12151871959053695\n",
            "Acc : 0.729955792427063 train_loss : 0.12082597786294562\n",
            "Acc : 0.7350063323974609 train_loss : 0.12038733254539201\n",
            "Acc : 0.7398989796638489 train_loss : 0.12016569276109038\n",
            "Acc : 0.7447916865348816 train_loss : 0.12113291112706065\n",
            "Acc : 0.7496843338012695 train_loss : 0.12171966289570987\n",
            "Acc : 0.7547348737716675 train_loss : 0.12099298569496329\n",
            "Acc : 0.7597853541374207 train_loss : 0.12055061886001842\n",
            "Acc : 0.7648358345031738 train_loss : 0.11993587427687916\n",
            "Acc : 0.7697285413742065 train_loss : 0.12052862331151001\n",
            "Acc : 0.7744633555412292 train_loss : 0.12077389040794702\n",
            "Acc : 0.779356062412262 train_loss : 0.12108292638852149\n",
            "Acc : 0.7844065427780151 train_loss : 0.12086728855943944\n",
            "Acc : 0.7892992496490479 train_loss : 0.12156522132854604\n",
            "Acc : 0.794349730014801 train_loss : 0.1211004919547122\n",
            "Acc : 0.799400269985199 train_loss : 0.12066483728425659\n",
            "Acc : 0.8044507503509521 train_loss : 0.12010585957632204\n",
            "Acc : 0.8093434572219849 train_loss : 0.1202121414415584\n",
            "Acc : 0.814393937587738 train_loss : 0.120077932474953\n",
            "Acc : 0.8194444179534912 train_loss : 0.11948625278856718\n",
            "Acc : 0.8243371248245239 train_loss : 0.11949142845661705\n",
            "Acc : 0.8293876051902771 train_loss : 0.11903399508446455\n",
            "Acc : 0.8342803120613098 train_loss : 0.11918230854263086\n",
            "Acc : 0.839330792427063 train_loss : 0.11868040087858778\n",
            "Acc : 0.8443813323974609 train_loss : 0.11808810050544494\n",
            "Acc : 0.8491161465644836 train_loss : 0.11933189064634649\n",
            "Acc : 0.8541666865348816 train_loss : 0.11886909507150047\n",
            "Acc : 0.8592171669006348 train_loss : 0.1183705622373375\n",
            "Acc : 0.8641098737716675 train_loss : 0.11840941855983658\n",
            "Acc : 0.8690025210380554 train_loss : 0.11829533990472556\n",
            "Acc : 0.8740530014038086 train_loss : 0.11780544326492501\n",
            "Acc : 0.8791035413742065 train_loss : 0.1174393212107026\n",
            "Acc : 0.8839961886405945 train_loss : 0.11759648899537291\n",
            "Acc : 0.8890467286109924 train_loss : 0.11711797768031586\n",
            "Acc : 0.8940972089767456 train_loss : 0.11663323000797796\n",
            "Acc : 0.8991477489471436 train_loss : 0.11610536000984643\n",
            "Acc : 0.9040403962135315 train_loss : 0.11689780123920722\n",
            "Acc : 0.9090909361839294 train_loss : 0.11643890003912924\n",
            "Acc : 0.9139835834503174 train_loss : 0.11710892416759515\n",
            "Acc : 0.9188762903213501 train_loss : 0.11763635005399183\n",
            "Acc : 0.9239267706871033 train_loss : 0.11735324755371097\n",
            "Acc : 0.9289772510528564 train_loss : 0.11689311448723715\n",
            "Acc : 0.9340277910232544 train_loss : 0.11645024009742477\n",
            "Acc : 0.9389204382896423 train_loss : 0.11664299967959918\n",
            "Acc : 0.943813145160675 train_loss : 0.11611729569635108\n",
            "Acc : 0.9488636255264282 train_loss : 0.11557354722464584\n",
            "Acc : 0.9539141654968262 train_loss : 0.1150776554713957\n",
            "Acc : 0.9589646458625793 train_loss : 0.11473543584887227\n",
            "Acc : 0.9640151262283325 train_loss : 0.11427834672258072\n",
            "Acc : 0.9690656661987305 train_loss : 0.11382152183124652\n",
            "Acc : 0.9741161465644836 train_loss : 0.1134243699037755\n",
            "Acc : 0.9788510203361511 train_loss : 0.11447394683692358\n",
            "Acc : 0.9839015007019043 train_loss : 0.11427128825788245\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.5261750817298889\n",
            "Acc 0.036616161465644836 train_loss 0.6550118625164032\n",
            "Acc 0.05681818351149559 train_loss 0.45035576075315475\n",
            "Acc 0.07512626051902771 train_loss 0.4573447871953249\n",
            "Acc 0.09469696879386902 train_loss 0.45712253004312514\n",
            "Acc 0.11237373948097229 train_loss 0.5310504250228405\n",
            "Acc 0.12941919267177582 train_loss 0.6216491641742843\n",
            "Acc 0.14898990094661713 train_loss 0.5844776788726449\n",
            "Acc 0.1679292917251587 train_loss 0.5656801304883428\n",
            "Acc 0.1862373799085617 train_loss 0.5833992935717106\n",
            "Acc 0.20391413569450378 train_loss 0.6176568086851727\n",
            "Acc 0.22159090638160706 train_loss 0.63613554649055\n",
            "Acc 0.24053029716014862 train_loss 0.6011966575796788\n",
            "Acc 0.2594696879386902 train_loss 0.5747057928570679\n",
            "Acc 0.2790403962135315 train_loss 0.543444939951102\n",
            "Acc 0.29671716690063477 train_loss 0.5700133037753403\n",
            "Acc 0.3156565725803375 train_loss 0.5461864616064465\n",
            "Acc 0.3333333432674408 train_loss 0.5561975575983524\n",
            "Acc 0.35353535413742065 train_loss 0.5275575627425784\n",
            "Acc 0.37310606241226196 train_loss 0.5065673812292516\n",
            "Acc 0.3933080732822418 train_loss 0.4828066368188177\n",
            "Acc 0.41287878155708313 train_loss 0.47963314401832496\n",
            "Acc 0.4318181872367859 train_loss 0.47844203626332077\n",
            "Acc 0.4501262605190277 train_loss 0.48416336098064977\n",
            "Acc 0.467803031206131 train_loss 0.5012020656466484\n",
            "Acc 0.4861111044883728 train_loss 0.5130313104734971\n",
            "Acc 0.5056818127632141 train_loss 0.5040433381994566\n",
            "Acc 0.5239899158477783 train_loss 0.5074656113450017\n",
            "Acc 0.5416666865348816 train_loss 0.5259850289801071\n",
            "Acc 0.5593434572219849 train_loss 0.5324210820098718\n",
            "Acc 0.5770202279090881 train_loss 0.5400616408836457\n",
            "Acc 0.5965909361839294 train_loss 0.5311653062235564\n",
            "Acc 0.6161616444587708 train_loss 0.5230466052889824\n",
            "Acc 0.6332070827484131 train_loss 0.5442296544856885\n",
            "Validating....\n",
            "Acc 0.6515151262283325 train_loss 0.5524620667099953\n",
            "Acc 0.6704545617103577 train_loss 0.5486878303603994\n",
            "Acc 0.6881313323974609 train_loss 0.5604981778038515\n",
            "Acc 0.7064393758773804 train_loss 0.5611433478954592\n",
            "Acc 0.7234848737716675 train_loss 0.5720525198639967\n",
            "Acc 0.7398989796638489 train_loss 0.5911621971055865\n",
            "Acc 0.7582070827484131 train_loss 0.5917242676383112\n",
            "Acc 0.7746211886405945 train_loss 0.6059209986456803\n",
            "Acc 0.7929292917251587 train_loss 0.6086245288336\n",
            "Acc 0.810606062412262 train_loss 0.615406212312254\n",
            "Acc 0.8301767706871033 train_loss 0.608268456823296\n",
            "Acc 0.8491161465644836 train_loss 0.6046322763937971\n",
            "Acc 0.8674242496490479 train_loss 0.6048644665390888\n",
            "Acc 0.8863636255264282 train_loss 0.6020049376723667\n",
            "Acc 0.9053030014038086 train_loss 0.6004359708452711\n",
            "Acc 0.9135100841522217 train_loss 0.6015753839910031\n",
            "F1_score : 0.8401400233372229\n",
            "Epoch completed in 1.4001403212547303 minutes\n",
            "Training completed in 15.973938961823782 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKP1r_H-NOuq"
      },
      "source": [
        "class MakeTestDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        cleaned_tweet = data_row['cleaned_tweet']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                                              cleaned_tweet,\n",
        "                                              add_special_tokens = True,\n",
        "                                              max_length = self.max_token_len,\n",
        "                                              return_token_type_ids = False,\n",
        "                                              padding = \"max_length\",\n",
        "                                              truncation = True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_tensors = 'pt',\n",
        "                                             )\n",
        "        return dict(\n",
        "                    cleaned_tweet = cleaned_tweet,\n",
        "                    input_ids = encoding[\"input_ids\"].flatten(),\n",
        "                    attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "                   )\n",
        "\n",
        "valid_dataset = MakeTestDataset(\n",
        "                               valid_df,\n",
        "                               tokenizer,\n",
        "                               max_token_len = MAX_TOKEN_COUNT\n",
        "                              )\n",
        "test_dataset = MakeTestDataset(\n",
        "                               test,\n",
        "                               tokenizer,\n",
        "                               max_token_len = MAX_TOKEN_COUNT\n",
        "                              )\n",
        "\n",
        "valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                valid_dataset,\n",
        "                                                                num_replicas = xm.xrt_world_size(),\n",
        "                                                                rank = xm.get_ordinal(),\n",
        "                                                                shuffle = False\n",
        "                                                               )\n",
        "\n",
        "test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                test_dataset,\n",
        "                                                                num_replicas = xm.xrt_world_size(),\n",
        "                                                                rank = xm.get_ordinal(),\n",
        "                                                                shuffle = False\n",
        "                                                               )\n",
        "BATCH_SIZE = 32\n",
        "valid_data_loader = DataLoader(\n",
        "                               valid_dataset,\n",
        "                               sampler = valid_sampler,\n",
        "                               batch_size = BATCH_SIZE,\n",
        "                               num_workers = 2\n",
        "                              )\n",
        "test_data_loader = DataLoader(\n",
        "                               test_dataset,\n",
        "                               sampler = test_sampler,\n",
        "                               batch_size = BATCH_SIZE,\n",
        "                               num_workers = 2\n",
        "                              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlvsEYY4WOE"
      },
      "source": [
        "#predict valid set\n",
        "val_preds = []\n",
        "model = model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(valid_data_loader):\n",
        "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs,_ = torch.max(outputs, dim = 1)\n",
        "        val_preds.extend(outputs.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "ndoFEmci8zKT",
        "outputId": "5e5642f3-2414-4895-ac84-9e32cbb46166"
      },
      "source": [
        "#plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(valid_df['label'], val_preds)\n",
        "\n",
        "# calculate the g-mean for each threshold\n",
        "gmeans = np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "# locate the index of the largest g-mean\n",
        "ix = np.argmax(gmeans)\n",
        "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.figure(num = 0, figsize = [6.4, 4.8])\n",
        "plt.plot([0,1], [0,1], linestyle = '--', label = 'No Skill')\n",
        "plt.plot(fpr, tpr, marker = '.', label = 'Bert')\n",
        "plt.scatter(fpr[ix], tpr[ix], marker = 'o', color = 'black', label = 'Best')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Threshold=0.026218, G-Mean=0.927\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 460.8x345.6 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uufGSnSU6Cva"
      },
      "source": [
        "val_prediction = [int(pred > thresholds[ix]) for pred in val_preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXh4Uypp6iTi",
        "outputId": "f91c433a-9cbd-431c-e843-2c15b99aafb2"
      },
      "source": [
        "print(classification_report(valid_df['label'], val_prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      1152\n",
            "           1       0.80      0.94      0.87       432\n",
            "\n",
            "    accuracy                           0.92      1584\n",
            "   macro avg       0.89      0.93      0.90      1584\n",
            "weighted avg       0.93      0.92      0.92      1584\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQwDuTYrQs6I"
      },
      "source": [
        "#predict test set\n",
        "preds = []\n",
        "model = model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_data_loader):\n",
        "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs,_ = torch.max(outputs, dim = 1)\n",
        "        preds.extend(outputs.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XGgZbF_RLE_"
      },
      "source": [
        "prediction = [int(pred > thresholds[ix]) for pred in preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pd6m4GmNqOe"
      },
      "source": [
        "ss['label'] = prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXtgDKReDYTe",
        "outputId": "4bd78b06-a879-4112-eeaa-26bfe348da1d"
      },
      "source": [
        "ss['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1317\n",
              "1     636\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DXLRzmykN0b0",
        "outputId": "1b669831-a263-4fad-f8ff-5216739c3616"
      },
      "source": [
        "from google.colab import files\n",
        "ss.to_csv('sub8.csv', index = False)\n",
        "files.download('sub8.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_480fe52b-feb7-4d85-93d5-cce3107962f4\", \"sub8.csv\", 13680)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}