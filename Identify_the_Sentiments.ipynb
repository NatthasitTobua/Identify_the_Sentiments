{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identify the Sentiments",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXeySQl/+Ed13eegGP+G6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b09f8c8aa0b84115af7138e1f03e3157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3145c04cab8f4f409423bd59619b9469",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_895009880e01420b8c3bfab3744bd26c",
              "IPY_MODEL_7ce1ba0a90844203994b0a5d3c8feb3c",
              "IPY_MODEL_189aa6c5bade412daf8958652b37a069"
            ]
          }
        },
        "3145c04cab8f4f409423bd59619b9469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "895009880e01420b8c3bfab3744bd26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d394e8517a2d44d8832806bf2f18d605",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d518ec36e1ba4366809ae079560aea46"
          }
        },
        "7ce1ba0a90844203994b0a5d3c8feb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03deaee69cf3479aa33e2b2a2da225d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9edd53537f61455e99125e1618d2eaa3"
          }
        },
        "189aa6c5bade412daf8958652b37a069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8818e5b0942b464587ea857ead3cdf04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 711kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_526c1a19ee494a3883a32c014806b82d"
          }
        },
        "d394e8517a2d44d8832806bf2f18d605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d518ec36e1ba4366809ae079560aea46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03deaee69cf3479aa33e2b2a2da225d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9edd53537f61455e99125e1618d2eaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8818e5b0942b464587ea857ead3cdf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "526c1a19ee494a3883a32c014806b82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ea0b92b402d4f8388f9037f8e1288db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b51f65ab2d954ea4b9c51207f84b67bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_51e364dbcbc04f6cb7a00239d4b33a37",
              "IPY_MODEL_8d5b31205138466db475d6867d6ce078",
              "IPY_MODEL_b3baf31f038b4755b7c46853388185bf"
            ]
          }
        },
        "b51f65ab2d954ea4b9c51207f84b67bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51e364dbcbc04f6cb7a00239d4b33a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_628a15c5acb6420fb5e24a095ee92ce1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4700890b98104dd18b7a6cea24c103c3"
          }
        },
        "8d5b31205138466db475d6867d6ce078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a73b265d5c0f41279401bb7f82e0ee46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1b9b226a9c6494a94e7726a37a94f5e"
          }
        },
        "b3baf31f038b4755b7c46853388185bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_487458c2306143ed8e3c9cf33e0a0107",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 1.16MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f58bdfd9165c4f18b480a81fb30de184"
          }
        },
        "628a15c5acb6420fb5e24a095ee92ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4700890b98104dd18b7a6cea24c103c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a73b265d5c0f41279401bb7f82e0ee46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1b9b226a9c6494a94e7726a37a94f5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "487458c2306143ed8e3c9cf33e0a0107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f58bdfd9165c4f18b480a81fb30de184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37eb18b38b844121908ddb17db9158c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb2fa330cf5b4f7a957d8eb548f002a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a205d922bd3445ccb236e35ab8515fcb",
              "IPY_MODEL_2a8d387850344d2a87893d5d2c0b398c",
              "IPY_MODEL_163bcdcec1ef4712b6d96a31e324a18d"
            ]
          }
        },
        "cb2fa330cf5b4f7a957d8eb548f002a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a205d922bd3445ccb236e35ab8515fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3caaa01a4fc845309797a685346e99e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27eb396e90cc40308bcc27beb78de5a0"
          }
        },
        "2a8d387850344d2a87893d5d2c0b398c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9983528bc3424a40a3e5136529074d2a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38f0322d8da9445388bf7f975a8f2b2e"
          }
        },
        "163bcdcec1ef4712b6d96a31e324a18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_406cbbd9cda64f3fa673d18f46b3db5f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 4.75MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bc68be487a94daca99a5fed37c205f1"
          }
        },
        "3caaa01a4fc845309797a685346e99e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27eb396e90cc40308bcc27beb78de5a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9983528bc3424a40a3e5136529074d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38f0322d8da9445388bf7f975a8f2b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "406cbbd9cda64f3fa673d18f46b3db5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bc68be487a94daca99a5fed37c205f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ed31a7caf1e413d9d56b22eeac74aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1f26f8dcbb3d44d8ae2b63a8ba50130a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_06aeb05490a648c2ad29efc2aca24940",
              "IPY_MODEL_038424b5993243d688b15c0c8f714cde",
              "IPY_MODEL_2926b79ab81645fb9a8919f1b488316e"
            ]
          }
        },
        "1f26f8dcbb3d44d8ae2b63a8ba50130a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06aeb05490a648c2ad29efc2aca24940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54808e84f2dc4b009c02955c7e590358",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efaf979e8ef441be97e71b78cc1cf64c"
          }
        },
        "038424b5993243d688b15c0c8f714cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01045c9f8b354bc7b662b93fd3aebb99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e78ab16142f4603a151dfdbd65d9025"
          }
        },
        "2926b79ab81645fb9a8919f1b488316e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_706e91da95a248a4b5bea45c805b0152",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 2.79kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_424dd373bbfb434e8252abca0ce3292b"
          }
        },
        "54808e84f2dc4b009c02955c7e590358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efaf979e8ef441be97e71b78cc1cf64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01045c9f8b354bc7b662b93fd3aebb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e78ab16142f4603a151dfdbd65d9025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "706e91da95a248a4b5bea45c805b0152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "424dd373bbfb434e8252abca0ce3292b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a31a69d656d34ed7804210157844a190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eefb636a60c044dfa3c9e940020ef73f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2238dd964a548279fd90d3d4e036b0b",
              "IPY_MODEL_4f42b01ba437411f9032a6146c5e25fd",
              "IPY_MODEL_195528a407fe46ce807e4b04ce63a496"
            ]
          }
        },
        "eefb636a60c044dfa3c9e940020ef73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2238dd964a548279fd90d3d4e036b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71f9088a950f4b56a7d96525951d4602",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f71e1a9fdd45404ca3a2b834f5f4e247"
          }
        },
        "4f42b01ba437411f9032a6146c5e25fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02cddbb288684c549f7dcf0ec3edea41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45d40e727b7043c69b469e011de73ce9"
          }
        },
        "195528a407fe46ce807e4b04ce63a496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56e43c34c4f546fa8da00c8685ae50fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 478M/478M [00:18&lt;00:00, 37.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f2e4da303194d2fa3123a24f59b94d6"
          }
        },
        "71f9088a950f4b56a7d96525951d4602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f71e1a9fdd45404ca3a2b834f5f4e247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02cddbb288684c549f7dcf0ec3edea41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45d40e727b7043c69b469e011de73ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56e43c34c4f546fa8da00c8685ae50fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f2e4da303194d2fa3123a24f59b94d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatthasitTobua/Identify_the_Sentiments/blob/main/Identify_the_Sentiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEWJiSiDU2I",
        "outputId": "45dbb83f-9d52-4949-eed6-9f1899c354a7"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rzWwOvDpc6",
        "outputId": "f96579ea-c60e-4e85-eba4-a76b218d9e6c"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.18-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting ruamel.yaml==0.17.16\n",
            "  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Collecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.18 pyyaml-5.4.1 ruamel.yaml-0.17.16 ruamel.yaml.clib-0.2.6 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.2\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 28 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.284 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk-82GQyDsxS",
        "outputId": "af983a41-e5d0-4026-a8ba-337c01356842"
      },
      "source": [
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import warnings\n",
        "\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, RobertaModel, RobertaTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, roc_curve"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5TAMeNQ5019"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNrTcpbBD1j5"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/test.csv\")\n",
        "ss = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/sample_submission.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sksxRsToGCnQ",
        "outputId": "6e9ec70a-1dff-4d7f-b1f2-17193b01c190"
      },
      "source": [
        "print('train data has', train.shape[0], 'rows')\n",
        "print('test data has', test.shape[0], 'rows')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data has 7920 rows\n",
            "test data has 1953 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "NW2FAQikGe4X",
        "outputId": "f840e5f6-8b34-4c02-8ddf-d33d10869929"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
              "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
              "2   3      0  We love this! Would you go? #talk #makememorie..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oeCb_NvGokC"
      },
      "source": [
        "**EXPLORING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "8VaizDhxHfnw",
        "outputId": "ecd4bb39-0f67-401c-e373-210998d29464"
      },
      "source": [
        "#see how many labels in each category\n",
        "sns.countplot(train['label'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbdb3b99090>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OUeXHepiGw0H",
        "outputId": "e3808d47-f85e-43c0-b1c8-5629db43c414"
      },
      "source": [
        "#plot lenght of tweet\n",
        "sns.histplot(train['tweet'].apply(lambda x: len(x.split())), bins = 30, color = 'blue')\n",
        "sns.histplot(test['tweet'].apply(lambda x: len(x.split())), bins = 30, color = 'red')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbe78303290>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekSWtpOTHMrf",
        "outputId": "cc321114-44b1-47d4-90eb-7320460e935d"
      },
      "source": [
        "#average length of text\n",
        "print('train average length :',train['tweet'].apply(lambda x: len(x.split())).mean())\n",
        "print('test average length :',test['tweet'].apply(lambda x: len(x.split())).mean())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train average length : 16.843434343434343\n",
            "test average length : 16.872503840245777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Rv3yGeLKh2"
      },
      "source": [
        "def cleaned_tweet(doc):\n",
        "  # remove links and non-ASCII characters\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  doc = url.sub('',doc)\n",
        "\n",
        "  # lower case and remove special characters\\whitespaces\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "  \n",
        "  return doc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kuTQrK7LTqn"
      },
      "source": [
        "train['cleaned_tweet'] = train['tweet'].apply(lambda x : cleaned_tweet(x))\n",
        "test['cleaned_tweet'] = test['tweet'].apply(lambda x : cleaned_tweet(x))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "qbAqu6ZdLwmt",
        "outputId": "fd4e8c48-40a9-4f42-8e40-3d1c0e49e6ba"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
              "      <td>fingerprint pregnancy test  android apps beaut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
              "      <td>finally a transparant silicon case  thanks to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
              "      <td>we love this would you go talk makememories un...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                      cleaned_tweet\n",
              "0   1  ...  fingerprint pregnancy test  android apps beaut...\n",
              "1   2  ...  finally a transparant silicon case  thanks to ...\n",
              "2   3  ...  we love this would you go talk makememories un...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExzBfSMHyGqd",
        "outputId": "08548cd3-f8e6-4987-8a50-70c9e995fa2e"
      },
      "source": [
        "np.argmax(train['tweet'].apply(lambda x: len(x.split())))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1794"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzOQDaA8ngJa",
        "outputId": "9d54c99c-082d-4f9d-a643-ab365c8904a2"
      },
      "source": [
        "print(train['tweet'][1794])\n",
        "print(train['cleaned_tweet'][1794])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Me lo dices o me lo preguntas #Apple ? Ya mi teléfono funciona solo conectado a la electricidad. Do you tell me or do yo ask me #apple ? Now my iPhone works only connected.Have you asked yourself if you were in a bad economic situation could you pay a new phone? #apple https://twitter.com/el_interes/status/944881090115899392 …\n",
            "me lo dices o me lo preguntas apple  ya mi telfono funciona solo conectado a la electricidad do you tell me or do yo ask me apple  now my iphone works only connectedhave you asked yourself if you were in a bad economic situation could you pay a new phone apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDLEvCLhPCxF"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXIkjmklPCZ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "b09f8c8aa0b84115af7138e1f03e3157",
            "3145c04cab8f4f409423bd59619b9469",
            "895009880e01420b8c3bfab3744bd26c",
            "7ce1ba0a90844203994b0a5d3c8feb3c",
            "189aa6c5bade412daf8958652b37a069",
            "d394e8517a2d44d8832806bf2f18d605",
            "d518ec36e1ba4366809ae079560aea46",
            "03deaee69cf3479aa33e2b2a2da225d8",
            "9edd53537f61455e99125e1618d2eaa3",
            "8818e5b0942b464587ea857ead3cdf04",
            "526c1a19ee494a3883a32c014806b82d",
            "1ea0b92b402d4f8388f9037f8e1288db",
            "b51f65ab2d954ea4b9c51207f84b67bc",
            "51e364dbcbc04f6cb7a00239d4b33a37",
            "8d5b31205138466db475d6867d6ce078",
            "b3baf31f038b4755b7c46853388185bf",
            "628a15c5acb6420fb5e24a095ee92ce1",
            "4700890b98104dd18b7a6cea24c103c3",
            "a73b265d5c0f41279401bb7f82e0ee46",
            "b1b9b226a9c6494a94e7726a37a94f5e",
            "487458c2306143ed8e3c9cf33e0a0107",
            "f58bdfd9165c4f18b480a81fb30de184",
            "37eb18b38b844121908ddb17db9158c5",
            "cb2fa330cf5b4f7a957d8eb548f002a9",
            "a205d922bd3445ccb236e35ab8515fcb",
            "2a8d387850344d2a87893d5d2c0b398c",
            "163bcdcec1ef4712b6d96a31e324a18d",
            "3caaa01a4fc845309797a685346e99e6",
            "27eb396e90cc40308bcc27beb78de5a0",
            "9983528bc3424a40a3e5136529074d2a",
            "38f0322d8da9445388bf7f975a8f2b2e",
            "406cbbd9cda64f3fa673d18f46b3db5f",
            "8bc68be487a94daca99a5fed37c205f1",
            "8ed31a7caf1e413d9d56b22eeac74aa1",
            "1f26f8dcbb3d44d8ae2b63a8ba50130a",
            "06aeb05490a648c2ad29efc2aca24940",
            "038424b5993243d688b15c0c8f714cde",
            "2926b79ab81645fb9a8919f1b488316e",
            "54808e84f2dc4b009c02955c7e590358",
            "efaf979e8ef441be97e71b78cc1cf64c",
            "01045c9f8b354bc7b662b93fd3aebb99",
            "0e78ab16142f4603a151dfdbd65d9025",
            "706e91da95a248a4b5bea45c805b0152",
            "424dd373bbfb434e8252abca0ce3292b"
          ]
        },
        "outputId": "eefac545-7cf3-498c-b3d6-22039c00eae2"
      },
      "source": [
        "#use pretrained model \n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b09f8c8aa0b84115af7138e1f03e3157",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea0b92b402d4f8388f9037f8e1288db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37eb18b38b844121908ddb17db9158c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ed31a7caf1e413d9d56b22eeac74aa1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "aEDOinsPPRC0",
        "outputId": "9b9f3310-d1ec-420f-9e30-02d5a3c4be1b"
      },
      "source": [
        "#plot the distribution after tokenize\n",
        "token_counts = []\n",
        "for _, row in train.iterrows():\n",
        "    token_count = len(tokenizer.encode(\n",
        "                                       row[\"cleaned_tweet\"],\n",
        "                                       max_length = 128,\n",
        "                                       truncation = True\n",
        "                                      )\n",
        "                     )\n",
        "    token_counts.append(token_count)\n",
        "sns.distplot(token_counts)\n",
        "plt.xlim([0, 128])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 128.0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aRvGsofPwKs"
      },
      "source": [
        "#most tweets contain less than 128 tokens, then stick with this number\n",
        "MAX_TOKEN_COUNT = 128"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIn4WB48Pzis",
        "outputId": "d13f84a4-64e8-446f-ad58-d40a048d9a39"
      },
      "source": [
        "#split the data into train set and valid set\n",
        "train_df, valid_df = train_test_split(train, test_size = 0.2, random_state = 42)\n",
        "print('train df has', train_df.shape[0], 'rows')\n",
        "print('valid df has', valid_df.shape[0], 'rows')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train df has 6336 rows\n",
            "valid df has 1584 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3CEqyXGQNI0"
      },
      "source": [
        "class MakeDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "        self.labels = self.data['label']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        cleaned_tweet = data_row['cleaned_tweet']\n",
        "        labels = data_row['label']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                                              cleaned_tweet,\n",
        "                                              add_special_tokens = True,\n",
        "                                              max_length = self.max_token_len,\n",
        "                                              return_token_type_ids = False,\n",
        "                                              padding = \"max_length\",\n",
        "                                              truncation = True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_tensors = 'pt',\n",
        "                                             )\n",
        "        return dict(\n",
        "                    cleaned_tweet = cleaned_tweet,\n",
        "                    input_ids = encoding[\"input_ids\"].flatten(),\n",
        "                    attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "                    labels = labels\n",
        "                   )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgXxa5JkR5Vr"
      },
      "source": [
        "train_dataset = MakeDataset(\n",
        "                            train_df,\n",
        "                            tokenizer,\n",
        "                            max_token_len = MAX_TOKEN_COUNT\n",
        "                           )\n",
        "valid_dataset = MakeDataset(\n",
        "                            valid_df,\n",
        "                            tokenizer,\n",
        "                            max_token_len = MAX_TOKEN_COUNT\n",
        "                           )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jp2N-exHogm",
        "outputId": "2d448283-dacd-4bf7-9134-a659ec2396f6"
      },
      "source": [
        "#see how output look like\n",
        "train_dataset[987]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'cleaned_tweet': 'hey apple shopcrabtree you suck spent  to have a power cable last me  year and  months',\n",
              " 'input_ids': tensor([    0, 12229, 15162,  2792,  8344,   873, 21512,    47, 23829,  1240,\n",
              "          1437,     7,    33,    10,   476,  6129,    94,   162,  1437,    76,\n",
              "             8,  1437,   377,     2,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'labels': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "a31a69d656d34ed7804210157844a190",
            "eefb636a60c044dfa3c9e940020ef73f",
            "a2238dd964a548279fd90d3d4e036b0b",
            "4f42b01ba437411f9032a6146c5e25fd",
            "195528a407fe46ce807e4b04ce63a496",
            "71f9088a950f4b56a7d96525951d4602",
            "f71e1a9fdd45404ca3a2b834f5f4e247",
            "02cddbb288684c549f7dcf0ec3edea41",
            "45d40e727b7043c69b469e011de73ce9",
            "56e43c34c4f546fa8da00c8685ae50fb",
            "0f2e4da303194d2fa3123a24f59b94d6"
          ]
        },
        "id": "0dpHuK6ESG4L",
        "outputId": "01c0a07f-b329-49c5-960e-d59e9fb99536"
      },
      "source": [
        "#define model from pretrained model\n",
        "class RobertaClass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.bert_model = RobertaModel.from_pretrained(model_name, return_dict = True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert_model(input_ids, attention_mask)\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        output = torch.sigmoid(output)\n",
        "        return output\n",
        "    \n",
        "model = RobertaClass()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a31a69d656d34ed7804210157844a190",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKU6cLvLkHIG"
      },
      "source": [
        "#calculate weight for imbalanced data\n",
        "train_label_count = train_df['label'].value_counts().values\n",
        "neg_label_count = [sum(train_label_count) - count for count in train_label_count]\n",
        "pos_weights = neg_label_count/train_label_count.min()\n",
        "pos_weights = torch.tensor(pos_weights)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rBgJlDLhqKf"
      },
      "source": [
        "#define train and validation loop\n",
        "def _run(model):\n",
        "\n",
        "    def loss_fn(outputs, labels):\n",
        "        return nn.BCELoss(weight = pos_weights)(outputs, labels)\n",
        "    \n",
        "    def train_fn(train_dataloader, model, optimizer, scheduler, device):\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            \n",
        "            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = loss_fn(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            preds = (outputs > 0.5).float()\n",
        "            \n",
        "            correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            acc = correct_predictions.double() / len(train_df)\n",
        "\n",
        "            xm.master_print(f'Acc : {acc} train_loss : {np.mean(losses)}')\n",
        "\n",
        "    def valid_fn(valid_dataloader, model, device):\n",
        "\n",
        "        model.eval()\n",
        "        global y_pred \n",
        "        global y_valid \n",
        "        y_pred = []\n",
        "        y_valid = []\n",
        "\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(valid_dataloader):\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.float)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = loss_fn(outputs, labels.unsqueeze(1))\n",
        "\n",
        "            preds = (outputs > 0.5).float()\n",
        "            \n",
        "            correct_predictions += torch.sum(preds == labels.unsqueeze(1))\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred.extend(preds.cpu().squeeze().numpy())\n",
        "            y_valid.extend(labels.cpu().numpy())\n",
        "\n",
        "            acc = correct_predictions.double() / len(valid_df)\n",
        "            \n",
        "            xm.master_print(f'Acc {acc} train_loss {np.mean(losses)}')\n",
        "    \n",
        "    EPOCHS = 10\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                    train_dataset,\n",
        "                                                                    num_replicas = xm.xrt_world_size(),\n",
        "                                                                    rank = xm.get_ordinal(),\n",
        "                                                                    shuffle = True\n",
        "                                                                   )\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                    valid_dataset,\n",
        "                                                                    num_replicas = xm.xrt_world_size(),\n",
        "                                                                    rank = xm.get_ordinal(),\n",
        "                                                                    shuffle = False\n",
        "                                                                   )\n",
        "    \n",
        "    train_data_loader = DataLoader(\n",
        "                                   train_dataset,\n",
        "                                   sampler = train_sampler,\n",
        "                                   batch_size = BATCH_SIZE,\n",
        "                                   num_workers = 2\n",
        "                                  )\n",
        "    valid_data_loader = DataLoader(\n",
        "                                   valid_dataset,\n",
        "                                   sampler = valid_sampler,\n",
        "                                   batch_size = BATCH_SIZE,\n",
        "                                   num_workers = 2\n",
        "                                  )\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "    model = model.to(device)\n",
        "\n",
        "    lr = 0.4 * 2e-05 * xm.xrt_world_size()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr)\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
        "    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
        "    num_train_steps = int(len(train_dataset) / BATCH_SIZE * EPOCHS)\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                                                   optimizer,\n",
        "                                                   num_warmup_steps = 0,\n",
        "                                                   num_training_steps = num_train_steps\n",
        "                                                  )\n",
        "\n",
        "    train_begin = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
        "\n",
        "        start = time.time()\n",
        "        print('*'*15)\n",
        "        print(f'EPOCH: {epoch+1}')\n",
        "        print('*'*15)\n",
        "\n",
        "        print('Training.....')\n",
        "        train_fn( \n",
        "                 train_dataloader = para_loader.per_device_loader(device),\n",
        "                 model = model, \n",
        "                 optimizer = optimizer,\n",
        "                 scheduler = lr_scheduler,\n",
        "                 device = device\n",
        "                )\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "          para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
        "            \n",
        "          print('Validating....')\n",
        "          valid_fn( \n",
        "                 valid_dataloader = para_loader.per_device_loader(device),\n",
        "                 model = model,\n",
        "                 device = device\n",
        "                )\n",
        "        \n",
        "        print(f'F1_score : {f1_score(y_valid, y_pred)}')\n",
        "\n",
        "        print(f'Epoch completed in {(time.time() - start)/60} minutes')\n",
        "    print(f'Training completed in {(time.time() - train_begin)/60} minutes')\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5EVU37bm-Qs",
        "outputId": "0d9f8d6b-7cf4-4805-d8c2-efc445a8c38f"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    a = _run(model)\n",
        "\n",
        "FLAGS={}\n",
        "xmp.spawn(_mp_fn, args = (FLAGS, ), nprocs = 1, start_method = 'fork')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train_steps = 1980, world_size=1\n",
            "***************\n",
            "EPOCH: 1\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.002998737385496497 train_loss : 1.3464545011520386\n",
            "Acc : 0.006786616053432226 train_loss : 1.3252769112586975\n",
            "Acc : 0.010416666977107525 train_loss : 1.3248448371887207\n",
            "Acc : 0.01499368716031313 train_loss : 1.3005661368370056\n",
            "Acc : 0.018308080732822418 train_loss : 1.3022625923156739\n",
            "Acc : 0.02209595963358879 train_loss : 1.2944447596867878\n",
            "Acc : 0.026199495419859886 train_loss : 1.2840123176574707\n",
            "Acc : 0.030145201832056046 train_loss : 1.2792903929948807\n",
            "Acc : 0.03393308073282242 train_loss : 1.273453089925978\n",
            "Acc : 0.03787878900766373 train_loss : 1.261142098903656\n",
            "Acc : 0.041508838534355164 train_loss : 1.2550340349023992\n",
            "Acc : 0.04482323303818703 train_loss : 1.2568765083948772\n",
            "Acc : 0.048453282564878464 train_loss : 1.2472941692058856\n",
            "Acc : 0.052241161465644836 train_loss : 1.233756763594491\n",
            "Acc : 0.05602904036641121 train_loss : 1.219938071568807\n",
            "Acc : 0.05950126424431801 train_loss : 1.2159420102834702\n",
            "Acc : 0.0642361119389534 train_loss : 1.1821443964453304\n",
            "Acc : 0.06739267706871033 train_loss : 1.1869614389207628\n",
            "Acc : 0.07149621099233627 train_loss : 1.1701187491416931\n",
            "Acc : 0.07512626051902771 train_loss : 1.1647594839334487\n",
            "Acc : 0.07922979444265366 train_loss : 1.1543596869423276\n",
            "Acc : 0.08301767706871033 train_loss : 1.1487557996403088\n",
            "Acc : 0.08743686974048615 train_loss : 1.1251142413719841\n",
            "Acc : 0.09138257801532745 train_loss : 1.1136562849084537\n",
            "Acc : 0.09485479444265366 train_loss : 1.1124045109748841\n",
            "Acc : 0.09943182021379471 train_loss : 1.0893809726605048\n",
            "Acc : 0.10290403664112091 train_loss : 1.086857749356164\n",
            "Acc : 0.10653409361839294 train_loss : 1.081177877528327\n",
            "Acc : 0.109375 train_loss : 1.087711478101796\n",
            "Acc : 0.11379419267177582 train_loss : 1.0745971163113912\n",
            "Acc : 0.1180555522441864 train_loss : 1.0636145049525845\n",
            "Acc : 0.12184343487024307 train_loss : 1.0572152622044086\n",
            "Acc : 0.12531565129756927 train_loss : 1.0503153295227976\n",
            "Acc : 0.12941919267177582 train_loss : 1.0440181581413044\n",
            "Acc : 0.1336805522441864 train_loss : 1.030940089906965\n",
            "Acc : 0.13809974491596222 train_loss : 1.0177657935354445\n",
            "Acc : 0.14141413569450378 train_loss : 1.0138514557400264\n",
            "Acc : 0.14472854137420654 train_loss : 1.0088114267901371\n",
            "Acc : 0.14835858345031738 train_loss : 1.000911017258962\n",
            "Acc : 0.15135732293128967 train_loss : 1.0095374539494515\n",
            "Acc : 0.1557765156030655 train_loss : 0.9962570914408055\n",
            "Acc : 0.15956439077854156 train_loss : 0.9864939905348278\n",
            "Acc : 0.16256313025951385 train_loss : 0.9862375217814778\n",
            "Acc : 0.1661931872367859 train_loss : 0.9796755747361616\n",
            "Acc : 0.16998106241226196 train_loss : 0.9744411283069186\n",
            "Acc : 0.17424242198467255 train_loss : 0.9632291120031605\n",
            "Acc : 0.17818813025951385 train_loss : 0.9545629899552528\n",
            "Acc : 0.18213383853435516 train_loss : 0.947181511670351\n",
            "Acc : 0.1857638955116272 train_loss : 0.9407599677844923\n",
            "Acc : 0.18892045319080353 train_loss : 0.9384214496612548\n",
            "Acc : 0.19302399456501007 train_loss : 0.9307680971482221\n",
            "Acc : 0.19681186974048615 train_loss : 0.9264605412116418\n",
            "Acc : 0.19996842741966248 train_loss : 0.9258801667195447\n",
            "Acc : 0.203125 train_loss : 0.9227254942611411\n",
            "Acc : 0.20722854137420654 train_loss : 0.9143178360028701\n",
            "Acc : 0.21117424964904785 train_loss : 0.9072193444839546\n",
            "Acc : 0.21511994302272797 train_loss : 0.900833818473314\n",
            "Acc : 0.21890783309936523 train_loss : 0.8950912865071461\n",
            "Acc : 0.22316919267177582 train_loss : 0.8879532404875351\n",
            "Acc : 0.2274305522441864 train_loss : 0.8839937642216682\n",
            "Acc : 0.23184974491596222 train_loss : 0.8785922395401313\n",
            "Acc : 0.23642677068710327 train_loss : 0.8734449402939889\n",
            "Acc : 0.24068813025951385 train_loss : 0.8688803300971076\n",
            "Acc : 0.2443181872367859 train_loss : 0.8667119494639337\n",
            "Acc : 0.2487373799085617 train_loss : 0.8632877684556521\n",
            "Acc : 0.25363004207611084 train_loss : 0.8553683220437078\n",
            "Acc : 0.25852271914482117 train_loss : 0.8472351369573109\n",
            "Acc : 0.26231059432029724 train_loss : 0.8460427995990304\n",
            "Acc : 0.2668876349925995 train_loss : 0.8398819142493649\n",
            "Acc : 0.2713068127632141 train_loss : 0.835776275396347\n",
            "Acc : 0.27588382363319397 train_loss : 0.8306682781434395\n",
            "Acc : 0.28061869740486145 train_loss : 0.826670909093486\n",
            "Acc : 0.2850378751754761 train_loss : 0.8228592407213499\n",
            "Acc : 0.2889835834503174 train_loss : 0.8258827433392808\n",
            "Acc : 0.29356059432029724 train_loss : 0.8230656480789185\n",
            "Acc : 0.297821968793869 train_loss : 0.8201636107344377\n",
            "Acc : 0.3023989796638489 train_loss : 0.8156157917790599\n",
            "Acc : 0.3069760203361511 train_loss : 0.8105635963953458\n",
            "Acc : 0.3117108643054962 train_loss : 0.8044415859481956\n",
            "Acc : 0.3164457082748413 train_loss : 0.7989091262221336\n",
            "Acc : 0.32102271914482117 train_loss : 0.7936804081186836\n",
            "Acc : 0.32575756311416626 train_loss : 0.7908989743488591\n",
            "Acc : 0.3303346037864685 train_loss : 0.7850177076207586\n",
            "Acc : 0.33475378155708313 train_loss : 0.7813061010979471\n",
            "Acc : 0.3398042917251587 train_loss : 0.7748472797520021\n",
            "Acc : 0.3445391356945038 train_loss : 0.7732330278255218\n",
            "Acc : 0.3492739796638489 train_loss : 0.7681490712124726\n",
            "Acc : 0.3538510203361511 train_loss : 0.7648874231698838\n",
            "Acc : 0.3585858643054962 train_loss : 0.7603873886754003\n",
            "Acc : 0.3633207082748413 train_loss : 0.7553617922796143\n",
            "Acc : 0.3675820827484131 train_loss : 0.7549597950099589\n",
            "Acc : 0.3723169267177582 train_loss : 0.7501428475522477\n",
            "Acc : 0.37689393758773804 train_loss : 0.749498492767734\n",
            "Acc : 0.38178661465644836 train_loss : 0.7444592639803886\n",
            "Acc : 0.3858901560306549 train_loss : 0.745176268408173\n",
            "Acc : 0.39046716690063477 train_loss : 0.7425834780248503\n",
            "Acc : 0.3950441777706146 train_loss : 0.7403832792314058\n",
            "Acc : 0.3997790515422821 train_loss : 0.7357460018627497\n",
            "Acc : 0.4045138955116272 train_loss : 0.7314481065429822\n",
            "Acc : 0.4094065725803375 train_loss : 0.7259384885430336\n",
            "Acc : 0.41429924964904785 train_loss : 0.7212329167540711\n",
            "Acc : 0.41856059432029724 train_loss : 0.7207593020855212\n",
            "Acc : 0.42297980189323425 train_loss : 0.7196772332909038\n",
            "Acc : 0.4275568127632141 train_loss : 0.7178702254134876\n",
            "Acc : 0.43213382363319397 train_loss : 0.7153227644307273\n",
            "Acc : 0.4362373650074005 train_loss : 0.7160901671872949\n",
            "Acc : 0.4401830732822418 train_loss : 0.7182157020145488\n",
            "Acc : 0.4449179172515869 train_loss : 0.7142606810839088\n",
            "Acc : 0.4491792917251587 train_loss : 0.7130903243471723\n",
            "Acc : 0.4535984992980957 train_loss : 0.711404140158133\n",
            "Acc : 0.4583333432674408 train_loss : 0.7080295955812609\n",
            "Acc : 0.4627525210380554 train_loss : 0.706249824059861\n",
            "Acc : 0.46764519810676575 train_loss : 0.7024593260963407\n",
            "Acc : 0.47238004207611084 train_loss : 0.6983466484306151\n",
            "Acc : 0.4769570827484131 train_loss : 0.6994028284497883\n",
            "Acc : 0.4813762605190277 train_loss : 0.697643231600523\n",
            "Acc : 0.48626893758773804 train_loss : 0.6941565235710552\n",
            "Acc : 0.49068814516067505 train_loss : 0.693913613461842\n",
            "Acc : 0.4952651560306549 train_loss : 0.6918880294601456\n",
            "Acc : 0.5 train_loss : 0.6897723380476236\n",
            "Acc : 0.504419207572937 train_loss : 0.6879300333497939\n",
            "Acc : 0.5086805820465088 train_loss : 0.6901708876744646\n",
            "Acc : 0.513099730014801 train_loss : 0.6890443865118957\n",
            "Acc : 0.5179924368858337 train_loss : 0.685506577333135\n",
            "Acc : 0.5225694179534912 train_loss : 0.6823741372823715\n",
            "Acc : 0.5266729593276978 train_loss : 0.6823746239145597\n",
            "Acc : 0.5307765007019043 train_loss : 0.6827026135104848\n",
            "Acc : 0.5355113744735718 train_loss : 0.6801836345111951\n",
            "Acc : 0.5397727489471436 train_loss : 0.680556380356005\n",
            "Acc : 0.5445075631141663 train_loss : 0.6786099861447628\n",
            "Acc : 0.5486111044883728 train_loss : 0.6781862720505882\n",
            "Acc : 0.5528724789619446 train_loss : 0.6771335201055715\n",
            "Acc : 0.5574495196342468 train_loss : 0.6754395985289624\n",
            "Acc : 0.5617108345031738 train_loss : 0.6759587838800986\n",
            "Acc : 0.5662878751754761 train_loss : 0.6734968599345948\n",
            "Acc : 0.5711805820465088 train_loss : 0.6708941379671588\n",
            "Acc : 0.5759153962135315 train_loss : 0.6685144067028143\n",
            "Acc : 0.5808081030845642 train_loss : 0.6653414570550987\n",
            "Acc : 0.5853850841522217 train_loss : 0.6645497944500807\n",
            "Acc : 0.5896464586257935 train_loss : 0.6636143504508905\n",
            "Acc : 0.5932765007019043 train_loss : 0.6674209071177963\n",
            "Acc : 0.5976957082748413 train_loss : 0.6668585508432187\n",
            "Acc : 0.6024305820465088 train_loss : 0.6645652600726881\n",
            "Acc : 0.6070075631141663 train_loss : 0.6629244328166047\n",
            "Acc : 0.6117424368858337 train_loss : 0.6610569136923757\n",
            "Acc : 0.6160038113594055 train_loss : 0.66031953090266\n",
            "Acc : 0.6207386255264282 train_loss : 0.6578655669681069\n",
            "Acc : 0.625 train_loss : 0.6578002605285194\n",
            "Acc : 0.6298926472663879 train_loss : 0.6554330674393866\n",
            "Acc : 0.6339961886405945 train_loss : 0.6549003774921099\n",
            "Acc : 0.6385732293128967 train_loss : 0.6536057578609479\n",
            "Acc : 0.6434659361839294 train_loss : 0.6509753168609581\n",
            "Acc : 0.6472538113594055 train_loss : 0.6507745222523321\n",
            "Acc : 0.6519886255264282 train_loss : 0.6492044497039411\n",
            "Acc : 0.6568813323974609 train_loss : 0.6471004942732472\n",
            "Acc : 0.6616161465644836 train_loss : 0.6449831897058548\n",
            "Acc : 0.6665088534355164 train_loss : 0.6432982661352036\n",
            "Acc : 0.6712436676025391 train_loss : 0.6418243985198722\n",
            "Acc : 0.6756628751754761 train_loss : 0.6419110417178592\n",
            "Acc : 0.6802399158477783 train_loss : 0.6407488022930921\n",
            "Acc : 0.6845012903213501 train_loss : 0.6407636071769347\n",
            "Acc : 0.6892361044883728 train_loss : 0.6386217707653105\n",
            "Acc : 0.6936553120613098 train_loss : 0.6380984250204694\n",
            "Acc : 0.6977588534355164 train_loss : 0.6390893961779955\n",
            "Acc : 0.7024936676025391 train_loss : 0.6370482060945395\n",
            "Acc : 0.7073863744735718 train_loss : 0.6341065084898329\n",
            "Acc : 0.7122790217399597 train_loss : 0.632202988523923\n",
            "Acc : 0.7170138955116272 train_loss : 0.6304442374301809\n",
            "Acc : 0.7217487096786499 train_loss : 0.6284908143549981\n",
            "Acc : 0.7266414165496826 train_loss : 0.6263719240532202\n",
            "Acc : 0.7313762903213501 train_loss : 0.6244456047377391\n",
            "Acc : 0.7356376051902771 train_loss : 0.6258570306696171\n",
            "Acc : 0.7402146458625793 train_loss : 0.62458404821123\n",
            "Acc : 0.7444760203361511 train_loss : 0.6245848427730045\n",
            "Acc : 0.7490530014038086 train_loss : 0.6233543630157198\n",
            "Acc : 0.7541035413742065 train_loss : 0.6215147046710957\n",
            "Acc : 0.7586805820465088 train_loss : 0.6199276815194869\n",
            "Acc : 0.7632575631141663 train_loss : 0.6188426075524158\n",
            "Acc : 0.7679924368858337 train_loss : 0.61781055259638\n",
            "Acc : 0.7722538113594055 train_loss : 0.6172973757816685\n",
            "Acc : 0.7766729593276978 train_loss : 0.6160152654127521\n",
            "Acc : 0.7809343338012695 train_loss : 0.6159821549778456\n",
            "Acc : 0.785669207572937 train_loss : 0.6142935955622157\n",
            "Acc : 0.7902461886405945 train_loss : 0.6131450834319644\n",
            "Acc : 0.794981062412262 train_loss : 0.6123334439219655\n",
            "Acc : 0.7995581030845642 train_loss : 0.6114993953416424\n",
            "Acc : 0.8038194179534912 train_loss : 0.6102652574446112\n",
            "Acc : 0.8087121248245239 train_loss : 0.6082137582149911\n",
            "Acc : 0.8128156661987305 train_loss : 0.608924440290562\n",
            "Acc : 0.8177083134651184 train_loss : 0.6066520535632184\n",
            "Acc : 0.8227588534355164 train_loss : 0.6039034134820493\n",
            "Acc : 0.8274936676025391 train_loss : 0.601994770928286\n",
            "Acc : 0.8322285413742065 train_loss : 0.6005224613714094\n",
            "Acc : 0.8366477489471436 train_loss : 0.6005542364701167\n",
            "Acc : 0.841224730014801 train_loss : 0.6015800913557028\n",
            "Acc : 0.845643937587738 train_loss : 0.6004588201413957\n",
            "Acc : 0.8502209782600403 train_loss : 0.5992786102564202\n",
            "Acc : 0.8546401262283325 train_loss : 0.5996401251641789\n",
            "Validating....\n",
            "Acc 0.01957070641219616 train_loss 0.4017021656036377\n",
            "Acc 0.03787878900766373 train_loss 0.40438374876976013\n",
            "Acc 0.05808080732822418 train_loss 0.2939685905973117\n",
            "Acc 0.0763888880610466 train_loss 0.29131954349577427\n",
            "Acc 0.09595959633588791 train_loss 0.30787343531847\n",
            "Acc 0.11489898711442947 train_loss 0.316710955152909\n",
            "Acc 0.13005051016807556 train_loss 0.37829146747078213\n",
            "Acc 0.14898990094661713 train_loss 0.38357238564640284\n",
            "Acc 0.16729797422885895 train_loss 0.3825707013408343\n",
            "Acc 0.18686868250370026 train_loss 0.3771431840956211\n",
            "Acc 0.20454545319080353 train_loss 0.41668824512850156\n",
            "Acc 0.2209595888853073 train_loss 0.4581062675764163\n",
            "Acc 0.23863635957241058 train_loss 0.45401802028600985\n",
            "Acc 0.25757575035095215 train_loss 0.43675898600901875\n",
            "Acc 0.27714645862579346 train_loss 0.42144839018583297\n",
            "Acc 0.29482322931289673 train_loss 0.4246584461070597\n",
            "Acc 0.3125 train_loss 0.43123960012898727\n",
            "Acc 0.33143940567970276 train_loss 0.4278786401781771\n",
            "Acc 0.35101011395454407 train_loss 0.4120057974206774\n",
            "Acc 0.3712121248245239 train_loss 0.4004085298627615\n",
            "Acc 0.3914141356945038 train_loss 0.3839150369167328\n",
            "Acc 0.41035354137420654 train_loss 0.38587216491048987\n",
            "Acc 0.42992424964904785 train_loss 0.3899770832580069\n",
            "Acc 0.4482323229312897 train_loss 0.39139621953169507\n",
            "Acc 0.4652777910232544 train_loss 0.39537858724594116\n",
            "Acc 0.48421716690063477 train_loss 0.4022656495754535\n",
            "Acc 0.5031565427780151 train_loss 0.39815382493866813\n",
            "Acc 0.5214646458625793 train_loss 0.40004988653319223\n",
            "Acc 0.5391414165496826 train_loss 0.41891044789347154\n",
            "Acc 0.5574495196342468 train_loss 0.4179642379283905\n",
            "Acc 0.5757575631141663 train_loss 0.4173509834274169\n",
            "Acc 0.5953282713890076 train_loss 0.41074897116050124\n",
            "Acc 0.6142676472663879 train_loss 0.4070870691176617\n",
            "Acc 0.6306818127632141 train_loss 0.4171390388818348\n",
            "Acc 0.6489899158477783 train_loss 0.4190805379833494\n",
            "Acc 0.6679292917251587 train_loss 0.4136692380739583\n",
            "Acc 0.685606062412262 train_loss 0.4202341207781354\n",
            "Acc 0.7026515007019043 train_loss 0.4233630201534221\n",
            "Acc 0.7209596037864685 train_loss 0.42957849494921857\n",
            "Acc 0.7373737096786499 train_loss 0.4369686748832464\n",
            "Acc 0.756313145160675 train_loss 0.4355342471744956\n",
            "Acc 0.7739899158477783 train_loss 0.4369112639909699\n",
            "Acc 0.7922979593276978 train_loss 0.43816908535569216\n",
            "Acc 0.8087121248245239 train_loss 0.44786243784156715\n",
            "Acc 0.8270202279090881 train_loss 0.4431153655052185\n",
            "Acc 0.8453282713890076 train_loss 0.44291185425675433\n",
            "Acc 0.8636363744735718 train_loss 0.4408394491418879\n",
            "Acc 0.8825757503509521 train_loss 0.4377515905847152\n",
            "Acc 0.9021464586257935 train_loss 0.4408410148961203\n",
            "Acc 0.9109848737716675 train_loss 0.44537419140338896\n",
            "F1_score : 0.8358556461001165\n",
            "Epoch completed in 2.8283271193504333 minutes\n",
            "***************\n",
            "EPOCH: 2\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004419191740453243 train_loss : 0.9540314674377441\n",
            "Acc : 0.00931186880916357 train_loss : 0.6096287816762924\n",
            "Acc : 0.013573232106864452 train_loss : 0.635966827472051\n",
            "Acc : 0.01815025322139263 train_loss : 0.5406885296106339\n",
            "Acc : 0.022727273404598236 train_loss : 0.5288950562477112\n",
            "Acc : 0.02777777798473835 train_loss : 0.4678939605752627\n",
            "Acc : 0.032354798167943954 train_loss : 0.46932172988142284\n",
            "Acc : 0.03724747523665428 train_loss : 0.45165170170366764\n",
            "Acc : 0.04182449355721474 train_loss : 0.4450562679105335\n",
            "Acc : 0.046243686228990555 train_loss : 0.4423639431595802\n",
            "Acc : 0.05050504952669144 train_loss : 0.44964099065823987\n",
            "Acc : 0.05523989722132683 train_loss : 0.4530881109337012\n",
            "Acc : 0.06029040366411209 train_loss : 0.43518879321905285\n",
            "Acc : 0.06518308073282242 train_loss : 0.42210876090185984\n",
            "Acc : 0.0694444477558136 train_loss : 0.4303982973098755\n",
            "Acc : 0.07417929172515869 train_loss : 0.43465467169880867\n",
            "Acc : 0.07828282564878464 train_loss : 0.44384046512491565\n",
            "Acc : 0.08301767706871033 train_loss : 0.4378787891732322\n",
            "Acc : 0.08775252848863602 train_loss : 0.4361154201783632\n",
            "Acc : 0.09248737245798111 train_loss : 0.4360962614417076\n",
            "Acc : 0.09706439077854156 train_loss : 0.450508116256623\n",
            "Acc : 0.10148358345031738 train_loss : 0.45438783954490314\n",
            "Acc : 0.1059027761220932 train_loss : 0.45101312062014703\n",
            "Acc : 0.11032196879386902 train_loss : 0.45068301012118656\n",
            "Acc : 0.11505682021379471 train_loss : 0.45015465259552\n",
            "Acc : 0.11947601288557053 train_loss : 0.45124907333117265\n",
            "Acc : 0.12357954680919647 train_loss : 0.4712578378341816\n",
            "Acc : 0.12831439077854156 train_loss : 0.46919921785593033\n",
            "Acc : 0.13257575035095215 train_loss : 0.4719212168249591\n",
            "Acc : 0.13699494302272797 train_loss : 0.4752086450656255\n",
            "Acc : 0.14125631749629974 train_loss : 0.47538457089854824\n",
            "Acc : 0.1453598439693451 train_loss : 0.4827493308112025\n",
            "Acc : 0.15025252103805542 train_loss : 0.47841640494086524\n",
            "Acc : 0.1545138955116272 train_loss : 0.48474052899024067\n",
            "Acc : 0.1592487394809723 train_loss : 0.47882449116025655\n",
            "Acc : 0.1636679321527481 train_loss : 0.4790521272354656\n",
            "Acc : 0.16824494302272797 train_loss : 0.47861802497425593\n",
            "Acc : 0.17329545319080353 train_loss : 0.47260472570594986\n",
            "Acc : 0.17787247896194458 train_loss : 0.47250165083469486\n",
            "Acc : 0.1822916716337204 train_loss : 0.47955639809370043\n",
            "Acc : 0.18671086430549622 train_loss : 0.4782520125551922\n",
            "Acc : 0.19160354137420654 train_loss : 0.47329633221739814\n",
            "Acc : 0.19633838534355164 train_loss : 0.4735039368618366\n",
            "Acc : 0.20107322931289673 train_loss : 0.4709349741989916\n",
            "Acc : 0.20580807328224182 train_loss : 0.46896606816185843\n",
            "Acc : 0.21070075035095215 train_loss : 0.46398015806208487\n",
            "Acc : 0.21559342741966248 train_loss : 0.45960660342206344\n",
            "Acc : 0.2204861044883728 train_loss : 0.4552332879975438\n",
            "Acc : 0.22537878155708313 train_loss : 0.45291660239501874\n",
            "Acc : 0.22995580732822418 train_loss : 0.45177767366170885\n",
            "Acc : 0.23453283309936523 train_loss : 0.4507343812900431\n",
            "Acc : 0.2391098439693451 train_loss : 0.4524016835941718\n",
            "Acc : 0.24368686974048615 train_loss : 0.4541257268415307\n",
            "Acc : 0.24857954680919647 train_loss : 0.45015044869096194\n",
            "Acc : 0.2531565725803375 train_loss : 0.44882450618527153\n",
            "Acc : 0.2578914165496826 train_loss : 0.4462767421667065\n",
            "Acc : 0.2624684274196625 train_loss : 0.4452520093896933\n",
            "Acc : 0.2668876349925995 train_loss : 0.44446928413777514\n",
            "Acc : 0.2717803120613098 train_loss : 0.44033190052388077\n",
            "Acc : 0.27619948983192444 train_loss : 0.44075916359821954\n",
            "Acc : 0.28125 train_loss : 0.43642136918716745\n",
            "Acc : 0.2859848439693451 train_loss : 0.43588014836272887\n",
            "Acc : 0.2907196879386902 train_loss : 0.434134569196474\n",
            "Acc : 0.2951388955116272 train_loss : 0.43402232020162046\n",
            "Acc : 0.29971590638160706 train_loss : 0.4360836760355876\n",
            "Acc : 0.30413511395454407 train_loss : 0.43479490889744327\n",
            "Acc : 0.3090277910232544 train_loss : 0.4312944383318745\n",
            "Acc : 0.3129734992980957 train_loss : 0.4409550277187544\n",
            "Acc : 0.3177083432674408 train_loss : 0.44070286098597705\n",
            "Acc : 0.32228535413742065 train_loss : 0.4406843217355864\n",
            "Acc : 0.3267045319080353 train_loss : 0.4405853742025268\n",
            "Acc : 0.3315972089767456 train_loss : 0.43732172395620084\n",
            "Acc : 0.33617424964904785 train_loss : 0.43760697992697156\n",
            "Acc : 0.34043559432029724 train_loss : 0.4427342497416445\n",
            "Acc : 0.3451704680919647 train_loss : 0.44128211319446564\n",
            "Acc : 0.35006314516067505 train_loss : 0.4387163202229299\n",
            "Acc : 0.3546401560306549 train_loss : 0.4391685561700301\n",
            "Acc : 0.359375 train_loss : 0.4379335947525807\n",
            "Acc : 0.3641098439693451 train_loss : 0.43696377963959415\n",
            "Acc : 0.3688446879386902 train_loss : 0.4351560339331627\n",
            "Acc : 0.3735795319080353 train_loss : 0.4341235429416468\n",
            "Acc : 0.3779987394809723 train_loss : 0.4370613428877621\n",
            "Acc : 0.3828914165496826 train_loss : 0.4345725907618741\n",
            "Acc : 0.3876262605190277 train_loss : 0.43358410043375833\n",
            "Acc : 0.39267677068710327 train_loss : 0.4306625541518716\n",
            "Acc : 0.39725378155708313 train_loss : 0.43316845977029134\n",
            "Acc : 0.4018308222293854 train_loss : 0.43202426481520995\n",
            "Acc : 0.40640783309936523 train_loss : 0.4321805624799295\n",
            "Acc : 0.4105113744735718 train_loss : 0.43268755446659046\n",
            "Acc : 0.41524621844291687 train_loss : 0.43059014363421333\n",
            "Acc : 0.4201388955116272 train_loss : 0.42991089542488475\n",
            "Acc : 0.42471590638160706 train_loss : 0.4292002843449945\n",
            "Acc : 0.42945075035095215 train_loss : 0.43053279881195355\n",
            "Acc : 0.4343434274196625 train_loss : 0.42814197866840564\n",
            "Acc : 0.4389204680919647 train_loss : 0.43036227524280546\n",
            "Acc : 0.44333964586257935 train_loss : 0.43130431370809674\n",
            "Acc : 0.4479166567325592 train_loss : 0.4312311710463357\n",
            "Acc : 0.45296716690063477 train_loss : 0.42875275061446794\n",
            "Acc : 0.4578598439693451 train_loss : 0.4271266073590577\n",
            "Acc : 0.46291035413742065 train_loss : 0.4242112694680691\n",
            "Acc : 0.4674873650074005 train_loss : 0.4225332897783506\n",
            "Acc : 0.47206440567970276 train_loss : 0.42407796736441405\n",
            "Acc : 0.4766414165496826 train_loss : 0.4241124149954435\n",
            "Acc : 0.4813762605190277 train_loss : 0.42491531959519935\n",
            "Acc : 0.48595327138900757 train_loss : 0.4255569223846708\n",
            "Acc : 0.4905303120613098 train_loss : 0.42773955146659093\n",
            "Acc : 0.4947916567325592 train_loss : 0.4304396909809558\n",
            "Acc : 0.49968433380126953 train_loss : 0.4277189656816147\n",
            "Acc : 0.504419207572937 train_loss : 0.4271952500310513\n",
            "Acc : 0.5088383555412292 train_loss : 0.4274566860361533\n",
            "Acc : 0.5134153962135315 train_loss : 0.4273285268394797\n",
            "Acc : 0.5179924368858337 train_loss : 0.4276262866333127\n",
            "Acc : 0.5228850841522217 train_loss : 0.42621984378953953\n",
            "Acc : 0.5277777910232544 train_loss : 0.42411675526384723\n",
            "Acc : 0.5325126051902771 train_loss : 0.4255961485531019\n",
            "Acc : 0.5374053120613098 train_loss : 0.423884361725429\n",
            "Acc : 0.542455792427063 train_loss : 0.42121297095575905\n",
            "Acc : 0.546875 train_loss : 0.423001570469242\n",
            "Acc : 0.5514520406723022 train_loss : 0.4226459329869567\n",
            "Acc : 0.556186854839325 train_loss : 0.4226627049346765\n",
            "Acc : 0.560606062412262 train_loss : 0.42206453242577796\n",
            "Acc : 0.5648674368858337 train_loss : 0.4251020922035467\n",
            "Acc : 0.5696022510528564 train_loss : 0.4249277529193134\n",
            "Acc : 0.5744949579238892 train_loss : 0.4232935233702583\n",
            "Acc : 0.5793876051902771 train_loss : 0.4215003049373627\n",
            "Acc : 0.5841224789619446 train_loss : 0.4210961943580991\n",
            "Acc : 0.5886995196342468 train_loss : 0.42200692901461145\n",
            "Acc : 0.5932765007019043 train_loss : 0.4216295829974115\n",
            "Acc : 0.5976957082748413 train_loss : 0.4228423824606016\n",
            "Acc : 0.6024305820465088 train_loss : 0.4224621309683873\n",
            "Acc : 0.6066918969154358 train_loss : 0.4232303354576344\n",
            "Acc : 0.6109532713890076 train_loss : 0.4240860550692587\n",
            "Acc : 0.615688145160675 train_loss : 0.4232050675646703\n",
            "Acc : 0.6199495196342468 train_loss : 0.42629889476655136\n",
            "Acc : 0.6246843338012695 train_loss : 0.4252407166692946\n",
            "Acc : 0.629419207572937 train_loss : 0.4245391457396395\n",
            "Acc : 0.6341540217399597 train_loss : 0.4236014113374\n",
            "Acc : 0.6388888955116272 train_loss : 0.42300756254057953\n",
            "Acc : 0.6437815427780151 train_loss : 0.4211721193018577\n",
            "Acc : 0.6482007503509521 train_loss : 0.4216224383030619\n",
            "Acc : 0.6523042917251587 train_loss : 0.42476955478918466\n",
            "Acc : 0.6568813323974609 train_loss : 0.4258868222085523\n",
            "Acc : 0.6616161465644836 train_loss : 0.42495226005574205\n",
            "Acc : 0.6663510203361511 train_loss : 0.42399291611380047\n",
            "Acc : 0.6710858345031738 train_loss : 0.423404080908874\n",
            "Acc : 0.6756628751754761 train_loss : 0.4233049417603506\n",
            "Acc : 0.6803977489471436 train_loss : 0.4224557562344739\n",
            "Acc : 0.6848168969154358 train_loss : 0.42245797470614715\n",
            "Acc : 0.6895517706871033 train_loss : 0.4216476124805092\n",
            "Acc : 0.6942866444587708 train_loss : 0.4214579615990321\n",
            "Acc : 0.6990214586257935 train_loss : 0.421103736817442\n",
            "Acc : 0.7039141654968262 train_loss : 0.41948148441549976\n",
            "Acc : 0.7081754803657532 train_loss : 0.4202439476267185\n",
            "Acc : 0.7127525210380554 train_loss : 0.4198284587496287\n",
            "Acc : 0.7176452279090881 train_loss : 0.41876490817916007\n",
            "Acc : 0.7223800420761108 train_loss : 0.4180942094669892\n",
            "Acc : 0.7272727489471436 train_loss : 0.41726740881515917\n",
            "Acc : 0.731849730014801 train_loss : 0.41729588212468954\n",
            "Acc : 0.7365846037864685 train_loss : 0.41678268157836024\n",
            "Acc : 0.7411616444587708 train_loss : 0.41700508492067456\n",
            "Acc : 0.7454229593276978 train_loss : 0.41842546679588577\n",
            "Acc : 0.7501578330993652 train_loss : 0.41720503725019503\n",
            "Acc : 0.7545770406723022 train_loss : 0.41809531488667234\n",
            "Acc : 0.7588383555412292 train_loss : 0.41920295875610375\n",
            "Acc : 0.7635732293128967 train_loss : 0.4181367407242457\n",
            "Acc : 0.7683081030845642 train_loss : 0.4168159880013351\n",
            "Acc : 0.7732007503509521 train_loss : 0.41550979285896894\n",
            "Acc : 0.7779356241226196 train_loss : 0.4150314817116374\n",
            "Acc : 0.7826704382896423 train_loss : 0.41373946266414147\n",
            "Acc : 0.787563145160675 train_loss : 0.4123942508417017\n",
            "Acc : 0.7922979593276978 train_loss : 0.41187222763808845\n",
            "Acc : 0.7967171669006348 train_loss : 0.4134486976057984\n",
            "Acc : 0.8011363744735718 train_loss : 0.4133747394029805\n",
            "Acc : 0.8057133555412292 train_loss : 0.4134646236211404\n",
            "Acc : 0.8102903962135315 train_loss : 0.4133937236240932\n",
            "Acc : 0.8153409361839294 train_loss : 0.4121979591860013\n",
            "Acc : 0.8200757503509521 train_loss : 0.41174949545644773\n",
            "Acc : 0.8246527910232544 train_loss : 0.4115962578674381\n",
            "Acc : 0.8295454382896423 train_loss : 0.411272825642005\n",
            "Acc : 0.8341224789619446 train_loss : 0.41086568501260545\n",
            "Acc : 0.8386995196342468 train_loss : 0.41065004023399143\n",
            "Acc : 0.8432765007019043 train_loss : 0.41120202593751004\n",
            "Acc : 0.8480113744735718 train_loss : 0.4099856717664687\n",
            "Acc : 0.8525883555412292 train_loss : 0.40993334223394806\n",
            "Acc : 0.857481062412262 train_loss : 0.4093049429558419\n",
            "Acc : 0.8620581030845642 train_loss : 0.40901320051121454\n",
            "Acc : 0.8669507503509521 train_loss : 0.40797528935307487\n",
            "Acc : 0.8720012903213501 train_loss : 0.40649905459994967\n",
            "Acc : 0.8764204382896423 train_loss : 0.40727131525990823\n",
            "Acc : 0.881313145160675 train_loss : 0.40572858390055205\n",
            "Acc : 0.886205792427063 train_loss : 0.4040563891773449\n",
            "Acc : 0.8909406661987305 train_loss : 0.40297974781909335\n",
            "Acc : 0.8956754803657532 train_loss : 0.40229236442653626\n",
            "Acc : 0.9004103541374207 train_loss : 0.4015144659302284\n",
            "Acc : 0.9049873948097229 train_loss : 0.40236574010207105\n",
            "Acc : 0.9095643758773804 train_loss : 0.40207054675081555\n",
            "Acc : 0.9141414165496826 train_loss : 0.4022573248913445\n",
            "Acc : 0.9185606241226196 train_loss : 0.4029259935698726\n",
            "Validating....\n",
            "Acc 0.018939394503831863 train_loss 0.3848452568054199\n",
            "Acc 0.03724747523665428 train_loss 0.42678265273571014\n",
            "Acc 0.05744949355721474 train_loss 0.29924194514751434\n",
            "Acc 0.0763888880610466 train_loss 0.28670212998986244\n",
            "Acc 0.09532828629016876 train_loss 0.2807136684656143\n",
            "Acc 0.11426767706871033 train_loss 0.2969183698296547\n",
            "Acc 0.13005051016807556 train_loss 0.356538787484169\n",
            "Acc 0.14898990094661713 train_loss 0.3604216296225786\n",
            "Acc 0.16729797422885895 train_loss 0.3601369874344932\n",
            "Acc 0.18560606241226196 train_loss 0.3602711036801338\n",
            "Acc 0.20328283309936523 train_loss 0.39563689584081824\n",
            "Acc 0.2209595888853073 train_loss 0.43120575572053593\n",
            "Acc 0.23989899456501007 train_loss 0.42371361874617064\n",
            "Acc 0.25883838534355164 train_loss 0.40888723731040955\n",
            "Acc 0.2790403962135315 train_loss 0.391134371360143\n",
            "Acc 0.29671716690063477 train_loss 0.40271621383726597\n",
            "Acc 0.3156565725803375 train_loss 0.39854050208540526\n",
            "Acc 0.33396464586257935 train_loss 0.39752339323361713\n",
            "Acc 0.35353535413742065 train_loss 0.3812241420934075\n",
            "Acc 0.3737373650074005 train_loss 0.3684907391667366\n",
            "Acc 0.39393940567970276 train_loss 0.3525205557899816\n",
            "Acc 0.41287878155708313 train_loss 0.3574993683194572\n",
            "Acc 0.4318181872367859 train_loss 0.36132136942899745\n",
            "Acc 0.4501262605190277 train_loss 0.363286838401109\n",
            "Acc 0.46717172861099243 train_loss 0.3643479160964489\n",
            "Acc 0.4861111044883728 train_loss 0.37409240508881897\n",
            "Acc 0.5050504803657532 train_loss 0.3705019347921566\n",
            "Acc 0.5233585834503174 train_loss 0.37217729152845486\n",
            "Acc 0.5404040217399597 train_loss 0.3903358654472335\n",
            "Acc 0.5587121248245239 train_loss 0.3913497610638539\n",
            "Acc 0.5770202279090881 train_loss 0.3929190245126524\n",
            "Acc 0.5965909361839294 train_loss 0.38552356080617756\n",
            "Acc 0.6155303120613098 train_loss 0.38049418038942595\n",
            "Acc 0.6325757503509521 train_loss 0.3902475355083452\n",
            "Acc 0.6515151262283325 train_loss 0.3929523803293705\n",
            "Acc 0.6710858345031738 train_loss 0.38827361942579347\n",
            "Acc 0.6881313323974609 train_loss 0.3980582335309402\n",
            "Acc 0.7058081030845642 train_loss 0.40033995115051146\n",
            "Acc 0.7241161465644836 train_loss 0.40698530993018395\n",
            "Acc 0.7405303120613098 train_loss 0.41240222109481695\n",
            "Acc 0.7594696879386902 train_loss 0.40929267346495535\n",
            "Acc 0.7765151262283325 train_loss 0.41375327420731384\n",
            "Acc 0.7954545617103577 train_loss 0.4140371391766293\n",
            "Acc 0.8125 train_loss 0.4236166723580523\n",
            "Acc 0.8308081030845642 train_loss 0.41911233390371005\n",
            "Acc 0.8484848737716675 train_loss 0.4188795704394579\n",
            "Acc 0.8667929172515869 train_loss 0.41708184295195216\n",
            "Acc 0.8857322931289673 train_loss 0.41409123432822526\n",
            "Acc 0.9053030014038086 train_loss 0.4167515026519493\n",
            "Acc 0.9141414165496826 train_loss 0.42230285637080667\n",
            "F1_score : 0.8447488584474886\n",
            "Epoch completed in 2.055622907479604 minutes\n",
            "***************\n",
            "EPOCH: 3\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.7198262214660645\n",
            "Acc : 0.00931186880916357 train_loss : 0.4869253486394882\n",
            "Acc : 0.013888888992369175 train_loss : 0.44539738694826764\n",
            "Acc : 0.018781565129756927 train_loss : 0.3859982378780842\n",
            "Acc : 0.023358585312962532 train_loss : 0.3933397978544235\n",
            "Acc : 0.028409091755747795 train_loss : 0.34562301884094876\n",
            "Acc : 0.03314393758773804 train_loss : 0.3365307130983898\n",
            "Acc : 0.038036614656448364 train_loss : 0.3180424366146326\n",
            "Acc : 0.04261363670229912 train_loss : 0.3155207054482566\n",
            "Acc : 0.04734848439693451 train_loss : 0.3160703405737877\n",
            "Acc : 0.05192550644278526 train_loss : 0.3232692141424526\n",
            "Acc : 0.05650252476334572 train_loss : 0.3365088937183221\n",
            "Acc : 0.061395201832056046 train_loss : 0.3234671915952976\n",
            "Acc : 0.06628787517547607 train_loss : 0.3126199170947075\n",
            "Acc : 0.07054924219846725 train_loss : 0.3245118866364161\n",
            "Acc : 0.07512626051902771 train_loss : 0.32659898046404123\n",
            "Acc : 0.07954545319080353 train_loss : 0.33446072041988373\n",
            "Acc : 0.08459595590829849 train_loss : 0.32766780588361955\n",
            "Acc : 0.08933080732822418 train_loss : 0.3324008285999298\n",
            "Acc : 0.09406565874814987 train_loss : 0.3324702262878418\n",
            "Acc : 0.09864267706871033 train_loss : 0.3385926343145825\n",
            "Acc : 0.10321969538927078 train_loss : 0.34528522057966754\n",
            "Acc : 0.10795454680919647 train_loss : 0.3438641299372134\n",
            "Acc : 0.11253156512975693 train_loss : 0.3460589113334815\n",
            "Acc : 0.11726641654968262 train_loss : 0.34864036321640013\n",
            "Acc : 0.12168560922145844 train_loss : 0.35174053792770094\n",
            "Acc : 0.12594696879386902 train_loss : 0.3698081274827321\n",
            "Acc : 0.13052399456501007 train_loss : 0.3734454182641847\n",
            "Acc : 0.1349431872367859 train_loss : 0.38007913067423066\n",
            "Acc : 0.13952019810676575 train_loss : 0.38155858119328817\n",
            "Acc : 0.14346590638160706 train_loss : 0.38570638433579474\n",
            "Acc : 0.14772726595401764 train_loss : 0.3977951891720295\n",
            "Acc : 0.15246212482452393 train_loss : 0.39528885303121625\n",
            "Acc : 0.15688131749629974 train_loss : 0.40466785167946534\n",
            "Acc : 0.1619318127632141 train_loss : 0.3972241984946387\n",
            "Acc : 0.1661931872367859 train_loss : 0.39837496189607513\n",
            "Acc : 0.17092803120613098 train_loss : 0.39883932592095556\n",
            "Acc : 0.17597854137420654 train_loss : 0.39342176875001506\n",
            "Acc : 0.1805555522441864 train_loss : 0.3940462955297568\n",
            "Acc : 0.1852904111146927 train_loss : 0.3970390360802412\n",
            "Acc : 0.18955177068710327 train_loss : 0.4002851011549554\n",
            "Acc : 0.1944444477558136 train_loss : 0.39580740176496054\n",
            "Acc : 0.19886364042758942 train_loss : 0.39742823811464534\n",
            "Acc : 0.2035984843969345 train_loss : 0.39547874304381286\n",
            "Acc : 0.2083333283662796 train_loss : 0.3927885525756412\n",
            "Acc : 0.21322600543498993 train_loss : 0.389769971370697\n",
            "Acc : 0.21811868250370026 train_loss : 0.38587191320480185\n",
            "Acc : 0.22285354137420654 train_loss : 0.383233105763793\n",
            "Acc : 0.22758838534355164 train_loss : 0.38173929586702465\n",
            "Acc : 0.23232322931289673 train_loss : 0.38097515881061556\n",
            "Acc : 0.23690025508403778 train_loss : 0.38306111681695076\n",
            "Acc : 0.24147726595401764 train_loss : 0.3839811258591138\n",
            "Acc : 0.2460542917251587 train_loss : 0.38520140692872823\n",
            "Acc : 0.25110480189323425 train_loss : 0.38201511016598455\n",
            "Acc : 0.2559974789619446 train_loss : 0.3788111757148396\n",
            "Acc : 0.2608901560306549 train_loss : 0.37522042595914434\n",
            "Acc : 0.265625 train_loss : 0.37443855195714715\n",
            "Acc : 0.2700441777706146 train_loss : 0.373550690453628\n",
            "Acc : 0.27493685483932495 train_loss : 0.37028920069589455\n",
            "Acc : 0.2795138955116272 train_loss : 0.37135686005155244\n",
            "Acc : 0.28456440567970276 train_loss : 0.3682729386892475\n",
            "Acc : 0.2891414165496826 train_loss : 0.3699472758077806\n",
            "Acc : 0.2938762605190277 train_loss : 0.36969484317870366\n",
            "Acc : 0.29845327138900757 train_loss : 0.37049609888345003\n",
            "Acc : 0.3030303120613098 train_loss : 0.3746574356005742\n",
            "Acc : 0.30792298913002014 train_loss : 0.3711695422728856\n",
            "Acc : 0.31281566619873047 train_loss : 0.36792094293814986\n",
            "Acc : 0.31707701086997986 train_loss : 0.37278129752067957\n",
            "Acc : 0.3219696879386902 train_loss : 0.369401467235192\n",
            "Acc : 0.32654672861099243 train_loss : 0.37060093730688093\n",
            "Acc : 0.3312815725803375 train_loss : 0.3715571104220941\n",
            "Acc : 0.33617424964904785 train_loss : 0.3697451413091686\n",
            "Acc : 0.3407512605190277 train_loss : 0.3688805958587829\n",
            "Acc : 0.34485480189323425 train_loss : 0.3792321156408336\n",
            "Acc : 0.3499053120613098 train_loss : 0.37608408470948534\n",
            "Acc : 0.35479798913002014 train_loss : 0.3732898403939448\n",
            "Acc : 0.359375 train_loss : 0.37375236873502854\n",
            "Acc : 0.3641098439693451 train_loss : 0.37349179005011535\n",
            "Acc : 0.3688446879386902 train_loss : 0.371555546416512\n",
            "Acc : 0.3735795319080353 train_loss : 0.3702000018209219\n",
            "Acc : 0.37831440567970276 train_loss : 0.36841098062786054\n",
            "Acc : 0.3828914165496826 train_loss : 0.37112148106098175\n",
            "Acc : 0.38778409361839294 train_loss : 0.3682762878127845\n",
            "Acc : 0.39251893758773804 train_loss : 0.3674437242249648\n",
            "Acc : 0.3975694477558136 train_loss : 0.36472206448807437\n",
            "Acc : 0.4024621248245239 train_loss : 0.36600687008264454\n",
            "Acc : 0.407196968793869 train_loss : 0.3656698706848868\n",
            "Acc : 0.4117739796638489 train_loss : 0.36763714270835574\n",
            "Acc : 0.41603535413742065 train_loss : 0.3680056030495783\n",
            "Acc : 0.420928031206131 train_loss : 0.3658922466966841\n",
            "Acc : 0.4256628751754761 train_loss : 0.36568128997152977\n",
            "Acc : 0.4305555522441864 train_loss : 0.3644770675379297\n",
            "Acc : 0.4352903962135315 train_loss : 0.3663005252038279\n",
            "Acc : 0.4401830732822418 train_loss : 0.36404659725884175\n",
            "Acc : 0.44476011395454407 train_loss : 0.3661092038217344\n",
            "Acc : 0.44949495792388916 train_loss : 0.36623154223586124\n",
            "Acc : 0.454071968793869 train_loss : 0.36638360401404274\n",
            "Acc : 0.4591224789619446 train_loss : 0.36383618239541443\n",
            "Acc : 0.4640151560306549 train_loss : 0.36204471333761407\n",
            "Acc : 0.46906566619873047 train_loss : 0.35903164602816107\n",
            "Acc : 0.4739583432674408 train_loss : 0.3579603924167038\n",
            "Acc : 0.47853535413742065 train_loss : 0.3593585439318535\n",
            "Acc : 0.4831123650074005 train_loss : 0.359909910672498\n",
            "Acc : 0.48768940567970276 train_loss : 0.3611045749858022\n",
            "Acc : 0.49242424964904785 train_loss : 0.3610914670995304\n",
            "Acc : 0.4968434274196625 train_loss : 0.3629965684464518\n",
            "Acc : 0.5011047720909119 train_loss : 0.36594861557829045\n",
            "Acc : 0.5059974789619446 train_loss : 0.3640253550062577\n",
            "Acc : 0.5107322931289673 train_loss : 0.3638592390428989\n",
            "Acc : 0.5154671669006348 train_loss : 0.3632513900372115\n",
            "Acc : 0.5202020406723022 train_loss : 0.3632076748453819\n",
            "Acc : 0.5247790217399597 train_loss : 0.36377050867304206\n",
            "Acc : 0.5296717286109924 train_loss : 0.3631111199481297\n",
            "Acc : 0.5347222089767456 train_loss : 0.3610437741797221\n",
            "Acc : 0.5392992496490479 train_loss : 0.3635480610572773\n",
            "Acc : 0.5440340638160706 train_loss : 0.36300026824505166\n",
            "Acc : 0.5490846037864685 train_loss : 0.3604384833930904\n",
            "Acc : 0.5533459782600403 train_loss : 0.3615624856645778\n",
            "Acc : 0.5583964586257935 train_loss : 0.36044469563400044\n",
            "Acc : 0.5629734992980957 train_loss : 0.3606984696040551\n",
            "Acc : 0.5677083134651184 train_loss : 0.36000423552083577\n",
            "Acc : 0.5719696879386902 train_loss : 0.3639690940741633\n",
            "Acc : 0.5767045617103577 train_loss : 0.36450515735924727\n",
            "Acc : 0.5817550420761108 train_loss : 0.3629365150005587\n",
            "Acc : 0.5868055820465088 train_loss : 0.3608333382606506\n",
            "Acc : 0.5915403962135315 train_loss : 0.36078908921234193\n",
            "Acc : 0.5961174368858337 train_loss : 0.3608986439667349\n",
            "Acc : 0.6010100841522217 train_loss : 0.3602898509707302\n",
            "Acc : 0.6055871248245239 train_loss : 0.3608412132706753\n",
            "Acc : 0.6104797720909119 train_loss : 0.36024899987074044\n",
            "Acc : 0.6152146458625793 train_loss : 0.3605648074441284\n",
            "Acc : 0.6196338534355164 train_loss : 0.36221999697612994\n",
            "Acc : 0.6243686676025391 train_loss : 0.36255100235006865\n",
            "Acc : 0.6287878751754761 train_loss : 0.3655734037730231\n",
            "Acc : 0.6336805820465088 train_loss : 0.3644504179557165\n",
            "Acc : 0.6385732293128967 train_loss : 0.3636536807479227\n",
            "Acc : 0.6434659361839294 train_loss : 0.3632795681266019\n",
            "Acc : 0.6482007503509521 train_loss : 0.36196575810511905\n",
            "Acc : 0.6529356241226196 train_loss : 0.3614450344722048\n",
            "Acc : 0.6576704382896423 train_loss : 0.3611222908965179\n",
            "Acc : 0.6617739796638489 train_loss : 0.36420906678581916\n",
            "Acc : 0.6663510203361511 train_loss : 0.36539922035496\n",
            "Acc : 0.6710858345031738 train_loss : 0.36468109540589205\n",
            "Acc : 0.6758207082748413 train_loss : 0.3638255733789669\n",
            "Acc : 0.6807133555412292 train_loss : 0.3631354289835897\n",
            "Acc : 0.6852903962135315 train_loss : 0.3632083656649067\n",
            "Acc : 0.6901831030845642 train_loss : 0.36154634870436725\n",
            "Acc : 0.6946022510528564 train_loss : 0.36237344462927934\n",
            "Acc : 0.6994949579238892 train_loss : 0.3612836315827882\n",
            "Acc : 0.7042297720909119 train_loss : 0.36165155852834385\n",
            "Acc : 0.7089646458625793 train_loss : 0.3612716001094572\n",
            "Acc : 0.7140151262283325 train_loss : 0.35969301171012613\n",
            "Acc : 0.7185921669006348 train_loss : 0.359869132001026\n",
            "Acc : 0.7236426472663879 train_loss : 0.3587881173309568\n",
            "Acc : 0.7285353541374207 train_loss : 0.3581352343001673\n",
            "Acc : 0.7332702279090881 train_loss : 0.35779079102361816\n",
            "Acc : 0.7380050420761108 train_loss : 0.3575722939174646\n",
            "Acc : 0.7427399158477783 train_loss : 0.3578132239606562\n",
            "Acc : 0.747474730014801 train_loss : 0.3576680226427204\n",
            "Acc : 0.7520517706871033 train_loss : 0.35821246202103796\n",
            "Acc : 0.7564709782600403 train_loss : 0.35888198513236846\n",
            "Acc : 0.7613636255264282 train_loss : 0.35774704938133556\n",
            "Acc : 0.7657828330993652 train_loss : 0.35914332492768397\n",
            "Acc : 0.7703598737716675 train_loss : 0.36029612813599227\n",
            "Acc : 0.7752525210380554 train_loss : 0.3590815279068369\n",
            "Acc : 0.7799873948097229 train_loss : 0.3582326570997037\n",
            "Acc : 0.7848800420761108 train_loss : 0.3573113522397544\n",
            "Acc : 0.7896149158477783 train_loss : 0.35682612245104145\n",
            "Acc : 0.7945075631141663 train_loss : 0.35567818868618745\n",
            "Acc : 0.7992424368858337 train_loss : 0.3549165914602139\n",
            "Acc : 0.8039772510528564 train_loss : 0.3544493144628597\n",
            "Acc : 0.8087121248245239 train_loss : 0.3555611636662899\n",
            "Acc : 0.8134469985961914 train_loss : 0.35494839770428704\n",
            "Acc : 0.8180239796638489 train_loss : 0.35533785704394866\n",
            "Acc : 0.8226010203361511 train_loss : 0.3553462129405567\n",
            "Acc : 0.8274936676025391 train_loss : 0.35455672100017016\n",
            "Acc : 0.8323863744735718 train_loss : 0.35417430426946467\n",
            "Acc : 0.8371211886405945 train_loss : 0.3542306474671605\n",
            "Acc : 0.8420138955116272 train_loss : 0.3538772293559\n",
            "Acc : 0.8469065427780151 train_loss : 0.35308556138641306\n",
            "Acc : 0.8517992496490479 train_loss : 0.35220834567566606\n",
            "Acc : 0.8563762903213501 train_loss : 0.3528394894367391\n",
            "Acc : 0.861268937587738 train_loss : 0.3519328734506675\n",
            "Acc : 0.8658459782600403 train_loss : 0.3519750047556084\n",
            "Acc : 0.8707386255264282 train_loss : 0.3513527843195039\n",
            "Acc : 0.8754734992980957 train_loss : 0.3509787667582753\n",
            "Acc : 0.8803661465644836 train_loss : 0.349855019725899\n",
            "Acc : 0.8854166865348816 train_loss : 0.34849266382925054\n",
            "Acc : 0.8898358345031738 train_loss : 0.3496047970321443\n",
            "Acc : 0.8948863744735718 train_loss : 0.3480154525488615\n",
            "Acc : 0.899936854839325 train_loss : 0.3463593161035895\n",
            "Acc : 0.9046717286109924 train_loss : 0.34539418650092557\n",
            "Acc : 0.9094065427780151 train_loss : 0.34491968647108795\n",
            "Acc : 0.9138257503509521 train_loss : 0.3459860964956665\n",
            "Acc : 0.9185606241226196 train_loss : 0.34715691313911706\n",
            "Acc : 0.9234532713890076 train_loss : 0.34675179237537845\n",
            "Acc : 0.9280303120613098 train_loss : 0.3472958917961205\n",
            "Acc : 0.9324495196342468 train_loss : 0.3483831533089732\n",
            "Validating....\n",
            "Acc 0.018939394503831863 train_loss 0.6396875381469727\n",
            "Acc 0.03787878900766373 train_loss 0.5480234324932098\n",
            "Acc 0.05808080732822418 train_loss 0.374681381508708\n",
            "Acc 0.07702020555734634 train_loss 0.3438241989351809\n",
            "Acc 0.09532828629016876 train_loss 0.32862590439617634\n",
            "Acc 0.11300504952669144 train_loss 0.3515635471170147\n",
            "Acc 0.12941919267177582 train_loss 0.40900941751897335\n",
            "Acc 0.14835858345031738 train_loss 0.40614469279535115\n",
            "Acc 0.1666666716337204 train_loss 0.40362909043000805\n",
            "Acc 0.18497474491596222 train_loss 0.42211087290197613\n",
            "Acc 0.2026515156030655 train_loss 0.46401737444102764\n",
            "Acc 0.21969696879386902 train_loss 0.5040616636785368\n",
            "Acc 0.23800505697727203 train_loss 0.489783683218635\n",
            "Acc 0.25757575035095215 train_loss 0.4700984557026199\n",
            "Acc 0.2777777910232544 train_loss 0.4457428521166245\n",
            "Acc 0.2954545319080353 train_loss 0.4602628523716703\n",
            "Acc 0.31439393758773804 train_loss 0.4490501044646782\n",
            "Acc 0.33270201086997986 train_loss 0.44594229643957484\n",
            "Acc 0.35227271914482117 train_loss 0.42751152784024415\n",
            "Acc 0.3724747598171234 train_loss 0.41023383373394606\n",
            "Acc 0.39267677068710327 train_loss 0.39171649959115756\n",
            "Acc 0.41161614656448364 train_loss 0.393919294022701\n",
            "Acc 0.4305555522441864 train_loss 0.39619834737285325\n",
            "Acc 0.44949495792388916 train_loss 0.397649130007873\n",
            "Acc 0.46717172861099243 train_loss 0.40028140500187875\n",
            "Acc 0.4861111044883728 train_loss 0.40954571690123814\n",
            "Acc 0.5050504803657532 train_loss 0.4047585522962941\n",
            "Acc 0.5233585834503174 train_loss 0.40834215669227497\n",
            "Acc 0.5404040217399597 train_loss 0.4313428192560015\n",
            "Acc 0.5587121248245239 train_loss 0.4325565562893947\n",
            "Acc 0.5770202279090881 train_loss 0.4349352761862739\n",
            "Acc 0.5965909361839294 train_loss 0.42573664302472025\n",
            "Acc 0.6155303120613098 train_loss 0.42127214473756874\n",
            "Acc 0.6319444179534912 train_loss 0.4331004169276532\n",
            "Validating....\n",
            "Acc 0.6508838534355164 train_loss 0.43652069962450435\n",
            "Acc 0.6704545617103577 train_loss 0.43129070237692857\n",
            "Acc 0.6875 train_loss 0.4434373794576606\n",
            "Acc 0.7051767706871033 train_loss 0.4477845741141784\n",
            "Acc 0.7234848737716675 train_loss 0.4535556478569141\n",
            "Acc 0.7398989796638489 train_loss 0.46089942725375294\n",
            "Acc 0.7588383555412292 train_loss 0.45649114805387286\n",
            "Acc 0.7758838534355164 train_loss 0.46358390409676803\n",
            "Acc 0.7948232293128967 train_loss 0.4628122716622297\n",
            "Acc 0.8118686676025391 train_loss 0.47172162927348504\n",
            "Acc 0.8301767706871033 train_loss 0.4667426928049988\n",
            "Acc 0.8484848737716675 train_loss 0.4659595266794381\n",
            "Acc 0.8661616444587708 train_loss 0.46395365204265776\n",
            "Acc 0.8851010203361511 train_loss 0.4604098421987146\n",
            "Acc 0.9046717286109924 train_loss 0.46158113140536816\n",
            "Acc 0.9135100841522217 train_loss 0.4678211412578821\n",
            "F1_score : 0.8427095292766935\n",
            "Epoch completed in 1.425468889872233 minutes\n",
            "***************\n",
            "EPOCH: 4\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.5779223442077637\n",
            "Acc : 0.009469697251915932 train_loss : 0.3878614008426666\n",
            "Acc : 0.014204545877873898 train_loss : 0.41077496608098346\n",
            "Acc : 0.018939394503831863 train_loss : 0.36040011420845985\n",
            "Acc : 0.02383207157254219 train_loss : 0.3478139728307724\n",
            "Acc : 0.028882576152682304 train_loss : 0.3047308027744293\n",
            "Acc : 0.03377525135874748 train_loss : 0.3013068948473249\n",
            "Acc : 0.03866792842745781 train_loss : 0.2770358631387353\n",
            "Acc : 0.0434027761220932 train_loss : 0.2712856787774298\n",
            "Acc : 0.04813762754201889 train_loss : 0.2693149887025356\n",
            "Acc : 0.052714645862579346 train_loss : 0.2742070440541614\n",
            "Acc : 0.0572916679084301 train_loss : 0.2894719659040372\n",
            "Acc : 0.06218434497714043 train_loss : 0.2758394926786423\n",
            "Acc : 0.06723485141992569 train_loss : 0.26485632359981537\n",
            "Acc : 0.07165403664112091 train_loss : 0.2780138651529948\n",
            "Acc : 0.0763888880610466 train_loss : 0.2764255106449127\n",
            "Acc : 0.08080808073282242 train_loss : 0.2942190626088311\n",
            "Acc : 0.08585858345031738 train_loss : 0.28947817120287156\n",
            "Acc : 0.09059343487024307 train_loss : 0.29530156442993566\n",
            "Acc : 0.09532828629016876 train_loss : 0.29886301010847094\n",
            "Acc : 0.10022095590829849 train_loss : 0.3000774255820683\n",
            "Acc : 0.10479798167943954 train_loss : 0.3091117631305348\n",
            "Acc : 0.10953282564878464 train_loss : 0.3056831217330435\n",
            "Acc : 0.11442550271749496 train_loss : 0.3030926262338956\n",
            "Acc : 0.11916035413742065 train_loss : 0.30372265219688416\n",
            "Acc : 0.12357954680919647 train_loss : 0.3086407517011349\n",
            "Acc : 0.12768307328224182 train_loss : 0.32680446019879095\n",
            "Acc : 0.13257575035095215 train_loss : 0.3264986478856632\n",
            "Acc : 0.13683712482452393 train_loss : 0.3426383065766302\n",
            "Acc : 0.14125631749629974 train_loss : 0.34401741325855256\n",
            "Acc : 0.14567551016807556 train_loss : 0.3460606625003199\n",
            "Acc : 0.15009470283985138 train_loss : 0.3566963691264391\n",
            "Acc : 0.15482954680919647 train_loss : 0.3545726682200576\n",
            "Acc : 0.15893307328224182 train_loss : 0.3646923496442683\n",
            "Acc : 0.1636679321527481 train_loss : 0.3610940320151193\n",
            "Acc : 0.16824494302272797 train_loss : 0.3616376900010639\n",
            "Acc : 0.17297980189323425 train_loss : 0.36030567014539566\n",
            "Acc : 0.17803029716014862 train_loss : 0.35562464634054586\n",
            "Acc : 0.18260732293128967 train_loss : 0.3547810128866098\n",
            "Acc : 0.18734216690063477 train_loss : 0.35590953938663006\n",
            "Acc : 0.19160354137420654 train_loss : 0.35908494889736176\n",
            "Acc : 0.1966540366411209 train_loss : 0.3545139630635579\n",
            "Acc : 0.20154671370983124 train_loss : 0.3549198619154997\n",
            "Acc : 0.20643939077854156 train_loss : 0.3546057695692236\n",
            "Acc : 0.21117424964904785 train_loss : 0.3522737430201636\n",
            "Acc : 0.21606691181659698 train_loss : 0.3496580243758533\n",
            "Acc : 0.2209595888853073 train_loss : 0.3464077010433725\n",
            "Acc : 0.22585226595401764 train_loss : 0.34343383988986415\n",
            "Acc : 0.23074494302272797 train_loss : 0.34103250138613644\n",
            "Acc : 0.23547980189323425 train_loss : 0.34142916917800903\n",
            "Acc : 0.24021464586257935 train_loss : 0.33958356666798684\n",
            "Acc : 0.2447916716337204 train_loss : 0.3420518467632624\n",
            "Acc : 0.24936868250370026 train_loss : 0.34284804480255776\n",
            "Acc : 0.2542613744735718 train_loss : 0.3403698037619944\n",
            "Acc : 0.25899621844291687 train_loss : 0.33890572650866074\n",
            "Acc : 0.26404672861099243 train_loss : 0.335084699892572\n",
            "Acc : 0.2687815725803375 train_loss : 0.333928241802935\n",
            "Acc : 0.2733585834503174 train_loss : 0.3326251308979659\n",
            "Acc : 0.2782512605190277 train_loss : 0.3298905279676793\n",
            "Acc : 0.2829861044883728 train_loss : 0.3297312378883362\n",
            "Acc : 0.28787878155708313 train_loss : 0.3266673674348925\n",
            "Acc : 0.2926136255264282 train_loss : 0.327665515965031\n",
            "Acc : 0.29750630259513855 train_loss : 0.32642420440439196\n",
            "Acc : 0.3020833432674408 train_loss : 0.3271936138626188\n",
            "Acc : 0.3068181872367859 train_loss : 0.3311538684826631\n",
            "Acc : 0.31186869740486145 train_loss : 0.32782620722145744\n",
            "Acc : 0.3167613744735718 train_loss : 0.3251117740326853\n",
            "Acc : 0.32133838534355164 train_loss : 0.33100481123170433\n",
            "Acc : 0.32607322931289673 train_loss : 0.32823811633431393\n",
            "Acc : 0.3308080732822418 train_loss : 0.32972588634916716\n",
            "Acc : 0.33538511395454407 train_loss : 0.3317668997275997\n",
            "Acc : 0.3402777910232544 train_loss : 0.3306956391574608\n",
            "Acc : 0.3450126349925995 train_loss : 0.3294981982405872\n",
            "Acc : 0.3494318127632141 train_loss : 0.33516802286376823\n",
            "Acc : 0.35432448983192444 train_loss : 0.3328733417391777\n",
            "Acc : 0.35921716690063477 train_loss : 0.3307295193601596\n",
            "Acc : 0.3636363744735718 train_loss : 0.3319200071228015\n",
            "Acc : 0.3685290515422821 train_loss : 0.33115539403680044\n",
            "Acc : 0.37342172861099243 train_loss : 0.3283170951129515\n",
            "Acc : 0.3784722089767456 train_loss : 0.32535446975380183\n",
            "Acc : 0.3832070827484131 train_loss : 0.3256085126856227\n",
            "Acc : 0.38778409361839294 train_loss : 0.32938965664404196\n",
            "Acc : 0.3928346037864685 train_loss : 0.32705073866499473\n",
            "Acc : 0.3975694477558136 train_loss : 0.3269851069365229\n",
            "Acc : 0.40214645862579346 train_loss : 0.326186563688166\n",
            "Acc : 0.4067234992980957 train_loss : 0.3294739646967067\n",
            "Acc : 0.41161614656448364 train_loss : 0.3274780600920491\n",
            "Acc : 0.41603535413742065 train_loss : 0.33095437017354096\n",
            "Acc : 0.420928031206131 train_loss : 0.32910961130361877\n",
            "Acc : 0.42597854137420654 train_loss : 0.3264636798865265\n",
            "Acc : 0.43087121844291687 train_loss : 0.32542743206351665\n",
            "Acc : 0.43560606241226196 train_loss : 0.32447346518545045\n",
            "Acc : 0.44034090638160706 train_loss : 0.3269077654006661\n",
            "Acc : 0.4452335834503174 train_loss : 0.3249624667174004\n",
            "Acc : 0.44981059432029724 train_loss : 0.3252260466155253\n",
            "Acc : 0.4545454680919647 train_loss : 0.3258787279482931\n",
            "Acc : 0.4592803120613098 train_loss : 0.3256730754504499\n",
            "Acc : 0.46417298913002014 train_loss : 0.3239028150481837\n",
            "Acc : 0.46906566619873047 train_loss : 0.32254187679953045\n",
            "Acc : 0.47411614656448364 train_loss : 0.31985550574958327\n",
            "Acc : 0.47900882363319397 train_loss : 0.3184949170687411\n",
            "Acc : 0.4835858643054962 train_loss : 0.31958990259205594\n",
            "Acc : 0.4883207082748413 train_loss : 0.31878012729790606\n",
            "Acc : 0.4930555522441864 train_loss : 0.31990380148188424\n",
            "Acc : 0.49794822931289673 train_loss : 0.3191566133073398\n",
            "Acc : 0.5023674368858337 train_loss : 0.3216995057772915\n",
            "Acc : 0.5066288113594055 train_loss : 0.32467485873777174\n",
            "Acc : 0.5115214586257935 train_loss : 0.3230398877489346\n",
            "Acc : 0.5160984992980957 train_loss : 0.324110852537352\n",
            "Acc : 0.5208333134651184 train_loss : 0.32380395809357815\n",
            "Acc : 0.5255681872367859 train_loss : 0.3231592176331056\n",
            "Acc : 0.5301452279090881 train_loss : 0.32462401162566884\n",
            "Acc : 0.5350378751754761 train_loss : 0.3242669053441655\n",
            "Acc : 0.5400883555412292 train_loss : 0.3227287309342309\n",
            "Acc : 0.5446653962135315 train_loss : 0.32421629564917603\n",
            "Acc : 0.5495581030845642 train_loss : 0.32286137284646776\n",
            "Acc : 0.5546085834503174 train_loss : 0.32097111113815224\n",
            "Acc : 0.5590277910232544 train_loss : 0.3213881538959883\n",
            "Acc : 0.5640782713890076 train_loss : 0.3200604153656158\n",
            "Acc : 0.5686553120613098 train_loss : 0.32050630717227857\n",
            "Acc : 0.5733901262283325 train_loss : 0.32002312948634803\n",
            "Acc : 0.5779671669006348 train_loss : 0.3241502419724816\n",
            "Acc : 0.5828598737716675 train_loss : 0.3243120588301643\n",
            "Acc : 0.5879103541374207 train_loss : 0.32294055825519946\n",
            "Acc : 0.5928030014038086 train_loss : 0.32124302929639814\n",
            "Acc : 0.5972222089767456 train_loss : 0.3213182512138571\n",
            "Acc : 0.6017992496490479 train_loss : 0.321089861841183\n",
            "Acc : 0.6066918969154358 train_loss : 0.32095099514117464\n",
            "Acc : 0.6111111044883728 train_loss : 0.32183080397604047\n",
            "Acc : 0.6160038113594055 train_loss : 0.3215679906308651\n",
            "Acc : 0.6208964586257935 train_loss : 0.3216583184499777\n",
            "Acc : 0.6254734992980957 train_loss : 0.32312305288558657\n",
            "Acc : 0.6303661465644836 train_loss : 0.3222000366426948\n",
            "Acc : 0.6346275210380554 train_loss : 0.32562733591714904\n",
            "Acc : 0.6395202279090881 train_loss : 0.32429363291572644\n",
            "Acc : 0.6442550420761108 train_loss : 0.32432875875383615\n",
            "Acc : 0.6491477489471436 train_loss : 0.32397153913757226\n",
            "Acc : 0.6540403962135315 train_loss : 0.3229234465330407\n",
            "Acc : 0.6590909361839294 train_loss : 0.3212157189846039\n",
            "Acc : 0.6635100841522217 train_loss : 0.32184278688260487\n",
            "Acc : 0.6677714586257935 train_loss : 0.32480854747143195\n",
            "Acc : 0.6725063323974609 train_loss : 0.32571533826035515\n",
            "Acc : 0.6770833134651184 train_loss : 0.325383697981601\n",
            "Acc : 0.6818181872367859 train_loss : 0.3248892358193795\n",
            "Acc : 0.6867108345031738 train_loss : 0.32431426562111954\n",
            "Acc : 0.6914457082748413 train_loss : 0.3238766707786142\n",
            "Acc : 0.6963383555412292 train_loss : 0.3226251500804408\n",
            "Acc : 0.7010732293128967 train_loss : 0.32279771667074514\n",
            "Acc : 0.7058081030845642 train_loss : 0.32215812432285923\n",
            "Acc : 0.7107007503509521 train_loss : 0.3220208868384361\n",
            "Acc : 0.7155934572219849 train_loss : 0.32171103723396527\n",
            "Acc : 0.7204861044883728 train_loss : 0.3204995960389313\n",
            "Acc : 0.725063145160675 train_loss : 0.32091893479714984\n",
            "Acc : 0.7301136255264282 train_loss : 0.3203287452652857\n",
            "Acc : 0.7348484992980957 train_loss : 0.31971606564137245\n",
            "Acc : 0.7395833134651184 train_loss : 0.31970697841965234\n",
            "Acc : 0.7444760203361511 train_loss : 0.3192579079015999\n",
            "Acc : 0.7492108345031738 train_loss : 0.3192585351157792\n",
            "Acc : 0.7539457082748413 train_loss : 0.31929017509679375\n",
            "Acc : 0.7582070827484131 train_loss : 0.319849303457886\n",
            "Acc : 0.7629418969154358 train_loss : 0.3204100182893113\n",
            "Acc : 0.7679924368858337 train_loss : 0.31919592467171176\n",
            "Acc : 0.7724116444587708 train_loss : 0.3201759005525361\n",
            "Acc : 0.7769886255264282 train_loss : 0.3211396363359399\n",
            "Acc : 0.7820391654968262 train_loss : 0.31966113249460854\n",
            "Acc : 0.7869318127632141 train_loss : 0.31861590549170254\n",
            "Acc : 0.7918245196342468 train_loss : 0.3177467670269355\n",
            "Acc : 0.7965593338012695 train_loss : 0.31729310076861156\n",
            "Acc : 0.8014520406723022 train_loss : 0.31631878600318053\n",
            "Acc : 0.8063446879386902 train_loss : 0.3150481248164878\n",
            "Acc : 0.8110795617103577 train_loss : 0.31481331896189363\n",
            "Acc : 0.8158143758773804 train_loss : 0.3161583334467439\n",
            "Acc : 0.8207070827484131 train_loss : 0.31553908170475437\n",
            "Acc : 0.8254418969154358 train_loss : 0.31593469474678754\n",
            "Acc : 0.830018937587738 train_loss : 0.3157912462524005\n",
            "Acc : 0.8350694179534912 train_loss : 0.31469007149677386\n",
            "Acc : 0.8398042917251587 train_loss : 0.31426547695014434\n",
            "Acc : 0.8446969985961914 train_loss : 0.3140294874986906\n",
            "Acc : 0.8495896458625793 train_loss : 0.3136310308505703\n",
            "Acc : 0.8546401262283325 train_loss : 0.31259059235453607\n",
            "Acc : 0.859375 train_loss : 0.31200618210418446\n",
            "Acc : 0.8639520406723022 train_loss : 0.3123509571119979\n",
            "Acc : 0.8690025210380554 train_loss : 0.31112831892048726\n",
            "Acc : 0.8737373948097229 train_loss : 0.3110433856997153\n",
            "Acc : 0.8786300420761108 train_loss : 0.31063101803128784\n",
            "Acc : 0.8833649158477783 train_loss : 0.3103157066770138\n",
            "Acc : 0.8884153962135315 train_loss : 0.3093215613202615\n",
            "Acc : 0.8934659361839294 train_loss : 0.30809816971738285\n",
            "Acc : 0.8980429172515869 train_loss : 0.3091021246380276\n",
            "Acc : 0.9030934572219849 train_loss : 0.30768558432004955\n",
            "Acc : 0.908143937587738 train_loss : 0.3061897366244287\n",
            "Acc : 0.9130366444587708 train_loss : 0.3052785462641623\n",
            "Acc : 0.9179292917251587 train_loss : 0.30435936416407633\n",
            "Acc : 0.9228219985961914 train_loss : 0.3034229587437105\n",
            "Acc : 0.9277146458625793 train_loss : 0.30269161769403863\n",
            "Acc : 0.9327651262283325 train_loss : 0.30170737686852106\n",
            "Acc : 0.9373421669006348 train_loss : 0.3024069498278919\n",
            "Acc : 0.941919207572937 train_loss : 0.303018936840347\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.7632761001586914\n",
            "Acc 0.036616161465644836 train_loss 0.6878200471401215\n",
            "Acc 0.05681818351149559 train_loss 0.4704802980025609\n",
            "Acc 0.07575757801532745 train_loss 0.4214017800986767\n",
            "Acc 0.09469696879386902 train_loss 0.4352046102285385\n",
            "Acc 0.11237373948097229 train_loss 0.4424252286553383\n",
            "Acc 0.12815657258033752 train_loss 0.5333995457206454\n",
            "Acc 0.14646464586257935 train_loss 0.5384162273257971\n",
            "Acc 0.16414141654968262 train_loss 0.5436237437857522\n",
            "Acc 0.18308080732822418 train_loss 0.5487416788935662\n",
            "Acc 0.2013888955116272 train_loss 0.6009226427836851\n",
            "Acc 0.21780303120613098 train_loss 0.6458242672185103\n",
            "Acc 0.2361111044883728 train_loss 0.6494465917348862\n",
            "Acc 0.25505051016807556 train_loss 0.6262973451188633\n",
            "Acc 0.27462121844291687 train_loss 0.5964044362306595\n",
            "Acc 0.29229798913002014 train_loss 0.603728230111301\n",
            "Acc 0.3112373650074005 train_loss 0.5892354206127279\n",
            "Acc 0.3295454680919647 train_loss 0.5830554208821721\n",
            "Acc 0.34911614656448364 train_loss 0.558854836382364\n",
            "Acc 0.3693181872367859 train_loss 0.5345991808921099\n",
            "Acc 0.38952019810676575 train_loss 0.5100339965096542\n",
            "Acc 0.40909090638160706 train_loss 0.5051949730312283\n",
            "Acc 0.42866161465644836 train_loss 0.502729206143514\n",
            "Acc 0.4457070827484131 train_loss 0.5009546380800506\n",
            "Acc 0.4627525210380554 train_loss 0.5165434004366398\n",
            "Acc 0.4804292917251587 train_loss 0.5246760594443634\n",
            "Acc 0.5 train_loss 0.5160341197969737\n",
            "Acc 0.5183081030845642 train_loss 0.5205339599134666\n",
            "Acc 0.5359848737716675 train_loss 0.547315533947328\n",
            "Acc 0.5536616444587708 train_loss 0.5430007965614398\n",
            "Acc 0.5719696879386902 train_loss 0.5416439244103047\n",
            "Acc 0.5915403962135315 train_loss 0.5310377829009667\n",
            "Acc 0.6104797720909119 train_loss 0.5260959110702529\n",
            "Acc 0.627525269985199 train_loss 0.5351843681624707\n",
            "Acc 0.6458333134651184 train_loss 0.5361721980784621\n",
            "Acc 0.6647727489471436 train_loss 0.5325255183916953\n",
            "Acc 0.6818181872367859 train_loss 0.5487130965936828\n",
            "Acc 0.6994949579238892 train_loss 0.5549140640191341\n",
            "Acc 0.7159090638160706 train_loss 0.5593643987981173\n",
            "Acc 0.7323232293128967 train_loss 0.570602608192712\n",
            "Acc 0.7512626051902771 train_loss 0.5677651156921212\n",
            "Acc 0.7689393758773804 train_loss 0.5760559401519242\n",
            "Acc 0.7872474789619446 train_loss 0.5732306412659413\n",
            "Acc 0.8049242496490479 train_loss 0.5790631585669789\n",
            "Acc 0.8238636255264282 train_loss 0.5723144840035174\n",
            "Acc 0.8409090638160706 train_loss 0.5796389026648324\n",
            "Acc 0.8592171669006348 train_loss 0.5774273540269821\n",
            "Acc 0.877525269985199 train_loss 0.5752590196983268\n",
            "Acc 0.8970959782600403 train_loss 0.5740815715066024\n",
            "Acc 0.9059343338012695 train_loss 0.5796957383304835\n",
            "F1_score : 0.8193939393939395\n",
            "Epoch completed in 1.4282840212186179 minutes\n",
            "***************\n",
            "EPOCH: 5\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.6132026314735413\n",
            "Acc : 0.009469697251915932 train_loss : 0.3912162706255913\n",
            "Acc : 0.014204545877873898 train_loss : 0.3460191736618678\n",
            "Acc : 0.018939394503831863 train_loss : 0.3092740811407566\n",
            "Acc : 0.023516414687037468 train_loss : 0.33731691539287567\n",
            "Acc : 0.028566919267177582 train_loss : 0.2996800070007642\n",
            "Acc : 0.03345959633588791 train_loss : 0.27732505968638826\n",
            "Acc : 0.03851010277867317 train_loss : 0.24796794587746263\n",
            "Acc : 0.04356060549616814 train_loss : 0.22775257006287575\n",
            "Acc : 0.04829545319080353 train_loss : 0.2287527684122324\n",
            "Acc : 0.05303030461072922 train_loss : 0.2360473982989788\n",
            "Acc : 0.05760732293128967 train_loss : 0.2476012511178851\n",
            "Acc : 0.0625 train_loss : 0.24134375985998374\n",
            "Acc : 0.06739267706871033 train_loss : 0.23128376289137773\n",
            "Acc : 0.07196969538927078 train_loss : 0.25176768327752747\n",
            "Acc : 0.07686237245798111 train_loss : 0.2510932779405266\n",
            "Acc : 0.08128156512975693 train_loss : 0.27284300524522276\n",
            "Acc : 0.08617424219846725 train_loss : 0.26439042451481026\n",
            "Acc : 0.09090909361839294 train_loss : 0.2742755356195726\n",
            "Acc : 0.09564393758773804 train_loss : 0.2784328391775489\n",
            "Acc : 0.10053661465644836 train_loss : 0.2769306449308282\n",
            "Acc : 0.10527146607637405 train_loss : 0.2862553831867196\n",
            "Acc : 0.11016414314508438 train_loss : 0.28370919937024947\n",
            "Acc : 0.1145833358168602 train_loss : 0.28634861096118885\n",
            "Acc : 0.11931817978620529 train_loss : 0.28738792046904565\n",
            "Acc : 0.12373737245798111 train_loss : 0.29562951839314056\n",
            "Acc : 0.1279987394809723 train_loss : 0.3109896170596282\n",
            "Acc : 0.13273358345031738 train_loss : 0.30933072191796135\n",
            "Acc : 0.13731060922145844 train_loss : 0.31574109183817073\n",
            "Acc : 0.14220328629016876 train_loss : 0.3120076938221852\n",
            "Acc : 0.14678029716014862 train_loss : 0.31178033580222436\n",
            "Acc : 0.15119948983192444 train_loss : 0.33123921451624483\n",
            "Acc : 0.15609216690063477 train_loss : 0.32684399763291533\n",
            "Acc : 0.16066919267177582 train_loss : 0.33565211723394256\n",
            "Acc : 0.16556186974048615 train_loss : 0.32973076094474113\n",
            "Acc : 0.16998106241226196 train_loss : 0.33080828013933367\n",
            "Acc : 0.1748737394809723 train_loss : 0.3285970888226419\n",
            "Acc : 0.17976641654968262 train_loss : 0.32516943888836786\n",
            "Acc : 0.18434342741966248 train_loss : 0.32634646111191845\n",
            "Acc : 0.18907828629016876 train_loss : 0.3288256938569248\n",
            "Acc : 0.19333964586257935 train_loss : 0.33151456159425946\n",
            "Acc : 0.1983901560306549 train_loss : 0.32696205297751085\n",
            "Acc : 0.20344065129756927 train_loss : 0.3248825207525908\n",
            "Acc : 0.2083333283662796 train_loss : 0.3219474641267549\n",
            "Acc : 0.2130681872367859 train_loss : 0.31912243192394574\n",
            "Acc : 0.21796086430549622 train_loss : 0.31714802454023255\n",
            "Acc : 0.22285354137420654 train_loss : 0.31405886682741185\n",
            "Acc : 0.22774621844291687 train_loss : 0.3113176377955824\n",
            "Acc : 0.2326388955116272 train_loss : 0.30839294552498936\n",
            "Acc : 0.23721590638160706 train_loss : 0.3076028060168028\n",
            "Acc : 0.24195075035095215 train_loss : 0.30811386358212023\n",
            "Acc : 0.2465277761220932 train_loss : 0.31182213235073364\n",
            "Acc : 0.25110480189323425 train_loss : 0.3136768064830663\n",
            "Acc : 0.2561553120613098 train_loss : 0.31050655211287514\n",
            "Acc : 0.26104798913002014 train_loss : 0.30745913677594877\n",
            "Acc : 0.26594066619873047 train_loss : 0.30453420902735423\n",
            "Acc : 0.27067551016807556 train_loss : 0.3036310551851465\n",
            "Acc : 0.27541035413742065 train_loss : 0.3026509942037278\n",
            "Acc : 0.280303031206131 train_loss : 0.30024085503260967\n",
            "Acc : 0.2851957082748413 train_loss : 0.30003525260835884\n",
            "Acc : 0.29024621844291687 train_loss : 0.2972290439561742\n",
            "Acc : 0.29482322931289673 train_loss : 0.29852811193033574\n",
            "Acc : 0.2995580732822418 train_loss : 0.2984452202088303\n",
            "Acc : 0.3042929172515869 train_loss : 0.29765977122588083\n",
            "Acc : 0.3090277910232544 train_loss : 0.300723568464701\n",
            "Acc : 0.3139204680919647 train_loss : 0.2991400897841562\n",
            "Acc : 0.31881314516067505 train_loss : 0.29677072495444495\n",
            "Acc : 0.32354798913002014 train_loss : 0.2988655371591449\n",
            "Acc : 0.3285984992980957 train_loss : 0.2955501231270424\n",
            "Acc : 0.33317551016807556 train_loss : 0.2953317212739161\n",
            "Acc : 0.33791035413742065 train_loss : 0.29682043871619335\n",
            "Acc : 0.342803031206131 train_loss : 0.2961393222730193\n",
            "Acc : 0.34738004207611084 train_loss : 0.29602855938958794\n",
            "Acc : 0.3519570827484131 train_loss : 0.2976624176812333\n",
            "Acc : 0.35700756311416626 train_loss : 0.29506169820825257\n",
            "Acc : 0.3619002401828766 train_loss : 0.29294989262952614\n",
            "Acc : 0.36647728085517883 train_loss : 0.2932997861175568\n",
            "Acc : 0.3712121248245239 train_loss : 0.293858371149653\n",
            "Acc : 0.37610480189323425 train_loss : 0.29138838476206685\n",
            "Acc : 0.3811553120613098 train_loss : 0.288421900710091\n",
            "Acc : 0.3858901560306549 train_loss : 0.2876661281434842\n",
            "Acc : 0.390625 train_loss : 0.29162618095373233\n",
            "Acc : 0.39567551016807556 train_loss : 0.288531246808279\n",
            "Acc : 0.40041035413742065 train_loss : 0.28972256480760517\n",
            "Acc : 0.4054608643054962 train_loss : 0.28729577375685467\n",
            "Acc : 0.41035354137420654 train_loss : 0.2878920698841644\n",
            "Acc : 0.41524621844291687 train_loss : 0.2862524908763924\n",
            "Acc : 0.41982322931289673 train_loss : 0.28936166904697364\n",
            "Acc : 0.42471590638160706 train_loss : 0.28765507423308456\n",
            "Acc : 0.4296085834503174 train_loss : 0.28579380764729445\n",
            "Acc : 0.4345012605190277 train_loss : 0.2847486019216396\n",
            "Acc : 0.43939393758773804 train_loss : 0.2837215212133268\n",
            "Acc : 0.44412878155708313 train_loss : 0.2852691687963983\n",
            "Acc : 0.4491792917251587 train_loss : 0.28307492924021915\n",
            "Acc : 0.45375630259513855 train_loss : 0.2841762353323008\n",
            "Acc : 0.45849114656448364 train_loss : 0.2850864016218111\n",
            "Acc : 0.4632260203361511 train_loss : 0.28444013279891506\n",
            "Acc : 0.46811869740486145 train_loss : 0.2825580891982025\n",
            "Acc : 0.47285354137420654 train_loss : 0.2819129399052172\n",
            "Acc : 0.4779040515422821 train_loss : 0.27946686558425426\n",
            "Acc : 0.48279672861099243 train_loss : 0.2780957874832767\n",
            "Acc : 0.4875315725803375 train_loss : 0.2791815742704214\n",
            "Acc : 0.49242424964904785 train_loss : 0.2777122696771205\n",
            "Acc : 0.49715909361839294 train_loss : 0.27994248593369353\n",
            "Acc : 0.5020517706871033 train_loss : 0.27881501863400143\n",
            "Acc : 0.5064709782600403 train_loss : 0.28258617875992126\n",
            "Acc : 0.5108901262283325 train_loss : 0.2850063254482278\n",
            "Acc : 0.5157828330993652 train_loss : 0.28361535796688664\n",
            "Acc : 0.5205176472663879 train_loss : 0.2829455094189819\n",
            "Acc : 0.5252525210380554 train_loss : 0.2819958881221034\n",
            "Acc : 0.5301452279090881 train_loss : 0.2815227694473825\n",
            "Acc : 0.5348800420761108 train_loss : 0.2823660403623113\n",
            "Acc : 0.5397727489471436 train_loss : 0.282025147789875\n",
            "Acc : 0.5448232293128967 train_loss : 0.2804579361619657\n",
            "Acc : 0.5495581030845642 train_loss : 0.28212138109880947\n",
            "Acc : 0.5544507503509521 train_loss : 0.28071900681945783\n",
            "Acc : 0.5595012903213501 train_loss : 0.2788414546630831\n",
            "Acc : 0.5640782713890076 train_loss : 0.2807581176255214\n",
            "Acc : 0.5691288113594055 train_loss : 0.27949334558944744\n",
            "Acc : 0.5738636255264282 train_loss : 0.27985341465100644\n",
            "Acc : 0.5789141654968262 train_loss : 0.2780596367275912\n",
            "Acc : 0.5834911465644836 train_loss : 0.28164398422861686\n",
            "Acc : 0.5883838534355164 train_loss : 0.2817591387263643\n",
            "Acc : 0.5932765007019043 train_loss : 0.2807082711388507\n",
            "Acc : 0.5983270406723022 train_loss : 0.27900930562615395\n",
            "Acc : 0.603061854839325 train_loss : 0.2789935925236297\n",
            "Acc : 0.6077967286109924 train_loss : 0.278522540177182\n",
            "Acc : 0.6126893758773804 train_loss : 0.2780533207405824\n",
            "Acc : 0.6174242496490479 train_loss : 0.2782889766575292\n",
            "Acc : 0.6221590638160706 train_loss : 0.27843140985530157\n",
            "Acc : 0.6270517706871033 train_loss : 0.27799550896041264\n",
            "Acc : 0.6316288113594055 train_loss : 0.2800477390092882\n",
            "Acc : 0.6366792917251587 train_loss : 0.27889182303745047\n",
            "Acc : 0.6412563323974609 train_loss : 0.2814507930636851\n",
            "Acc : 0.6463068127632141 train_loss : 0.27971216870678794\n",
            "Acc : 0.6511995196342468 train_loss : 0.279335181831437\n",
            "Acc : 0.6560921669006348 train_loss : 0.2792175896193859\n",
            "Acc : 0.6609848737716675 train_loss : 0.2782215457679569\n",
            "Acc : 0.6660353541374207 train_loss : 0.27667043588573126\n",
            "Acc : 0.6707702279090881 train_loss : 0.2762258793626513\n",
            "Acc : 0.6748737096786499 train_loss : 0.27967653037808465\n",
            "Acc : 0.6796085834503174 train_loss : 0.28087331564493584\n",
            "Acc : 0.6845012903213501 train_loss : 0.28002750883986066\n",
            "Acc : 0.689393937587738 train_loss : 0.27986633415437406\n",
            "Acc : 0.6942866444587708 train_loss : 0.279669913649559\n",
            "Acc : 0.6990214586257935 train_loss : 0.27974796672798186\n",
            "Acc : 0.7039141654968262 train_loss : 0.2790120690047335\n",
            "Acc : 0.7086489796638489 train_loss : 0.2791955869745564\n",
            "Acc : 0.7133838534355164 train_loss : 0.2783315086724774\n",
            "Acc : 0.7181186676025391 train_loss : 0.27867009152968725\n",
            "Acc : 0.7228535413742065 train_loss : 0.27878338503127065\n",
            "Acc : 0.7279040217399597 train_loss : 0.27746634807829795\n",
            "Acc : 0.7327967286109924 train_loss : 0.2769967255736488\n",
            "Acc : 0.7376893758773804 train_loss : 0.27634552203409085\n",
            "Acc : 0.7425820827484131 train_loss : 0.2758142405459958\n",
            "Acc : 0.747474730014801 train_loss : 0.2752419191961869\n",
            "Acc : 0.7522096037864685 train_loss : 0.2757649899573083\n",
            "Acc : 0.7567866444587708 train_loss : 0.27606178400448605\n",
            "Acc : 0.7616792917251587 train_loss : 0.27616181102751186\n",
            "Acc : 0.7665719985961914 train_loss : 0.2757082687225193\n",
            "Acc : 0.7714646458625793 train_loss : 0.27581617101934386\n",
            "Acc : 0.7763572931289673 train_loss : 0.2752926773219197\n",
            "Acc : 0.7809343338012695 train_loss : 0.2754568708546323\n",
            "Acc : 0.7853535413742065 train_loss : 0.2760642638384569\n",
            "Acc : 0.7904040217399597 train_loss : 0.2750536188483238\n",
            "Acc : 0.7954545617103577 train_loss : 0.27375401504876384\n",
            "Acc : 0.8003472089767456 train_loss : 0.2729216215540906\n",
            "Acc : 0.8052399158477783 train_loss : 0.27249585644208957\n",
            "Acc : 0.8101325631141663 train_loss : 0.2714148851761804\n",
            "Acc : 0.815025269985199 train_loss : 0.27046662746545147\n",
            "Acc : 0.8197600841522217 train_loss : 0.27065127881036866\n",
            "Acc : 0.8244949579238892 train_loss : 0.27166917593066775\n",
            "Acc : 0.8293876051902771 train_loss : 0.2710689675127495\n",
            "Acc : 0.8341224789619446 train_loss : 0.27128355703905394\n",
            "Acc : 0.8390151262283325 train_loss : 0.2707939589236464\n",
            "Acc : 0.8440656661987305 train_loss : 0.2698141714685004\n",
            "Acc : 0.8488004803657532 train_loss : 0.26964871010790437\n",
            "Acc : 0.8536931872367859 train_loss : 0.269095022252269\n",
            "Acc : 0.8585858345031738 train_loss : 0.26912363437550696\n",
            "Acc : 0.8636363744735718 train_loss : 0.26822702584581243\n",
            "Acc : 0.8685290217399597 train_loss : 0.26755015624079914\n",
            "Acc : 0.8732638955116272 train_loss : 0.2680999850956621\n",
            "Acc : 0.8781565427780151 train_loss : 0.2674223510090445\n",
            "Acc : 0.8828914165496826 train_loss : 0.2674679647442763\n",
            "Acc : 0.8877840638160706 train_loss : 0.26722153697062184\n",
            "Acc : 0.8923611044883728 train_loss : 0.2673605858117983\n",
            "Acc : 0.8974116444587708 train_loss : 0.26624212525585755\n",
            "Acc : 0.9024621248245239 train_loss : 0.2651893310645159\n",
            "Acc : 0.9068813323974609 train_loss : 0.2672745895685342\n",
            "Acc : 0.9119318127632141 train_loss : 0.26618122332974486\n",
            "Acc : 0.9169822931289673 train_loss : 0.2649194345911916\n",
            "Acc : 0.921875 train_loss : 0.2645353201175264\n",
            "Acc : 0.9267676472663879 train_loss : 0.26381742109790673\n",
            "Acc : 0.9318181872367859 train_loss : 0.2629264839780853\n",
            "Acc : 0.9367108345031738 train_loss : 0.2623614290681405\n",
            "Acc : 0.9416035413742065 train_loss : 0.2616193922597687\n",
            "Acc : 0.9463383555412292 train_loss : 0.2621865529793925\n",
            "Acc : 0.9509153962135315 train_loss : 0.2627942030873112\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.7267831563949585\n",
            "Acc 0.03598484769463539 train_loss 0.724101334810257\n",
            "Acc 0.056186869740486145 train_loss 0.4932279313604037\n",
            "Acc 0.07323232293128967 train_loss 0.4974579308182001\n",
            "Acc 0.09217171370983124 train_loss 0.5069920852780342\n",
            "Acc 0.11047979444265366 train_loss 0.5284219148258368\n",
            "Acc 0.1262626200914383 train_loss 0.607316184256758\n",
            "Acc 0.1445707082748413 train_loss 0.5986054064705968\n",
            "Acc 0.16224747896194458 train_loss 0.616697302295102\n",
            "Acc 0.18118686974048615 train_loss 0.616340009123087\n",
            "Acc 0.19886364042758942 train_loss 0.667651006443934\n",
            "Acc 0.21590909361839294 train_loss 0.7062169145792723\n",
            "Acc 0.23421716690063477 train_loss 0.693725983110758\n",
            "Acc 0.2531565725803375 train_loss 0.672128961022411\n",
            "Acc 0.27146464586257935 train_loss 0.6400143081943194\n",
            "Acc 0.2891414165496826 train_loss 0.6436266168020666\n",
            "Acc 0.3080808222293854 train_loss 0.6310732570641181\n",
            "Acc 0.32575756311416626 train_loss 0.6211307591034306\n",
            "Acc 0.34532827138900757 train_loss 0.5961016600853518\n",
            "Acc 0.3655303120613098 train_loss 0.5693716328591109\n",
            "Acc 0.3857323229312897 train_loss 0.5436135612073398\n",
            "Acc 0.405303031206131 train_loss 0.5368050780485977\n",
            "Acc 0.4248737394809723 train_loss 0.5332364009126372\n",
            "Acc 0.4419191777706146 train_loss 0.5408098706975579\n",
            "Acc 0.45896464586257935 train_loss 0.5537647184729576\n",
            "Acc 0.4766414165496826 train_loss 0.5605337192805914\n",
            "Acc 0.4955808222293854 train_loss 0.5515247205341304\n",
            "Acc 0.5138888955116272 train_loss 0.5549821063343968\n",
            "Acc 0.5315656661987305 train_loss 0.5817884676929178\n",
            "Acc 0.5498737096786499 train_loss 0.5759325144191583\n",
            "Acc 0.5675504803657532 train_loss 0.5764605688952631\n",
            "Acc 0.5871211886405945 train_loss 0.5677909085061401\n",
            "Acc 0.6060606241226196 train_loss 0.5633086677302014\n",
            "Acc 0.623106062412262 train_loss 0.5744681101949776\n",
            "Acc 0.6414141654968262 train_loss 0.5746126215372767\n",
            "Acc 0.6603535413742065 train_loss 0.5687264210234085\n",
            "Acc 0.6773989796638489 train_loss 0.5840144936700125\n",
            "Acc 0.6950757503509521 train_loss 0.5860225407308653\n",
            "Acc 0.7121211886405945 train_loss 0.5902208768022366\n",
            "Acc 0.7279040217399597 train_loss 0.5999824261292815\n",
            "Acc 0.745580792427063 train_loss 0.5971521544383793\n",
            "Acc 0.7626262903213501 train_loss 0.6061870312052113\n",
            "Acc 0.7809343338012695 train_loss 0.6029354311352553\n",
            "Acc 0.7986111044883728 train_loss 0.6074578171087937\n",
            "Acc 0.8175504803657532 train_loss 0.6010116370187866\n",
            "Acc 0.8345959782600403 train_loss 0.6004262973109017\n",
            "Acc 0.8529040217399597 train_loss 0.6001637113538194\n",
            "Acc 0.8712121248245239 train_loss 0.6004549319234987\n",
            "Acc 0.8901515007019043 train_loss 0.6002201746617045\n",
            "Acc 0.8989899158477783 train_loss 0.6078935424983501\n",
            "F1_score : 0.8048780487804876\n",
            "Epoch completed in 1.4387404163678488 minutes\n",
            "***************\n",
            "EPOCH: 6\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.0045770201832056046 train_loss : 0.6257428526878357\n",
            "Acc : 0.009469697251915932 train_loss : 0.3885062411427498\n",
            "Acc : 0.014362373389303684 train_loss : 0.326187123854955\n",
            "Acc : 0.019255051389336586 train_loss : 0.266947353258729\n",
            "Acc : 0.024147726595401764 train_loss : 0.25236104279756544\n",
            "Acc : 0.028882576152682304 train_loss : 0.23680761580665907\n",
            "Acc : 0.03377525135874748 train_loss : 0.21818247118166514\n",
            "Acc : 0.038825757801532745 train_loss : 0.19659907231107354\n",
            "Acc : 0.04387626424431801 train_loss : 0.183923383967744\n",
            "Acc : 0.04876893758773804 train_loss : 0.18556479699909686\n",
            "Acc : 0.053661614656448364 train_loss : 0.18993892215869643\n",
            "Acc : 0.058396466076374054 train_loss : 0.20307602267712355\n",
            "Acc : 0.06344696879386902 train_loss : 0.1905202240898059\n",
            "Acc : 0.06833964586257935 train_loss : 0.1870963131742818\n",
            "Acc : 0.0729166641831398 train_loss : 0.2090170368552208\n",
            "Acc : 0.07765151560306549 train_loss : 0.21087336586788297\n",
            "Acc : 0.08207070827484131 train_loss : 0.22863547284813487\n",
            "Acc : 0.08696338534355164 train_loss : 0.2245918284687731\n",
            "Acc : 0.09169822931289673 train_loss : 0.23564865283275904\n",
            "Acc : 0.09643308073282242 train_loss : 0.2403578046709299\n",
            "Acc : 0.10132575780153275 train_loss : 0.2338558595095362\n",
            "Acc : 0.10621843487024307 train_loss : 0.24085150692950597\n",
            "Acc : 0.1111111119389534 train_loss : 0.23921986021425412\n",
            "Acc : 0.11600378900766373 train_loss : 0.2374034272506833\n",
            "Acc : 0.12089646607637405 train_loss : 0.23459021121263504\n",
            "Acc : 0.12531565129756927 train_loss : 0.24686434893653944\n",
            "Acc : 0.1297348439693451 train_loss : 0.2594564339077031\n",
            "Acc : 0.13462752103805542 train_loss : 0.2610949482768774\n",
            "Acc : 0.13952019810676575 train_loss : 0.2618335508580866\n",
            "Acc : 0.14441287517547607 train_loss : 0.259221792469422\n",
            "Acc : 0.14898990094661713 train_loss : 0.2616863531931754\n",
            "Acc : 0.15372474491596222 train_loss : 0.27393938531167805\n",
            "Acc : 0.15861742198467255 train_loss : 0.2695060162381692\n",
            "Acc : 0.1631944477558136 train_loss : 0.28347272798419\n",
            "Acc : 0.16824494302272797 train_loss : 0.27763745720897404\n",
            "Acc : 0.17297980189323425 train_loss : 0.2777401086770826\n",
            "Acc : 0.17787247896194458 train_loss : 0.2758364325036874\n",
            "Acc : 0.18292297422885895 train_loss : 0.27216549237307747\n",
            "Acc : 0.18765783309936523 train_loss : 0.27420429789867157\n",
            "Acc : 0.19255051016807556 train_loss : 0.27472205329686405\n",
            "Acc : 0.19681186974048615 train_loss : 0.2789169122896543\n",
            "Acc : 0.20170454680919647 train_loss : 0.2773209318873428\n",
            "Acc : 0.20675505697727203 train_loss : 0.2746101713111234\n",
            "Acc : 0.21164773404598236 train_loss : 0.2710404321551323\n",
            "Acc : 0.2165404111146927 train_loss : 0.2683290137184991\n",
            "Acc : 0.22143307328224182 train_loss : 0.26617737794699875\n",
            "Acc : 0.22632575035095215 train_loss : 0.26337302967588955\n",
            "Acc : 0.23121842741966248 train_loss : 0.2615992845967412\n",
            "Acc : 0.2361111044883728 train_loss : 0.25942634106898793\n",
            "Acc : 0.24116161465644836 train_loss : 0.2575461900234222\n",
            "Acc : 0.24558080732822418 train_loss : 0.2583257106004977\n",
            "Acc : 0.25031566619873047 train_loss : 0.260624498128891\n",
            "Acc : 0.2548926770687103 train_loss : 0.2605621156827459\n",
            "Acc : 0.2599431872367859 train_loss : 0.25746348182912226\n",
            "Acc : 0.26499369740486145 train_loss : 0.25414388816465033\n",
            "Acc : 0.2698863744735718 train_loss : 0.25198143388011623\n",
            "Acc : 0.27446338534355164 train_loss : 0.2521379384816739\n",
            "Acc : 0.27935606241226196 train_loss : 0.2507458708152689\n",
            "Acc : 0.2844065725803375 train_loss : 0.24854599709733058\n",
            "Acc : 0.2891414165496826 train_loss : 0.24944545465211074\n",
            "Acc : 0.2941919267177582 train_loss : 0.2471942122353882\n",
            "Acc : 0.2990846037864685 train_loss : 0.24850574928906657\n",
            "Acc : 0.30397728085517883 train_loss : 0.24687299085041833\n",
            "Acc : 0.3085542917251587 train_loss : 0.2467679635155946\n",
            "Acc : 0.313446968793869 train_loss : 0.2498314456297801\n",
            "Acc : 0.3184974789619446 train_loss : 0.24715442357189726\n",
            "Acc : 0.32354798913002014 train_loss : 0.24472862148462837\n",
            "Acc : 0.32844066619873047 train_loss : 0.2466566095895627\n",
            "Acc : 0.3333333432674408 train_loss : 0.24440732814263608\n",
            "Acc : 0.3380681872367859 train_loss : 0.2439318299293518\n",
            "Acc : 0.3429608643054962 train_loss : 0.24580097954038163\n",
            "Acc : 0.34785354137420654 train_loss : 0.24550063328610527\n",
            "Acc : 0.3529040515422821 train_loss : 0.2437613367626112\n",
            "Acc : 0.3579545319080353 train_loss : 0.24304183572530746\n",
            "Acc : 0.36300504207611084 train_loss : 0.24103373855352403\n",
            "Acc : 0.36773988604545593 train_loss : 0.24129824663855529\n",
            "Acc : 0.37263256311416626 train_loss : 0.24079177102872304\n",
            "Acc : 0.3775252401828766 train_loss : 0.24128062879809967\n",
            "Acc : 0.3824179172515869 train_loss : 0.23975863071936596\n",
            "Acc : 0.3874684274196625 train_loss : 0.2372033204883337\n",
            "Acc : 0.3923611044883728 train_loss : 0.23542100660226964\n",
            "Acc : 0.3970959484577179 train_loss : 0.2396801443906819\n",
            "Acc : 0.40214645862579346 train_loss : 0.23709756252636394\n",
            "Acc : 0.40688130259513855 train_loss : 0.23863428236827963\n",
            "Acc : 0.4119318127632141 train_loss : 0.23654643121887656\n",
            "Acc : 0.41682448983192444 train_loss : 0.23722389652285464\n",
            "Acc : 0.42171716690063477 train_loss : 0.23537805273957635\n",
            "Acc : 0.4262941777706146 train_loss : 0.23847736257382415\n",
            "Acc : 0.4313446879386902 train_loss : 0.23694357836849234\n",
            "Acc : 0.43639519810676575 train_loss : 0.23472234590186014\n",
            "Acc : 0.4412878751754761 train_loss : 0.23337677473222818\n",
            "Acc : 0.4461805522441864 train_loss : 0.2324067040791978\n",
            "Acc : 0.45107322931289673 train_loss : 0.23429744290087812\n",
            "Acc : 0.4561237394809723 train_loss : 0.23235990867969838\n",
            "Acc : 0.4610164165496826 train_loss : 0.23151543422749168\n",
            "Acc : 0.4657512605190277 train_loss : 0.2329158124824365\n",
            "Acc : 0.4704861044883728 train_loss : 0.2327983391653631\n",
            "Acc : 0.47553661465644836 train_loss : 0.2308505511253464\n",
            "Acc : 0.4804292917251587 train_loss : 0.23013846026827592\n",
            "Acc : 0.48547980189323425 train_loss : 0.2280836093239486\n",
            "Acc : 0.4903724789619446 train_loss : 0.22711977714875548\n",
            "Acc : 0.4952651560306549 train_loss : 0.22755125659864908\n",
            "Acc : 0.5003156661987305 train_loss : 0.22647647912279495\n",
            "Acc : 0.5052083134651184 train_loss : 0.22865824736296558\n",
            "Acc : 0.5099431872367859 train_loss : 0.2303810842512619\n",
            "Acc : 0.5145202279090881 train_loss : 0.23243741120018488\n",
            "Acc : 0.5190972089767456 train_loss : 0.2347590504649365\n",
            "Acc : 0.5239899158477783 train_loss : 0.23392606462800392\n",
            "Acc : 0.5288825631141663 train_loss : 0.23317124665535371\n",
            "Acc : 0.5339331030845642 train_loss : 0.2318885160271417\n",
            "Acc : 0.5388257503509521 train_loss : 0.2305440628011753\n",
            "Acc : 0.5434027910232544 train_loss : 0.2318065743061847\n",
            "Acc : 0.5482954382896423 train_loss : 0.231829193626753\n",
            "Acc : 0.5533459782600403 train_loss : 0.23034952507403336\n",
            "Acc : 0.558080792427063 train_loss : 0.23171280912407066\n",
            "Acc : 0.5631313323974609 train_loss : 0.23015094848735065\n",
            "Acc : 0.5681818127632141 train_loss : 0.22849832011912113\n",
            "Acc : 0.5730745196342468 train_loss : 0.22806488225331245\n",
            "Acc : 0.578125 train_loss : 0.2270121380467625\n",
            "Acc : 0.5830176472663879 train_loss : 0.22743754351201156\n",
            "Acc : 0.5880681872367859 train_loss : 0.22589464533365955\n",
            "Acc : 0.5926452279090881 train_loss : 0.229586561302059\n",
            "Acc : 0.5975378751754761 train_loss : 0.2299793458172703\n",
            "Acc : 0.6024305820465088 train_loss : 0.22887398385172408\n",
            "Acc : 0.607481062412262 train_loss : 0.2274989296346903\n",
            "Acc : 0.6123737096786499 train_loss : 0.22685943332515537\n",
            "Acc : 0.6172664165496826 train_loss : 0.22653533226451067\n",
            "Acc : 0.6220012903213501 train_loss : 0.226541505442583\n",
            "Acc : 0.626893937587738 train_loss : 0.22588208769700785\n",
            "Acc : 0.6316288113594055 train_loss : 0.22596088259552533\n",
            "Acc : 0.6365214586257935 train_loss : 0.22637047955609915\n",
            "Acc : 0.6410984992980957 train_loss : 0.22725768208108615\n",
            "Acc : 0.6461489796638489 train_loss : 0.22647493037122085\n",
            "Acc : 0.6507260203361511 train_loss : 0.22918557399300052\n",
            "Acc : 0.6557765007019043 train_loss : 0.2279786137105138\n",
            "Acc : 0.660669207572937 train_loss : 0.2280489189468105\n",
            "Acc : 0.665561854839325 train_loss : 0.22792781461166203\n",
            "Acc : 0.6706123948097229 train_loss : 0.22683964104162177\n",
            "Acc : 0.6755050420761108 train_loss : 0.22588856868017063\n",
            "Acc : 0.6803977489471436 train_loss : 0.2256709413469902\n",
            "Acc : 0.6848168969154358 train_loss : 0.22861956827448193\n",
            "Acc : 0.6895517706871033 train_loss : 0.2300681311925742\n",
            "Acc : 0.6944444179534912 train_loss : 0.22943344562464243\n",
            "Acc : 0.6991792917251587 train_loss : 0.22946098825842556\n",
            "Acc : 0.7040719985961914 train_loss : 0.22904764574406475\n",
            "Acc : 0.7089646458625793 train_loss : 0.22878748174654703\n",
            "Acc : 0.7140151262283325 train_loss : 0.22749826718806004\n",
            "Acc : 0.7189078330993652 train_loss : 0.22773262839512648\n",
            "Acc : 0.7238004803657532 train_loss : 0.22756619642694925\n",
            "Acc : 0.7286931872367859 train_loss : 0.22784371417015792\n",
            "Acc : 0.7335858345031738 train_loss : 0.22798408184274538\n",
            "Acc : 0.7386363744735718 train_loss : 0.22676287783849003\n",
            "Acc : 0.7435290217399597 train_loss : 0.22643096195959966\n",
            "Acc : 0.7484217286109924 train_loss : 0.22601915940690737\n",
            "Acc : 0.7533143758773804 train_loss : 0.22636962212141484\n",
            "Acc : 0.7580492496490479 train_loss : 0.2265308327996769\n",
            "Acc : 0.7629418969154358 train_loss : 0.2263381308075159\n",
            "Acc : 0.7678346037864685 train_loss : 0.22605308603872604\n",
            "Acc : 0.7727272510528564 train_loss : 0.2261937764479117\n",
            "Acc : 0.7774621248245239 train_loss : 0.2260783682228066\n",
            "Acc : 0.7821969985961914 train_loss : 0.22631928589634642\n",
            "Acc : 0.7870896458625793 train_loss : 0.2257007922126371\n",
            "Acc : 0.7919822931289673 train_loss : 0.22536251082605013\n",
            "Acc : 0.7967171669006348 train_loss : 0.225722078381606\n",
            "Acc : 0.8017676472663879 train_loss : 0.22486767923515855\n",
            "Acc : 0.8066603541374207 train_loss : 0.2241546535101461\n",
            "Acc : 0.8115530014038086 train_loss : 0.22362461624789737\n",
            "Acc : 0.8164457082748413 train_loss : 0.22334591291534403\n",
            "Acc : 0.8213383555412292 train_loss : 0.2232688493563755\n",
            "Acc : 0.826231062412262 train_loss : 0.22248695379890063\n",
            "Acc : 0.8309659361839294 train_loss : 0.22290748750997913\n",
            "Acc : 0.8353850841522217 train_loss : 0.22501435296523364\n",
            "Acc : 0.8404356241226196 train_loss : 0.22415245682905519\n",
            "Acc : 0.8451704382896423 train_loss : 0.2242751308320754\n",
            "Acc : 0.850063145160675 train_loss : 0.22344511671790054\n",
            "Acc : 0.8551136255264282 train_loss : 0.2225839604136788\n",
            "Acc : 0.8600063323974609 train_loss : 0.22215327082759578\n",
            "Acc : 0.8647411465644836 train_loss : 0.22257509309631052\n",
            "Acc : 0.8696338534355164 train_loss : 0.22256316362188183\n",
            "Acc : 0.8746843338012695 train_loss : 0.22152593176191052\n",
            "Acc : 0.8797348737716675 train_loss : 0.22057647219967116\n",
            "Acc : 0.884311854839325 train_loss : 0.22139552545703048\n",
            "Acc : 0.8892045617103577 train_loss : 0.22067824078347187\n",
            "Acc : 0.8940972089767456 train_loss : 0.22109427067208226\n",
            "Acc : 0.8989899158477783 train_loss : 0.22127968982063434\n",
            "Acc : 0.9035668969154358 train_loss : 0.2215544474761813\n",
            "Acc : 0.9084596037864685 train_loss : 0.22123211645147697\n",
            "Acc : 0.9135100841522217 train_loss : 0.22036158185491853\n",
            "Acc : 0.9179292917251587 train_loss : 0.22183394009276988\n",
            "Acc : 0.9228219985961914 train_loss : 0.2213190762914325\n",
            "Acc : 0.9278724789619446 train_loss : 0.22026895969638025\n",
            "Acc : 0.9329229593276978 train_loss : 0.21950663614552468\n",
            "Acc : 0.9379734992980957 train_loss : 0.21865666904752118\n",
            "Acc : 0.9430239796638489 train_loss : 0.21794481226003048\n",
            "Acc : 0.9477588534355164 train_loss : 0.2179740688357598\n",
            "Acc : 0.9526515007019043 train_loss : 0.21777863925969115\n",
            "Acc : 0.9572285413742065 train_loss : 0.21795317171340062\n",
            "Acc : 0.9622790217399597 train_loss : 0.21780534641760768\n",
            "Validating....\n",
            "Acc 0.017045455053448677 train_loss 0.8927372097969055\n",
            "Acc 0.035353533923625946 train_loss 0.7733227908611298\n",
            "Acc 0.054924242198467255 train_loss 0.595034177104632\n",
            "Acc 0.07133838534355164 train_loss 0.6145929507911205\n",
            "Acc 0.0902777761220932 train_loss 0.6240223973989487\n",
            "Acc 0.10795454680919647 train_loss 0.6490583494305611\n",
            "Acc 0.12373737245798111 train_loss 0.7726891445262092\n",
            "Acc 0.14204545319080353 train_loss 0.7819900196045637\n",
            "Acc 0.1597222238779068 train_loss 0.8273590389225218\n",
            "Acc 0.17866161465644836 train_loss 0.8025618299841881\n",
            "Acc 0.19696970283985138 train_loss 0.8407604247331619\n",
            "Acc 0.21338383853435516 train_loss 0.8955018408596516\n",
            "Acc 0.23106060922145844 train_loss 0.888360164486445\n",
            "Acc 0.25 train_loss 0.857411866741521\n",
            "Acc 0.2683080732822418 train_loss 0.8204543819030126\n",
            "Acc 0.2859848439693451 train_loss 0.8150767153128982\n",
            "Acc 0.30492424964904785 train_loss 0.7974882082027548\n",
            "Acc 0.3232323229312897 train_loss 0.7846592093507448\n",
            "Acc 0.342803031206131 train_loss 0.7493939568337641\n",
            "Acc 0.3623737394809723 train_loss 0.7181757893413305\n",
            "Acc 0.38257575035095215 train_loss 0.6852036254214389\n",
            "Acc 0.40214645862579346 train_loss 0.6721716544336893\n",
            "Acc 0.4210858643054962 train_loss 0.6632988460199989\n",
            "Acc 0.43939393758773804 train_loss 0.6682577040822556\n",
            "Acc 0.4558080732822418 train_loss 0.6901684577018022\n",
            "Acc 0.4734848439693451 train_loss 0.7016070445712942\n",
            "Acc 0.4930555522441864 train_loss 0.6869283115008363\n",
            "Acc 0.5107322931289673 train_loss 0.6898716981522739\n",
            "Acc 0.5277777910232544 train_loss 0.7135865642721283\n",
            "Acc 0.5460858345031738 train_loss 0.7006961172446609\n",
            "Acc 0.5637626051902771 train_loss 0.6957746115062506\n",
            "Acc 0.5833333134651184 train_loss 0.6834319918998517\n",
            "Acc 0.6016414165496826 train_loss 0.6754900176981182\n",
            "Acc 0.6180555820465088 train_loss 0.6822232915745938\n",
            "Acc 0.6357322931289673 train_loss 0.6825516203152282\n",
            "Acc 0.6546717286109924 train_loss 0.6766537283547223\n",
            "Acc 0.6710858345031738 train_loss 0.6933131303034119\n",
            "Acc 0.6875 train_loss 0.7091215680794496\n",
            "Acc 0.7039141654968262 train_loss 0.7116517627563996\n",
            "Acc 0.7209596037864685 train_loss 0.7222797308582812\n",
            "Acc 0.7380050420761108 train_loss 0.7221706571829755\n",
            "Acc 0.756313145160675 train_loss 0.726872245869821\n",
            "Acc 0.7733585834503174 train_loss 0.7220629097157439\n",
            "Acc 0.7916666865348816 train_loss 0.7251463847827505\n",
            "Acc 0.809974730014801 train_loss 0.7165029370536407\n",
            "Acc 0.8263888955116272 train_loss 0.7233862725046017\n",
            "Acc 0.8453282713890076 train_loss 0.7190048799632077\n",
            "Acc 0.8623737096786499 train_loss 0.7207714471733198\n",
            "Acc 0.8800504803657532 train_loss 0.7213002860318033\n",
            "Acc 0.8876262903213501 train_loss 0.7300800970569253\n",
            "F1_score : 0.7682291666666666\n",
            "Epoch completed in 1.4314160148302715 minutes\n",
            "***************\n",
            "EPOCH: 7\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004734848625957966 train_loss : 0.48229068517684937\n",
            "Acc : 0.009469697251915932 train_loss : 0.31772444397211075\n",
            "Acc : 0.014520201832056046 train_loss : 0.25624363621075946\n",
            "Acc : 0.019412878900766373 train_loss : 0.21775323525071144\n",
            "Acc : 0.024463383480906487 train_loss : 0.20101033747196198\n",
            "Acc : 0.029198233038187027 train_loss : 0.22070563584566116\n",
            "Acc : 0.034090910106897354 train_loss : 0.21177160952772414\n",
            "Acc : 0.03898358717560768 train_loss : 0.1952049331739545\n",
            "Acc : 0.044034089893102646 train_loss : 0.1809304795331425\n",
            "Acc : 0.04908459633588791 train_loss : 0.17380521073937416\n",
            "Acc : 0.05413510277867317 train_loss : 0.16360510479320178\n",
            "Acc : 0.0590277761220932 train_loss : 0.16549296552936235\n",
            "Acc : 0.06407828629016876 train_loss : 0.15459643863141537\n",
            "Acc : 0.06912878900766373 train_loss : 0.1462784924411348\n",
            "Acc : 0.07386363297700882 train_loss : 0.16295380033552648\n",
            "Acc : 0.07859848439693451 train_loss : 0.16623179928865284\n",
            "Acc : 0.08317550271749496 train_loss : 0.18472606414819465\n",
            "Acc : 0.08822601288557053 train_loss : 0.17690004356619385\n",
            "Acc : 0.09296085685491562 train_loss : 0.19141954134561515\n",
            "Acc : 0.09769570827484131 train_loss : 0.2032129387371242\n",
            "Acc : 0.10274621099233627 train_loss : 0.19513241778172197\n",
            "Acc : 0.1076388880610466 train_loss : 0.2022622353820638\n",
            "Acc : 0.11190025508403778 train_loss : 0.22059498832601568\n",
            "Acc : 0.11663509905338287 train_loss : 0.223483415087685\n",
            "Acc : 0.1215277761220932 train_loss : 0.22242935575544834\n",
            "Acc : 0.12594696879386902 train_loss : 0.2459545677814346\n",
            "Acc : 0.1302083283662796 train_loss : 0.2693049480655679\n",
            "Acc : 0.13525883853435516 train_loss : 0.26267961552366614\n",
            "Acc : 0.1401515156030655 train_loss : 0.2600153199686059\n",
            "Acc : 0.14488635957241058 train_loss : 0.26057141901304326\n",
            "Acc : 0.14962121844291687 train_loss : 0.26109356343025164\n",
            "Acc : 0.15419822931289673 train_loss : 0.27140796632738784\n",
            "Acc : 0.1592487394809723 train_loss : 0.26526527902619407\n",
            "Acc : 0.1636679321527481 train_loss : 0.27644622528596835\n",
            "Acc : 0.16871842741966248 train_loss : 0.27080515713563985\n",
            "Acc : 0.1736111044883728 train_loss : 0.26907773941962254\n",
            "Acc : 0.17850378155708313 train_loss : 0.26699293268894825\n",
            "Acc : 0.1835542917251587 train_loss : 0.26405310988622277\n",
            "Acc : 0.18844696879386902 train_loss : 0.26591554479912305\n",
            "Acc : 0.1931818127632141 train_loss : 0.2725440096575767\n",
            "Acc : 0.19775883853435516 train_loss : 0.2751328200556156\n",
            "Acc : 0.2026515156030655 train_loss : 0.27192762143732535\n",
            "Acc : 0.20722854137420654 train_loss : 0.27324058986160643\n",
            "Acc : 0.21196338534355164 train_loss : 0.27286154848777433\n",
            "Acc : 0.2170138955116272 train_loss : 0.2707171847836839\n",
            "Acc : 0.22190657258033752 train_loss : 0.2688169729207521\n",
            "Acc : 0.22679924964904785 train_loss : 0.2660895085477449\n",
            "Acc : 0.23169191181659698 train_loss : 0.2636610307187463\n",
            "Acc : 0.2365845888853073 train_loss : 0.26000331680537486\n",
            "Acc : 0.24147726595401764 train_loss : 0.2575476161018014\n",
            "Acc : 0.24636994302272797 train_loss : 0.2578460308631845\n",
            "Acc : 0.25110480189323425 train_loss : 0.2596905079956811\n",
            "Acc : 0.2556818127632141 train_loss : 0.2603588664095919\n",
            "Acc : 0.2607323229312897 train_loss : 0.2568971210124868\n",
            "Acc : 0.26578283309936523 train_loss : 0.25320785062556916\n",
            "Acc : 0.2708333432674408 train_loss : 0.25023627404256593\n",
            "Acc : 0.2755681872367859 train_loss : 0.2491388923831676\n",
            "Acc : 0.2804608643054962 train_loss : 0.24794577206645546\n",
            "Acc : 0.28535354137420654 train_loss : 0.24605671597360554\n",
            "Acc : 0.28977271914482117 train_loss : 0.25127384131774305\n",
            "Acc : 0.2946653962135315 train_loss : 0.2497768955702176\n",
            "Acc : 0.29924243688583374 train_loss : 0.25310508345043464\n",
            "Acc : 0.30397728085517883 train_loss : 0.25319912287569235\n",
            "Acc : 0.3082386255264282 train_loss : 0.25631993883871473\n",
            "Acc : 0.3129734992980957 train_loss : 0.25936824602003283\n",
            "Acc : 0.3180239796638489 train_loss : 0.25616076151191286\n",
            "Acc : 0.32307448983192444 train_loss : 0.2533529473385259\n",
            "Acc : 0.32780933380126953 train_loss : 0.256230967146728\n",
            "Acc : 0.3328598439693451 train_loss : 0.25366204493827577\n",
            "Acc : 0.3377525210380554 train_loss : 0.25277421679347756\n",
            "Acc : 0.34264519810676575 train_loss : 0.25528402681606754\n",
            "Acc : 0.3475378751754761 train_loss : 0.25443280873716706\n",
            "Acc : 0.35258838534355164 train_loss : 0.25330373800474487\n",
            "Acc : 0.35748106241226196 train_loss : 0.2527831187975165\n",
            "Acc : 0.36221590638160706 train_loss : 0.25229177984098594\n",
            "Acc : 0.36695075035095215 train_loss : 0.2521178202066374\n",
            "Acc : 0.3718434274196625 train_loss : 0.25139782599524246\n",
            "Acc : 0.37657827138900757 train_loss : 0.2520214742622696\n",
            "Acc : 0.3814709484577179 train_loss : 0.24977841098568862\n",
            "Acc : 0.38652145862579346 train_loss : 0.2471585661871359\n",
            "Acc : 0.391571968793869 train_loss : 0.24518624111366125\n",
            "Acc : 0.3963068127632141 train_loss : 0.2500057356522941\n",
            "Acc : 0.4013573229312897 train_loss : 0.24767971189176463\n",
            "Acc : 0.40625 train_loss : 0.24777592796211442\n",
            "Acc : 0.41130051016807556 train_loss : 0.24562358205371043\n",
            "Acc : 0.4161931872367859 train_loss : 0.24511078742960857\n",
            "Acc : 0.42124369740486145 train_loss : 0.24296246004430042\n",
            "Acc : 0.4258207082748413 train_loss : 0.24448473410765556\n",
            "Acc : 0.43071338534355164 train_loss : 0.24269045066883724\n",
            "Acc : 0.4357638955116272 train_loss : 0.24069411346895828\n",
            "Acc : 0.44081440567970276 train_loss : 0.2394151007040189\n",
            "Acc : 0.4457070827484131 train_loss : 0.23801156293357845\n",
            "Acc : 0.4505997598171234 train_loss : 0.2398571733946121\n",
            "Acc : 0.4556502401828766 train_loss : 0.2379293694022171\n",
            "Acc : 0.46070075035095215 train_loss : 0.2361950787470529\n",
            "Acc : 0.46543559432029724 train_loss : 0.23734843430186933\n",
            "Acc : 0.47032827138900757 train_loss : 0.23661300902898164\n",
            "Acc : 0.47537878155708313 train_loss : 0.23461259447266253\n",
            "Acc : 0.48027145862579346 train_loss : 0.2333245985021796\n",
            "Acc : 0.485321968793869 train_loss : 0.23118101567029953\n",
            "Acc : 0.49021464586257935 train_loss : 0.23002514398038978\n",
            "Acc : 0.4951073229312897 train_loss : 0.23092909047708793\n",
            "Acc : 0.5 train_loss : 0.2297524921957729\n",
            "Acc : 0.5048926472663879 train_loss : 0.2316281215932507\n",
            "Acc : 0.5097853541374207 train_loss : 0.23075180387213118\n",
            "Acc : 0.5145202279090881 train_loss : 0.2322668398748029\n",
            "Acc : 0.5189393758773804 train_loss : 0.23490828602949035\n",
            "Acc : 0.5239899158477783 train_loss : 0.23300714559715102\n",
            "Acc : 0.5290403962135315 train_loss : 0.23197977713078533\n",
            "Acc : 0.5340909361839294 train_loss : 0.23021938807585024\n",
            "Acc : 0.5391414165496826 train_loss : 0.22867094446812664\n",
            "Acc : 0.5438762903213501 train_loss : 0.22980619431473315\n",
            "Acc : 0.548768937587738 train_loss : 0.2298447580398184\n",
            "Acc : 0.5538194179534912 train_loss : 0.22815224364922757\n",
            "Acc : 0.5585542917251587 train_loss : 0.22876550937476364\n",
            "Acc : 0.5636047720909119 train_loss : 0.22709819356557623\n",
            "Acc : 0.5686553120613098 train_loss : 0.22532464165845487\n",
            "Acc : 0.5735479593276978 train_loss : 0.224743600334909\n",
            "Acc : 0.5785984992980957 train_loss : 0.22340773681507392\n",
            "Acc : 0.5834911465644836 train_loss : 0.22391449532782037\n",
            "Acc : 0.5883838534355164 train_loss : 0.22353974658965078\n",
            "Acc : 0.5928030014038086 train_loss : 0.22847082780399283\n",
            "Acc : 0.5975378751754761 train_loss : 0.22942136467351176\n",
            "Acc : 0.6025883555412292 train_loss : 0.22811262516845618\n",
            "Acc : 0.6076388955116272 train_loss : 0.2265688620209694\n",
            "Acc : 0.6125315427780151 train_loss : 0.22555341476958896\n",
            "Acc : 0.6174242496490479 train_loss : 0.225039424858694\n",
            "Acc : 0.6223168969154358 train_loss : 0.22469227644614875\n",
            "Acc : 0.6273674368858337 train_loss : 0.22398469753043596\n",
            "Acc : 0.6322600841522217 train_loss : 0.22404502389522699\n",
            "Acc : 0.6369949579238892 train_loss : 0.22381427349480054\n",
            "Acc : 0.6415719985961914 train_loss : 0.22498181383266594\n",
            "Acc : 0.6466224789619446 train_loss : 0.2240256123748937\n",
            "Acc : 0.6510416865348816 train_loss : 0.2284558899144628\n",
            "Acc : 0.6560921669006348 train_loss : 0.22708781502313083\n",
            "Acc : 0.6609848737716675 train_loss : 0.22692611602628054\n",
            "Acc : 0.6658775210380554 train_loss : 0.22684699294232105\n",
            "Acc : 0.6709280014038086 train_loss : 0.2259655702535225\n",
            "Acc : 0.6759785413742065 train_loss : 0.224725832693654\n",
            "Acc : 0.6808711886405945 train_loss : 0.22385427645806755\n",
            "Acc : 0.6852903962135315 train_loss : 0.22650258457089992\n",
            "Acc : 0.690025269985199 train_loss : 0.22785895677204704\n",
            "Acc : 0.6949179172515869 train_loss : 0.2271587167430174\n",
            "Acc : 0.6996527910232544 train_loss : 0.22708405981150767\n",
            "Acc : 0.7045454382896423 train_loss : 0.2265210013194331\n",
            "Acc : 0.709438145160675 train_loss : 0.22609762309042558\n",
            "Acc : 0.7144886255264282 train_loss : 0.22491631259014006\n",
            "Acc : 0.7192234992980957 train_loss : 0.22490713707599286\n",
            "Acc : 0.7241161465644836 train_loss : 0.22482084010371425\n",
            "Acc : 0.7290088534355164 train_loss : 0.2249587783465783\n",
            "Acc : 0.7339015007019043 train_loss : 0.22469525387054248\n",
            "Acc : 0.7389520406723022 train_loss : 0.22357463003381303\n",
            "Acc : 0.7440025210380554 train_loss : 0.22294285903179567\n",
            "Acc : 0.7490530014038086 train_loss : 0.2219284839154064\n",
            "Acc : 0.7539457082748413 train_loss : 0.22183941434467994\n",
            "Acc : 0.7586805820465088 train_loss : 0.2218744588108399\n",
            "Acc : 0.7635732293128967 train_loss : 0.22199809451581567\n",
            "Acc : 0.7684659361839294 train_loss : 0.2222908344355565\n",
            "Acc : 0.7733585834503174 train_loss : 0.22231659054193856\n",
            "Acc : 0.7780934572219849 train_loss : 0.22205227785743772\n",
            "Acc : 0.7828282713890076 train_loss : 0.22208300524431726\n",
            "Acc : 0.7878788113594055 train_loss : 0.22105833133797587\n",
            "Acc : 0.792455792427063 train_loss : 0.22238169711060318\n",
            "Acc : 0.7973484992980957 train_loss : 0.22224651168032392\n",
            "Acc : 0.8023989796638489 train_loss : 0.22131729726538513\n",
            "Acc : 0.8074495196342468 train_loss : 0.22033266290034875\n",
            "Acc : 0.8125 train_loss : 0.21978631069114107\n",
            "Acc : 0.8173926472663879 train_loss : 0.2193643815061521\n",
            "Acc : 0.8222853541374207 train_loss : 0.21881450613486697\n",
            "Acc : 0.8273358345031738 train_loss : 0.2180711820064222\n",
            "Acc : 0.8323863744735718 train_loss : 0.21743351624113078\n",
            "Acc : 0.8369633555412292 train_loss : 0.21884963473001884\n",
            "Acc : 0.8420138955116272 train_loss : 0.21819394079677631\n",
            "Acc : 0.8469065427780151 train_loss : 0.21856279818919197\n",
            "Acc : 0.8519570827484131 train_loss : 0.2179479886165687\n",
            "Acc : 0.8570075631141663 train_loss : 0.2173560949423435\n",
            "Acc : 0.861900269985199 train_loss : 0.21676165477956755\n",
            "Acc : 0.8664772510528564 train_loss : 0.21691282342575238\n",
            "Acc : 0.8713699579238892 train_loss : 0.21671589532627739\n",
            "Acc : 0.8764204382896423 train_loss : 0.2158141067665484\n",
            "Acc : 0.8814709782600403 train_loss : 0.21505549086967884\n",
            "Acc : 0.8863636255264282 train_loss : 0.21543772500213032\n",
            "Acc : 0.8914141654968262 train_loss : 0.2145194800906494\n",
            "Acc : 0.8963068127632141 train_loss : 0.2144084166654426\n",
            "Acc : 0.9011995196342468 train_loss : 0.21433565983095684\n",
            "Acc : 0.9056186676025391 train_loss : 0.21482832709787994\n",
            "Acc : 0.910669207572937 train_loss : 0.21401115718053623\n",
            "Acc : 0.9157196879386902 train_loss : 0.21315351788430137\n",
            "Acc : 0.9202967286109924 train_loss : 0.21444157926888063\n",
            "Acc : 0.9253472089767456 train_loss : 0.2134355035169344\n",
            "Acc : 0.9303977489471436 train_loss : 0.21241320877370098\n",
            "Acc : 0.9352903962135315 train_loss : 0.2122072218626272\n",
            "Acc : 0.9403409361839294 train_loss : 0.2114676925898988\n",
            "Acc : 0.9453914165496826 train_loss : 0.21069145430186667\n",
            "Acc : 0.9504418969154358 train_loss : 0.20981849087163423\n",
            "Acc : 0.9554924368858337 train_loss : 0.20914719910455906\n",
            "Acc : 0.9600694179534912 train_loss : 0.21057792320885332\n",
            "Acc : 0.9648042917251587 train_loss : 0.2110237434337085\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.6975163221359253\n",
            "Acc 0.03724747523665428 train_loss 0.6687701940536499\n",
            "Acc 0.05744949355721474 train_loss 0.45168012753129005\n",
            "Acc 0.07512626051902771 train_loss 0.42895649652928114\n",
            "Acc 0.09469696879386902 train_loss 0.4255417428910732\n",
            "Acc 0.11174242198467255 train_loss 0.5046557057648897\n",
            "Acc 0.12815657258033752 train_loss 0.6023756664778505\n",
            "Acc 0.1470959633588791 train_loss 0.5670714764855802\n",
            "Acc 0.1654040366411209 train_loss 0.5727602123386331\n",
            "Acc 0.18434342741966248 train_loss 0.5802553486078977\n",
            "Acc 0.20202019810676575 train_loss 0.615764916959134\n",
            "Acc 0.21906565129756927 train_loss 0.6491576889529824\n",
            "Acc 0.2373737394809723 train_loss 0.6281409524381161\n",
            "Acc 0.25631314516067505 train_loss 0.6044207989637341\n",
            "Acc 0.2765151560306549 train_loss 0.5693066425621509\n",
            "Acc 0.2941919267177582 train_loss 0.590861278353259\n",
            "Acc 0.31313130259513855 train_loss 0.5703907580498386\n",
            "Acc 0.33143940567970276 train_loss 0.563241813538803\n",
            "Acc 0.3516414165496826 train_loss 0.5354539434376516\n",
            "Acc 0.3718434274196625 train_loss 0.5100307927466929\n",
            "Acc 0.3920454680919647 train_loss 0.4863938707414837\n",
            "Acc 0.41161614656448364 train_loss 0.4802494784393771\n",
            "Acc 0.4305555522441864 train_loss 0.4781273036952252\n",
            "Acc 0.4488636255264282 train_loss 0.48291100348190713\n",
            "Acc 0.4665403962135315 train_loss 0.4948659518733621\n",
            "Acc 0.4848484992980957 train_loss 0.5055904047778592\n",
            "Acc 0.504419207572937 train_loss 0.49780368173701894\n",
            "Acc 0.5227272510528564 train_loss 0.5026624035422823\n",
            "Acc 0.5404040217399597 train_loss 0.5219367087722339\n",
            "Acc 0.558080792427063 train_loss 0.5255522114845613\n",
            "Acc 0.5763888955116272 train_loss 0.5292818510544396\n",
            "Acc 0.5959596037864685 train_loss 0.5184595536265988\n",
            "Acc 0.6148989796638489 train_loss 0.5120350797801758\n",
            "Acc 0.6319444179534912 train_loss 0.5290176437017234\n",
            "Acc 0.6502525210380554 train_loss 0.5327320905934487\n",
            "Acc 0.6698232293128967 train_loss 0.5261224109110318\n",
            "Acc 0.6868686676025391 train_loss 0.538045431940338\n",
            "Acc 0.7045454382896423 train_loss 0.5426944241564917\n",
            "Acc 0.7215909361839294 train_loss 0.5489384484453461\n",
            "Acc 0.7386363744735718 train_loss 0.5575178100029007\n",
            "Acc 0.756313145160675 train_loss 0.5549645633096012\n",
            "Acc 0.7733585834503174 train_loss 0.5649000383425682\n",
            "Acc 0.7916666865348816 train_loss 0.5647531864125022\n",
            "Acc 0.8093434572219849 train_loss 0.5709442420312288\n",
            "Acc 0.8282828330993652 train_loss 0.5663242967799306\n",
            "Acc 0.8472222089767456 train_loss 0.5626927100972313\n",
            "Acc 0.8655303120613098 train_loss 0.564500320960391\n",
            "Acc 0.8844696879386902 train_loss 0.5621815005821796\n",
            "Acc 0.9027777910232544 train_loss 0.5632617685145566\n",
            "Acc 0.9116161465644836 train_loss 0.5731677453406155\n",
            "F1_score : 0.8345153664302601\n",
            "Epoch completed in 1.4284565409024557 minutes\n",
            "***************\n",
            "EPOCH: 8\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004734848625957966 train_loss : 0.3334599435329437\n",
            "Acc : 0.009627525694668293 train_loss : 0.22957733273506165\n",
            "Acc : 0.014678030274808407 train_loss : 0.17353728289405504\n",
            "Acc : 0.019728535786271095 train_loss : 0.14135768543928862\n",
            "Acc : 0.02477904036641121 train_loss : 0.13184139356017113\n",
            "Acc : 0.0295138880610466 train_loss : 0.13363582827150822\n",
            "Acc : 0.03456439450383186 train_loss : 0.12411664053797722\n",
            "Acc : 0.03961489722132683 train_loss : 0.11723136389628053\n",
            "Acc : 0.04466540366411209 train_loss : 0.11153600820236736\n",
            "Acc : 0.04955808073282242 train_loss : 0.11524926461279392\n",
            "Acc : 0.05460858717560768 train_loss : 0.10910338739102537\n",
            "Acc : 0.059659089893102646 train_loss : 0.10897127569963534\n",
            "Acc : 0.06470959633588791 train_loss : 0.102926383797939\n",
            "Acc : 0.06960227340459824 train_loss : 0.10429821961692401\n",
            "Acc : 0.07449495047330856 train_loss : 0.11551567961772283\n",
            "Acc : 0.07938762754201889 train_loss : 0.11891811201348901\n",
            "Acc : 0.08428030461072922 train_loss : 0.12296117184793248\n",
            "Acc : 0.08933080732822418 train_loss : 0.11831209291186598\n",
            "Acc : 0.09390782564878464 train_loss : 0.13207535426083364\n",
            "Acc : 0.09864267706871033 train_loss : 0.14098601546138526\n",
            "Acc : 0.10369317978620529 train_loss : 0.13723685397278695\n",
            "Acc : 0.10858585685491562 train_loss : 0.14840939556333152\n",
            "Acc : 0.11347853392362595 train_loss : 0.14621799455388731\n",
            "Acc : 0.11821338534355164 train_loss : 0.1586222181407114\n",
            "Acc : 0.12310606241226196 train_loss : 0.15684016928076744\n",
            "Acc : 0.12752525508403778 train_loss : 0.17665843307398832\n",
            "Acc : 0.1319444477558136 train_loss : 0.19111171461365842\n",
            "Acc : 0.13699494302272797 train_loss : 0.18650250509381294\n",
            "Acc : 0.1418876200914383 train_loss : 0.1858709315801489\n",
            "Acc : 0.14678029716014862 train_loss : 0.18521259874105453\n",
            "Acc : 0.15183080732822418 train_loss : 0.18477818706343252\n",
            "Acc : 0.15656565129756927 train_loss : 0.1925456956960261\n",
            "Acc : 0.16161616146564484 train_loss : 0.18903101980686188\n",
            "Acc : 0.16635100543498993 train_loss : 0.19996685534715652\n",
            "Acc : 0.17124368250370026 train_loss : 0.19987996178013936\n",
            "Acc : 0.17613635957241058 train_loss : 0.19837229574720064\n",
            "Acc : 0.1810290366411209 train_loss : 0.19737136001522476\n",
            "Acc : 0.18607954680919647 train_loss : 0.19377760383251466\n",
            "Acc : 0.19081439077854156 train_loss : 0.1938236026236644\n",
            "Acc : 0.19586490094661713 train_loss : 0.1920023809187114\n",
            "Acc : 0.20044191181659698 train_loss : 0.19551961696365985\n",
            "Acc : 0.20549242198467255 train_loss : 0.19207408900062242\n",
            "Acc : 0.2105429321527481 train_loss : 0.190306703191857\n",
            "Acc : 0.21543560922145844 train_loss : 0.19028430990874767\n",
            "Acc : 0.22032828629016876 train_loss : 0.19123585638072754\n",
            "Acc : 0.2252209633588791 train_loss : 0.1904326805278011\n",
            "Acc : 0.23027145862579346 train_loss : 0.18842009946386865\n",
            "Acc : 0.23516413569450378 train_loss : 0.18669187106812993\n",
            "Acc : 0.24021464586257935 train_loss : 0.1847945176521126\n",
            "Acc : 0.2452651560306549 train_loss : 0.18284987702965735\n",
            "Acc : 0.25015783309936523 train_loss : 0.18141881697902493\n",
            "Acc : 0.2548926770687103 train_loss : 0.18286166411752885\n",
            "Acc : 0.25978535413742065 train_loss : 0.1828201988114501\n",
            "Acc : 0.2648358643054962 train_loss : 0.18084777822649037\n",
            "Acc : 0.2698863744735718 train_loss : 0.1790954436768185\n",
            "Acc : 0.27493685483932495 train_loss : 0.17658761882090143\n",
            "Acc : 0.2799873650074005 train_loss : 0.17563680317579655\n",
            "Acc : 0.2850378751754761 train_loss : 0.17457054600376506\n",
            "Acc : 0.29008838534355164 train_loss : 0.17256213326827954\n",
            "Acc : 0.29482322931289673 train_loss : 0.17353610154241322\n",
            "Acc : 0.29971590638160706 train_loss : 0.1744714983296199\n",
            "Acc : 0.3046085834503174 train_loss : 0.17455113973588712\n",
            "Acc : 0.3095012605190277 train_loss : 0.17352712053864722\n",
            "Acc : 0.31407827138900757 train_loss : 0.17497480887686834\n",
            "Acc : 0.31881314516067505 train_loss : 0.18120091918569345\n",
            "Acc : 0.3238636255264282 train_loss : 0.17890989850980765\n",
            "Acc : 0.3289141356945038 train_loss : 0.176978908284609\n",
            "Acc : 0.3336489796638489 train_loss : 0.17978753404253545\n",
            "Acc : 0.33869948983192444 train_loss : 0.1776328974443933\n",
            "Acc : 0.34359216690063477 train_loss : 0.17700887003115245\n",
            "Acc : 0.3484848439693451 train_loss : 0.17904441058635712\n",
            "Acc : 0.3533775210380554 train_loss : 0.17930752597749233\n",
            "Acc : 0.358428031206131 train_loss : 0.17823751270771027\n",
            "Acc : 0.3633207082748413 train_loss : 0.17817043895656998\n",
            "Acc : 0.36821338534355164 train_loss : 0.17747402012348176\n",
            "Acc : 0.37310606241226196 train_loss : 0.17636245498923878\n",
            "Acc : 0.3779987394809723 train_loss : 0.1757057203681438\n",
            "Acc : 0.3827335834503174 train_loss : 0.1772038251734697\n",
            "Acc : 0.38778409361839294 train_loss : 0.1754802699330487\n",
            "Acc : 0.3928346037864685 train_loss : 0.17372104041278363\n",
            "Acc : 0.39772728085517883 train_loss : 0.17246129704110416\n",
            "Acc : 0.4023042917251587 train_loss : 0.17712439051488552\n",
            "Acc : 0.40735480189323425 train_loss : 0.1752749465136643\n",
            "Acc : 0.4122474789619446 train_loss : 0.17633193722438245\n",
            "Acc : 0.41729798913002014 train_loss : 0.1749638714334544\n",
            "Acc : 0.42219066619873047 train_loss : 0.17504164671828581\n",
            "Acc : 0.4270833432674408 train_loss : 0.1744143640001615\n",
            "Acc : 0.43166035413742065 train_loss : 0.17796944107183002\n",
            "Acc : 0.4367108643054962 train_loss : 0.17680644863442088\n",
            "Acc : 0.44160354137420654 train_loss : 0.17576850412620437\n",
            "Acc : 0.4466540515422821 train_loss : 0.17469173276817407\n",
            "Acc : 0.45154672861099243 train_loss : 0.17365724131788896\n",
            "Acc : 0.45643940567970276 train_loss : 0.17507577511251612\n",
            "Acc : 0.46148988604545593 train_loss : 0.1741027647351965\n",
            "Acc : 0.46638256311416626 train_loss : 0.17375556489354685\n",
            "Acc : 0.47111743688583374 train_loss : 0.17531953852934143\n",
            "Acc : 0.47601011395454407 train_loss : 0.1747777309153498\n",
            "Acc : 0.48106059432029724 train_loss : 0.17345660696832502\n",
            "Acc : 0.4861111044883728 train_loss : 0.17189521126148075\n",
            "Acc : 0.49116161465644836 train_loss : 0.17042707910761237\n",
            "Acc : 0.4960542917251587 train_loss : 0.17005253684623997\n",
            "Acc : 0.5009469985961914 train_loss : 0.17148442381956414\n",
            "Acc : 0.5058396458625793 train_loss : 0.1707963920153171\n",
            "Acc : 0.5107322931289673 train_loss : 0.1730969519390223\n",
            "Acc : 0.515625 train_loss : 0.1723042612097093\n",
            "Acc : 0.5202020406723022 train_loss : 0.17549797433938058\n",
            "Acc : 0.5247790217399597 train_loss : 0.17854623418196897\n",
            "Acc : 0.5295138955116272 train_loss : 0.17891460193183134\n",
            "Acc : 0.5342487096786499 train_loss : 0.17979985070543006\n",
            "Acc : 0.5392992496490479 train_loss : 0.1785779699513858\n",
            "Acc : 0.544349730014801 train_loss : 0.17728543753089668\n",
            "Acc : 0.5492424368858337 train_loss : 0.17883398292386637\n",
            "Acc : 0.5539772510528564 train_loss : 0.18126084518709543\n",
            "Acc : 0.5590277910232544 train_loss : 0.18016557426502308\n",
            "Acc : 0.5637626051902771 train_loss : 0.18253138070197208\n",
            "Acc : 0.568813145160675 train_loss : 0.18116012774407864\n",
            "Acc : 0.5738636255264282 train_loss : 0.17976217379427364\n",
            "Acc : 0.5787563323974609 train_loss : 0.17922369270759114\n",
            "Acc : 0.5838068127632141 train_loss : 0.17807833912993679\n",
            "Acc : 0.5886995196342468 train_loss : 0.17894944921135902\n",
            "Acc : 0.59375 train_loss : 0.17777040671587976\n",
            "Acc : 0.5983270406723022 train_loss : 0.18183596160446033\n",
            "Acc : 0.6032196879386902 train_loss : 0.1826155328350823\n",
            "Acc : 0.6082702279090881 train_loss : 0.18136336366015096\n",
            "Acc : 0.6133207082748413 train_loss : 0.18003411553800105\n",
            "Acc : 0.6183711886405945 train_loss : 0.17901255443160022\n",
            "Acc : 0.6232638955116272 train_loss : 0.17839246323904184\n",
            "Acc : 0.6281565427780151 train_loss : 0.17758205642167013\n",
            "Acc : 0.6330492496490479 train_loss : 0.17692875260010707\n",
            "Acc : 0.6379418969154358 train_loss : 0.1773520830588845\n",
            "Acc : 0.6428346037864685 train_loss : 0.176952529371581\n",
            "Acc : 0.6474116444587708 train_loss : 0.17808219980956477\n",
            "Acc : 0.6524621248245239 train_loss : 0.1771752440363617\n",
            "Acc : 0.6570391654968262 train_loss : 0.18058339343753768\n",
            "Acc : 0.6620896458625793 train_loss : 0.1795665536351778\n",
            "Acc : 0.6669822931289673 train_loss : 0.17989893275422647\n",
            "Acc : 0.671875 train_loss : 0.1805846866927225\n",
            "Acc : 0.6769254803657532 train_loss : 0.17986976874965258\n",
            "Acc : 0.6819760203361511 train_loss : 0.17919961342976676\n",
            "Acc : 0.6868686676025391 train_loss : 0.17868847066004362\n",
            "Acc : 0.6912878751754761 train_loss : 0.18172558556898688\n",
            "Acc : 0.6960227489471436 train_loss : 0.1834800254527322\n",
            "Acc : 0.7010732293128967 train_loss : 0.18293028951368548\n",
            "Acc : 0.7058081030845642 train_loss : 0.18333538100382107\n",
            "Acc : 0.7107007503509521 train_loss : 0.1832134307073108\n",
            "Acc : 0.7154356241226196 train_loss : 0.18286037186084136\n",
            "Acc : 0.7204861044883728 train_loss : 0.18182980625250308\n",
            "Acc : 0.7253788113594055 train_loss : 0.18141159844408566\n",
            "Acc : 0.7302714586257935 train_loss : 0.1810767546221114\n",
            "Acc : 0.7351641654968262 train_loss : 0.18186519638945658\n",
            "Acc : 0.7400568127632141 train_loss : 0.1821635810177255\n",
            "Acc : 0.7451072931289673 train_loss : 0.18119101274121358\n",
            "Acc : 0.7501578330993652 train_loss : 0.18088391986888608\n",
            "Acc : 0.7552083134651184 train_loss : 0.1800526777379118\n",
            "Acc : 0.7601010203361511 train_loss : 0.1799264339790229\n",
            "Acc : 0.7649936676025391 train_loss : 0.17991840028657746\n",
            "Acc : 0.770044207572937 train_loss : 0.17909139789830728\n",
            "Acc : 0.774936854839325 train_loss : 0.17881659405376715\n",
            "Acc : 0.7796717286109924 train_loss : 0.17981014889225644\n",
            "Acc : 0.7845643758773804 train_loss : 0.17962504090974107\n",
            "Acc : 0.7894570827484131 train_loss : 0.17937296079413861\n",
            "Acc : 0.7945075631141663 train_loss : 0.17889976061098736\n",
            "Acc : 0.799400269985199 train_loss : 0.1785416778268441\n",
            "Acc : 0.8044507503509521 train_loss : 0.1786250945112509\n",
            "Acc : 0.8095012903213501 train_loss : 0.17785973107498704\n",
            "Acc : 0.8145517706871033 train_loss : 0.17695192292512182\n",
            "Acc : 0.8196022510528564 train_loss : 0.1762543818327838\n",
            "Acc : 0.8243371248245239 train_loss : 0.17717094149529225\n",
            "Acc : 0.8292297720909119 train_loss : 0.17686804624263353\n",
            "Acc : 0.8342803120613098 train_loss : 0.17608879304984038\n",
            "Acc : 0.839330792427063 train_loss : 0.1754490316175578\n",
            "Acc : 0.8442234992980957 train_loss : 0.17652752904524638\n",
            "Acc : 0.8492739796638489 train_loss : 0.17582323296652363\n",
            "Acc : 0.8540088534355164 train_loss : 0.1766589723484612\n",
            "Acc : 0.8589015007019043 train_loss : 0.176580090458904\n",
            "Acc : 0.8639520406723022 train_loss : 0.17592458640733225\n",
            "Acc : 0.8690025210380554 train_loss : 0.17536987290628211\n",
            "Acc : 0.8740530014038086 train_loss : 0.1749903563242615\n",
            "Acc : 0.8789457082748413 train_loss : 0.17491611698902518\n",
            "Acc : 0.8839961886405945 train_loss : 0.17407541323660147\n",
            "Acc : 0.8890467286109924 train_loss : 0.17348140670573184\n",
            "Acc : 0.8939393758773804 train_loss : 0.17407343195812716\n",
            "Acc : 0.8989899158477783 train_loss : 0.17331669242452077\n",
            "Acc : 0.9038825631141663 train_loss : 0.17297397366643924\n",
            "Acc : 0.908775269985199 train_loss : 0.17300898028990708\n",
            "Acc : 0.9133522510528564 train_loss : 0.1732503246135449\n",
            "Acc : 0.9184027910232544 train_loss : 0.1725546555583649\n",
            "Acc : 0.9232954382896423 train_loss : 0.17238621624741465\n",
            "Acc : 0.9278724789619446 train_loss : 0.17413312469722417\n",
            "Acc : 0.9327651262283325 train_loss : 0.17449954400133144\n",
            "Acc : 0.9378156661987305 train_loss : 0.17365771756843904\n",
            "Acc : 0.9425504803657532 train_loss : 0.17361862296335553\n",
            "Acc : 0.9476010203361511 train_loss : 0.1728952891516639\n",
            "Acc : 0.9524936676025391 train_loss : 0.17246497918362008\n",
            "Acc : 0.957544207572937 train_loss : 0.1718447553853576\n",
            "Acc : 0.9625946879386902 train_loss : 0.17117234977551413\n",
            "Acc : 0.9671717286109924 train_loss : 0.17325771524840353\n",
            "Acc : 0.9722222089767456 train_loss : 0.17327233965066496\n",
            "Validating....\n",
            "Acc 0.017676766961812973 train_loss 0.6934621930122375\n",
            "Acc 0.03598484769463539 train_loss 0.6975177824497223\n",
            "Acc 0.056186869740486145 train_loss 0.4713250355174144\n",
            "Acc 0.07386363297700882 train_loss 0.45260264398530126\n",
            "Acc 0.09280303120613098 train_loss 0.4420221660286188\n",
            "Acc 0.10984848439693451 train_loss 0.5181670365855098\n",
            "Acc 0.1262626200914383 train_loss 0.6191184672393969\n",
            "Acc 0.14520202577114105 train_loss 0.5767038634512573\n",
            "Acc 0.16287878155708313 train_loss 0.5834370870143175\n",
            "Acc 0.1818181872367859 train_loss 0.5902932183817029\n",
            "Acc 0.19949494302272797 train_loss 0.6271229076453231\n",
            "Acc 0.2165404111146927 train_loss 0.6599333548607925\n",
            "Acc 0.2348484843969345 train_loss 0.638847146039972\n",
            "Acc 0.2537878751754761 train_loss 0.6135809582525066\n",
            "Acc 0.2733585834503174 train_loss 0.579386581107974\n",
            "Acc 0.29103535413742065 train_loss 0.6009213897632435\n",
            "Acc 0.3099747598171234 train_loss 0.5799084105254972\n",
            "Acc 0.3276515007019043 train_loss 0.5741808026408156\n",
            "Acc 0.34785354137420654 train_loss 0.5458106213298283\n",
            "Acc 0.3680555522441864 train_loss 0.5200398820452392\n",
            "Acc 0.38825756311416626 train_loss 0.49585495707357213\n",
            "Acc 0.40782827138900757 train_loss 0.48875695903023536\n",
            "Acc 0.4267676770687103 train_loss 0.48474594590294623\n",
            "Acc 0.44507575035095215 train_loss 0.48941089985116076\n",
            "Acc 0.46212121844291687 train_loss 0.5032673480734229\n",
            "Acc 0.4804292917251587 train_loss 0.5156275747128978\n",
            "Acc 0.49936869740486145 train_loss 0.5083670739981311\n",
            "Acc 0.5176767706871033 train_loss 0.5133709558618388\n",
            "Acc 0.5353535413742065 train_loss 0.5334813463469518\n",
            "Acc 0.5530303120613098 train_loss 0.5362431200531622\n",
            "Acc 0.5713383555412292 train_loss 0.540088279803674\n",
            "Acc 0.5909090638160706 train_loss 0.5301162425603252\n",
            "Acc 0.6098484992980957 train_loss 0.5269013582023255\n",
            "Acc 0.626893937587738 train_loss 0.5434843768256113\n",
            "Acc 0.6452020406723022 train_loss 0.5461077751591802\n",
            "Acc 0.6647727489471436 train_loss 0.5384948114709308\n",
            "Acc 0.6818181872367859 train_loss 0.5500830341180837\n",
            "Acc 0.7001262903213501 train_loss 0.5544486808296489\n",
            "Acc 0.7171717286109924 train_loss 0.5611649287195924\n",
            "Acc 0.7342171669006348 train_loss 0.5689079278847202\n",
            "Acc 0.751893937587738 train_loss 0.5664112249661873\n",
            "Acc 0.7695707082748413 train_loss 0.5761250982814956\n",
            "Acc 0.7878788113594055 train_loss 0.5763929959083366\n",
            "Acc 0.8055555820465088 train_loss 0.5800538389647211\n",
            "Acc 0.8244949579238892 train_loss 0.5738446660960714\n",
            "Acc 0.8428030014038086 train_loss 0.570473506540546\n",
            "Acc 0.8611111044883728 train_loss 0.5723239425767926\n",
            "Acc 0.8800504803657532 train_loss 0.5700779235727774\n",
            "Acc 0.8989899158477783 train_loss 0.5703421990596214\n",
            "Acc 0.9078282713890076 train_loss 0.5758591891638934\n",
            "F1_score : 0.8266033254156768\n",
            "Epoch completed in 1.4358858585357666 minutes\n",
            "***************\n",
            "EPOCH: 9\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004734848625957966 train_loss : 0.40346401929855347\n",
            "Acc : 0.009627525694668293 train_loss : 0.25924256071448326\n",
            "Acc : 0.014678030274808407 train_loss : 0.20120064665873846\n",
            "Acc : 0.019728535786271095 train_loss : 0.15681243781000376\n",
            "Acc : 0.02477904036641121 train_loss : 0.14696788862347604\n",
            "Acc : 0.029829544946551323 train_loss : 0.13139000969628492\n",
            "Acc : 0.03488004952669144 train_loss : 0.1233793491763728\n",
            "Acc : 0.0399305559694767 train_loss : 0.1136619676835835\n",
            "Acc : 0.04482323303818703 train_loss : 0.12030789959761831\n",
            "Acc : 0.04987373575568199 train_loss : 0.1125579096376896\n",
            "Acc : 0.054924242198467255 train_loss : 0.10931824960491875\n",
            "Acc : 0.05997474864125252 train_loss : 0.10949323823054631\n",
            "Acc : 0.06502525508403778 train_loss : 0.10342675705368702\n",
            "Acc : 0.06991793215274811 train_loss : 0.10786381284041065\n",
            "Acc : 0.07481060922145844 train_loss : 0.11263793831070264\n",
            "Acc : 0.0798611119389534 train_loss : 0.11249598604626954\n",
            "Acc : 0.08491161465644836 train_loss : 0.10868059317855273\n",
            "Acc : 0.08980429172515869 train_loss : 0.10945555319388707\n",
            "Acc : 0.09453914314508438 train_loss : 0.12319854450853247\n",
            "Acc : 0.09927398711442947 train_loss : 0.1329224191606045\n",
            "Acc : 0.1041666641831398 train_loss : 0.13352625923497335\n",
            "Acc : 0.10905934125185013 train_loss : 0.1451705050739375\n",
            "Acc : 0.11410985141992569 train_loss : 0.14258564684702002\n",
            "Acc : 0.11916035413742065 train_loss : 0.1382585077856978\n",
            "Acc : 0.12421085685491562 train_loss : 0.137101891040802\n",
            "Acc : 0.12878787517547607 train_loss : 0.15410818732701814\n",
            "Acc : 0.13336490094661713 train_loss : 0.1702267704186616\n",
            "Acc : 0.1384154111146927 train_loss : 0.16634489995028293\n",
            "Acc : 0.14330807328224182 train_loss : 0.16575492635883135\n",
            "Acc : 0.14835858345031738 train_loss : 0.16433075120051702\n",
            "Acc : 0.15309342741966248 train_loss : 0.1657897748293415\n",
            "Acc : 0.1579861044883728 train_loss : 0.1732234084047377\n",
            "Acc : 0.16287878155708313 train_loss : 0.17264286360957407\n",
            "Acc : 0.16761364042758942 train_loss : 0.18418002873659134\n",
            "Acc : 0.17266413569450378 train_loss : 0.18031731503350393\n",
            "Acc : 0.17739899456501007 train_loss : 0.18392576525608698\n",
            "Acc : 0.1822916716337204 train_loss : 0.18301392366757263\n",
            "Acc : 0.18734216690063477 train_loss : 0.1797993681148479\n",
            "Acc : 0.1922348439693451 train_loss : 0.1816593022682728\n",
            "Acc : 0.19696970283985138 train_loss : 0.18313179947435856\n",
            "Acc : 0.20154671370983124 train_loss : 0.18887513694239827\n",
            "Acc : 0.2065972238779068 train_loss : 0.18581356870986165\n",
            "Acc : 0.21164773404598236 train_loss : 0.18317138178403988\n",
            "Acc : 0.21669822931289673 train_loss : 0.1813931419429454\n",
            "Acc : 0.22143307328224182 train_loss : 0.18154351860284806\n",
            "Acc : 0.22632575035095215 train_loss : 0.17991525274903877\n",
            "Acc : 0.2313762605190277 train_loss : 0.177549088413411\n",
            "Acc : 0.23642677068710327 train_loss : 0.17577671694258848\n",
            "Acc : 0.2413194477558136 train_loss : 0.17494140443753223\n",
            "Acc : 0.24621212482452393 train_loss : 0.17564365059137343\n",
            "Acc : 0.2512626349925995 train_loss : 0.17415164498721852\n",
            "Acc : 0.2561553120613098 train_loss : 0.17523961886763573\n",
            "Acc : 0.26104798913002014 train_loss : 0.17639260443876376\n",
            "Acc : 0.26578283309936523 train_loss : 0.17559582574499977\n",
            "Acc : 0.2708333432674408 train_loss : 0.17282581850886344\n",
            "Acc : 0.27588382363319397 train_loss : 0.1706080002311085\n",
            "Acc : 0.2807765007019043 train_loss : 0.17009243414851657\n",
            "Acc : 0.28582701086997986 train_loss : 0.1688793552718286\n",
            "Acc : 0.2908775210380554 train_loss : 0.16686454844676843\n",
            "Acc : 0.29577019810676575 train_loss : 0.16816777661442756\n",
            "Acc : 0.3008207082748413 train_loss : 0.16632788829871867\n",
            "Acc : 0.30571338534355164 train_loss : 0.16818262870994308\n",
            "Acc : 0.31044822931289673 train_loss : 0.16781403213029816\n",
            "Acc : 0.3154987394809723 train_loss : 0.16714845545357093\n",
            "Acc : 0.3203914165496826 train_loss : 0.17054117407936317\n",
            "Acc : 0.3254419267177582 train_loss : 0.16864246875047684\n",
            "Acc : 0.33049243688583374 train_loss : 0.16648119332185432\n",
            "Acc : 0.33538511395454407 train_loss : 0.1683779372011914\n",
            "Acc : 0.34043559432029724 train_loss : 0.16622692163007846\n",
            "Acc : 0.3451704680919647 train_loss : 0.16731245336788042\n",
            "Acc : 0.35006314516067505 train_loss : 0.1694632423385768\n",
            "Acc : 0.3549558222293854 train_loss : 0.16998696606606245\n",
            "Acc : 0.3598484992980957 train_loss : 0.17017487416120425\n",
            "Acc : 0.36474114656448364 train_loss : 0.1696030039440941\n",
            "Acc : 0.3697916567325592 train_loss : 0.16793527076641718\n",
            "Acc : 0.37468433380126953 train_loss : 0.16674366456113363\n",
            "Acc : 0.37957701086997986 train_loss : 0.16638929464600302\n",
            "Acc : 0.38431185483932495 train_loss : 0.16795558234055838\n",
            "Acc : 0.3893623650074005 train_loss : 0.16677253165199787\n",
            "Acc : 0.3944128751754761 train_loss : 0.16523403408937157\n",
            "Acc : 0.3993055522441864 train_loss : 0.16495155934014438\n",
            "Acc : 0.4040403962135315 train_loss : 0.16892242045482483\n",
            "Acc : 0.40909090638160706 train_loss : 0.16725836564079824\n",
            "Acc : 0.4139835834503174 train_loss : 0.16796484620620808\n",
            "Acc : 0.41903409361839294 train_loss : 0.16663276632042492\n",
            "Acc : 0.42392677068710327 train_loss : 0.166804272854744\n",
            "Acc : 0.42897728085517883 train_loss : 0.1653938305018277\n",
            "Acc : 0.4337121248245239 train_loss : 0.1672179960984398\n",
            "Acc : 0.4387626349925995 train_loss : 0.16604514323760955\n",
            "Acc : 0.44381314516067505 train_loss : 0.16468901083701187\n",
            "Acc : 0.4488636255264282 train_loss : 0.1634916634104409\n",
            "Acc : 0.45375630259513855 train_loss : 0.1626454805145445\n",
            "Acc : 0.4586489796638489 train_loss : 0.16421881826814785\n",
            "Acc : 0.46369948983192444 train_loss : 0.1629718026899277\n",
            "Acc : 0.46875 train_loss : 0.1617415165430621\n",
            "Acc : 0.4736426770687103 train_loss : 0.16266395904434225\n",
            "Acc : 0.47853535413742065 train_loss : 0.1618708289929272\n",
            "Acc : 0.4835858643054962 train_loss : 0.16066570770071478\n",
            "Acc : 0.4886363744735718 train_loss : 0.15924884911363174\n",
            "Acc : 0.49368685483932495 train_loss : 0.15790019882842898\n",
            "Acc : 0.4985795319080353 train_loss : 0.15705734769822938\n",
            "Acc : 0.5034722089767456 train_loss : 0.15808227612618722\n",
            "Acc : 0.5085227489471436 train_loss : 0.15702821894829133\n",
            "Acc : 0.5134153962135315 train_loss : 0.15949981733082005\n",
            "Acc : 0.5183081030845642 train_loss : 0.158826224381725\n",
            "Acc : 0.5230429172515869 train_loss : 0.16035850889067044\n",
            "Acc : 0.5277777910232544 train_loss : 0.1625204814127951\n",
            "Acc : 0.5328282713890076 train_loss : 0.16129475504297902\n",
            "Acc : 0.537563145160675 train_loss : 0.16180801709447432\n",
            "Acc : 0.5426136255264282 train_loss : 0.16063775542107495\n",
            "Acc : 0.5473484992980957 train_loss : 0.16160851166591989\n",
            "Acc : 0.5520833134651184 train_loss : 0.1623256304966552\n",
            "Acc : 0.5569760203361511 train_loss : 0.16268004793508917\n",
            "Acc : 0.5620265007019043 train_loss : 0.16165078607828995\n",
            "Acc : 0.5667613744735718 train_loss : 0.16288308740957924\n",
            "Acc : 0.571811854839325 train_loss : 0.16185338278140488\n",
            "Acc : 0.5768623948097229 train_loss : 0.16065731405829772\n",
            "Acc : 0.5817550420761108 train_loss : 0.16131982930257158\n",
            "Acc : 0.5866477489471436 train_loss : 0.16088683912608803\n",
            "Acc : 0.5915403962135315 train_loss : 0.16175217004492878\n",
            "Acc : 0.5965909361839294 train_loss : 0.16081728943246454\n",
            "Acc : 0.6013257503509521 train_loss : 0.1641213974808572\n",
            "Acc : 0.6062184572219849 train_loss : 0.16482103794692007\n",
            "Acc : 0.6111111044883728 train_loss : 0.1647037712616786\n",
            "Acc : 0.6161616444587708 train_loss : 0.16378216341137886\n",
            "Acc : 0.6212121248245239 train_loss : 0.16289517079435645\n",
            "Acc : 0.6262626051902771 train_loss : 0.16199941000365836\n",
            "Acc : 0.6311553120613098 train_loss : 0.1616875280160457\n",
            "Acc : 0.636205792427063 train_loss : 0.16132925202449164\n",
            "Acc : 0.6410984992980957 train_loss : 0.16177574699887862\n",
            "Acc : 0.6461489796638489 train_loss : 0.1612421967487299\n",
            "Acc : 0.6510416865348816 train_loss : 0.1611094129920909\n",
            "Acc : 0.6560921669006348 train_loss : 0.16026576670040762\n",
            "Acc : 0.6605113744735718 train_loss : 0.16328056323439327\n",
            "Acc : 0.6657196879386902 train_loss : 0.16236988984876208\n",
            "Acc : 0.6704545617103577 train_loss : 0.16313243235516198\n",
            "Acc : 0.6753472089767456 train_loss : 0.1635529032915178\n",
            "Acc : 0.6803977489471436 train_loss : 0.16295162852907527\n",
            "Acc : 0.6852903962135315 train_loss : 0.16241085942057396\n",
            "Acc : 0.6903409361839294 train_loss : 0.1621645479862179\n",
            "Acc : 0.6950757503509521 train_loss : 0.1646707594289002\n",
            "Acc : 0.6998106241226196 train_loss : 0.1661172865457098\n",
            "Acc : 0.7048611044883728 train_loss : 0.1654679382389242\n",
            "Acc : 0.7097538113594055 train_loss : 0.16534168200774324\n",
            "Acc : 0.7146464586257935 train_loss : 0.1648631512604911\n",
            "Acc : 0.7196969985961914 train_loss : 0.16467594911586747\n",
            "Acc : 0.7247474789619446 train_loss : 0.16398209226982935\n",
            "Acc : 0.7297979593276978 train_loss : 0.16343790524311969\n",
            "Acc : 0.7346906661987305 train_loss : 0.16318563116876872\n",
            "Acc : 0.7395833134651184 train_loss : 0.16399605065584183\n",
            "Acc : 0.7444760203361511 train_loss : 0.16412706702750252\n",
            "Acc : 0.7495265007019043 train_loss : 0.16318803029379955\n",
            "Acc : 0.7545770406723022 train_loss : 0.16282527907173228\n",
            "Acc : 0.7596275210380554 train_loss : 0.1620319542632281\n",
            "Acc : 0.7645202279090881 train_loss : 0.16199470161189955\n",
            "Acc : 0.7694128751754761 train_loss : 0.1617284678758528\n",
            "Acc : 0.7744633555412292 train_loss : 0.16117361931806537\n",
            "Acc : 0.779356062412262 train_loss : 0.16114095007835688\n",
            "Acc : 0.7842487096786499 train_loss : 0.16145750218358054\n",
            "Acc : 0.7891414165496826 train_loss : 0.16122514003654942\n",
            "Acc : 0.7940340638160706 train_loss : 0.16087098279678674\n",
            "Acc : 0.7990846037864685 train_loss : 0.160047939794575\n",
            "Acc : 0.8038194179534912 train_loss : 0.15995222804325124\n",
            "Acc : 0.8088699579238892 train_loss : 0.15964701402614392\n",
            "Acc : 0.8139204382896423 train_loss : 0.15894594162025236\n",
            "Acc : 0.8189709782600403 train_loss : 0.15812835154538774\n",
            "Acc : 0.8240214586257935 train_loss : 0.15746379212035747\n",
            "Acc : 0.8285984992980957 train_loss : 0.1589158284810505\n",
            "Acc : 0.8336489796638489 train_loss : 0.15812343545258045\n",
            "Acc : 0.8386995196342468 train_loss : 0.15764642310712268\n",
            "Acc : 0.8435921669006348 train_loss : 0.1574840937463338\n",
            "Acc : 0.8484848737716675 train_loss : 0.15804524889735636\n",
            "Acc : 0.8533775210380554 train_loss : 0.1577451444968495\n",
            "Acc : 0.8584280014038086 train_loss : 0.1575004886728765\n",
            "Acc : 0.8634785413742065 train_loss : 0.15705300591886043\n",
            "Acc : 0.8685290217399597 train_loss : 0.15647242397112263\n",
            "Acc : 0.8734217286109924 train_loss : 0.15631402339681058\n",
            "Acc : 0.8784722089767456 train_loss : 0.15571556969681818\n",
            "Acc : 0.8833649158477783 train_loss : 0.15567955292511584\n",
            "Acc : 0.8882575631141663 train_loss : 0.1552229705473615\n",
            "Acc : 0.893150269985199 train_loss : 0.1550095858186154\n",
            "Acc : 0.8978850841522217 train_loss : 0.15597354298314223\n",
            "Acc : 0.9029356241226196 train_loss : 0.15537534761013555\n",
            "Acc : 0.9078282713890076 train_loss : 0.15524390267977572\n",
            "Acc : 0.9127209782600403 train_loss : 0.15540498117538723\n",
            "Acc : 0.917455792427063 train_loss : 0.15531941368094376\n",
            "Acc : 0.9223484992980957 train_loss : 0.1553130403340341\n",
            "Acc : 0.9273989796638489 train_loss : 0.1546483916091792\n",
            "Acc : 0.9321338534355164 train_loss : 0.1551181442601971\n",
            "Acc : 0.9371843338012695 train_loss : 0.1544081843311065\n",
            "Acc : 0.9422348737716675 train_loss : 0.15366742628777683\n",
            "Acc : 0.9472853541374207 train_loss : 0.15301099613134284\n",
            "Acc : 0.9523358345031738 train_loss : 0.15247094067093467\n",
            "Acc : 0.9573863744735718 train_loss : 0.15193515915673264\n",
            "Acc : 0.962436854839325 train_loss : 0.15130651932782851\n",
            "Acc : 0.9673295617103577 train_loss : 0.1514236557116846\n",
            "Acc : 0.9717487096786499 train_loss : 0.1541655019332234\n",
            "Acc : 0.9764835834503174 train_loss : 0.15443345847436124\n",
            "Validating....\n",
            "Acc 0.018939394503831863 train_loss 0.3756089210510254\n",
            "Acc 0.036616161465644836 train_loss 0.5583725273609161\n",
            "Acc 0.05681818351149559 train_loss 0.377248124529918\n",
            "Acc 0.07512626051902771 train_loss 0.371166137047112\n",
            "Acc 0.09406565874814987 train_loss 0.3380852617323399\n",
            "Acc 0.1111111119389534 train_loss 0.4528015187631051\n",
            "Acc 0.12752525508403778 train_loss 0.5454927794635296\n",
            "Acc 0.1470959633588791 train_loss 0.5092743472196162\n",
            "Acc 0.16603535413742065 train_loss 0.513813560621606\n",
            "Acc 0.18434342741966248 train_loss 0.5309874761849642\n",
            "Acc 0.20202019810676575 train_loss 0.5419523380696774\n",
            "Acc 0.21969696879386902 train_loss 0.5749175737922391\n",
            "Acc 0.23926767706871033 train_loss 0.5445877614502723\n",
            "Acc 0.25883838534355164 train_loss 0.521439772897533\n",
            "Acc 0.2790403962135315 train_loss 0.48868319764733315\n",
            "Acc 0.29671716690063477 train_loss 0.5199301594402641\n",
            "Acc 0.3162878751754761 train_loss 0.49795106382054444\n",
            "Acc 0.33396464586257935 train_loss 0.498787179382311\n",
            "Acc 0.3541666567325592 train_loss 0.4736309560309899\n",
            "Acc 0.37436869740486145 train_loss 0.4515862396918237\n",
            "Acc 0.3945707082748413 train_loss 0.4306744250601956\n",
            "Acc 0.4141414165496826 train_loss 0.4265099396844479\n",
            "Acc 0.4330808222293854 train_loss 0.424517913883471\n",
            "Acc 0.4513888955116272 train_loss 0.4352268202928826\n",
            "Acc 0.46906566619873047 train_loss 0.4457376335933805\n",
            "Acc 0.48674243688583374 train_loss 0.45928316839182604\n",
            "Acc 0.5056818127632141 train_loss 0.45441394371704924\n",
            "Acc 0.5239899158477783 train_loss 0.46141651262795286\n",
            "Acc 0.5416666865348816 train_loss 0.48671202814399167\n",
            "Acc 0.5593434572219849 train_loss 0.49591513465469084\n",
            "Acc 0.5776515007019043 train_loss 0.5035137244830689\n",
            "Acc 0.5972222089767456 train_loss 0.4930762219300959\n",
            "Acc 0.6167929172515869 train_loss 0.4856492036276243\n",
            "Acc 0.6344696879386902 train_loss 0.502761098309694\n",
            "Acc 0.6527777910232544 train_loss 0.5080386935866305\n",
            "Acc 0.6717171669006348 train_loss 0.49856949507051873\n",
            "Acc 0.689393937587738 train_loss 0.5082109408654474\n",
            "Acc 0.7077020406723022 train_loss 0.5102422032750359\n",
            "Acc 0.7247474789619446 train_loss 0.5191093789509091\n",
            "Acc 0.7411616444587708 train_loss 0.5244531854288652\n",
            "Acc 0.7588383555412292 train_loss 0.522542222175838\n",
            "Acc 0.7758838534355164 train_loss 0.533232386556587\n",
            "Acc 0.7929292917251587 train_loss 0.5370948272432352\n",
            "Acc 0.810606062412262 train_loss 0.5430328249296342\n",
            "Acc 0.8295454382896423 train_loss 0.5379715150636103\n",
            "Acc 0.8491161465644836 train_loss 0.5333042280222087\n",
            "Acc 0.8667929172515869 train_loss 0.5384452374097197\n",
            "Acc 0.8863636255264282 train_loss 0.5337569414987229\n",
            "Acc 0.9046717286109924 train_loss 0.5357563896484825\n",
            "Acc 0.9135100841522217 train_loss 0.5409824489988386\n",
            "F1_score : 0.844141069397042\n",
            "Epoch completed in 1.4463428298632304 minutes\n",
            "***************\n",
            "EPOCH: 10\n",
            "***************\n",
            "Training.....\n",
            "Acc : 0.004734848625957966 train_loss : 0.26200583577156067\n",
            "Acc : 0.009627525694668293 train_loss : 0.19154111295938492\n",
            "Acc : 0.014678030274808407 train_loss : 0.15542126695315042\n",
            "Acc : 0.019728535786271095 train_loss : 0.12204163568094373\n",
            "Acc : 0.024621212854981422 train_loss : 0.11786261536180972\n",
            "Acc : 0.029671717435121536 train_loss : 0.10421590910603602\n",
            "Acc : 0.0347222238779068 train_loss : 0.09864767348127705\n",
            "Acc : 0.039772726595401764 train_loss : 0.09127934207208455\n",
            "Acc : 0.04482323303818703 train_loss : 0.08429811750021246\n",
            "Acc : 0.04987373575568199 train_loss : 0.08017752040177584\n",
            "Acc : 0.054924242198467255 train_loss : 0.07839831929992545\n",
            "Acc : 0.05997474864125252 train_loss : 0.07971639636283119\n",
            "Acc : 0.06502525508403778 train_loss : 0.07681890648718064\n",
            "Acc : 0.07007575780153275 train_loss : 0.07383215839841537\n",
            "Acc : 0.07496843487024307 train_loss : 0.08575337715446948\n",
            "Acc : 0.08001893758773804 train_loss : 0.08293758344370872\n",
            "Acc : 0.08491161465644836 train_loss : 0.08765928458203287\n",
            "Acc : 0.08996212482452393 train_loss : 0.08527202997356653\n",
            "Acc : 0.09485479444265366 train_loss : 0.09361682293054305\n",
            "Acc : 0.09958964586257935 train_loss : 0.09865455636754632\n",
            "Acc : 0.10448232293128967 train_loss : 0.09883630994175162\n",
            "Acc : 0.109375 train_loss : 0.11055635872550985\n",
            "Acc : 0.11442550271749496 train_loss : 0.10701655960925248\n",
            "Acc : 0.11947601288557053 train_loss : 0.10514809540472925\n",
            "Acc : 0.12452651560306549 train_loss : 0.10423599161207676\n",
            "Acc : 0.1289457082748413 train_loss : 0.1214788227270429\n",
            "Acc : 0.13352273404598236 train_loss : 0.13148832797176307\n",
            "Acc : 0.13857322931289673 train_loss : 0.12956213345751166\n",
            "Acc : 0.1436237394809723 train_loss : 0.12799524573673463\n",
            "Acc : 0.14867424964904785 train_loss : 0.12576969048629205\n",
            "Acc : 0.15372474491596222 train_loss : 0.12386587172025634\n",
            "Acc : 0.1584595888853073 train_loss : 0.13391782535472885\n",
            "Acc : 0.16351009905338287 train_loss : 0.13084186246675072\n",
            "Acc : 0.16824494302272797 train_loss : 0.14123980651664383\n",
            "Acc : 0.17329545319080353 train_loss : 0.13823452437562603\n",
            "Acc : 0.17818813025951385 train_loss : 0.13715929553533593\n",
            "Acc : 0.18308080732822418 train_loss : 0.13688761373428074\n",
            "Acc : 0.18813131749629974 train_loss : 0.13425804679527095\n",
            "Acc : 0.19302399456501007 train_loss : 0.13588266843595567\n",
            "Acc : 0.19807448983192444 train_loss : 0.1347489055711776\n",
            "Acc : 0.20296716690063477 train_loss : 0.13594243371086875\n",
            "Acc : 0.20801767706871033 train_loss : 0.13348244312441065\n",
            "Acc : 0.2130681872367859 train_loss : 0.13141358474832635\n",
            "Acc : 0.21811868250370026 train_loss : 0.12976060557941144\n",
            "Acc : 0.22301135957241058 train_loss : 0.13097438203791778\n",
            "Acc : 0.2279040366411209 train_loss : 0.13069711694412905\n",
            "Acc : 0.23295454680919647 train_loss : 0.1288286722166107\n",
            "Acc : 0.23800505697727203 train_loss : 0.12755583028774709\n",
            "Acc : 0.24289773404598236 train_loss : 0.12773612768826437\n",
            "Acc : 0.2477904111146927 train_loss : 0.12751093480736017\n",
            "Acc : 0.25284090638160706 train_loss : 0.12621418288087144\n",
            "Acc : 0.2577335834503174 train_loss : 0.12752852735754389\n",
            "Acc : 0.26278409361839294 train_loss : 0.1270178842164998\n",
            "Acc : 0.26767677068710327 train_loss : 0.126465300615463\n",
            "Acc : 0.27272728085517883 train_loss : 0.12457568479532545\n",
            "Acc : 0.27761995792388916 train_loss : 0.12441962475090154\n",
            "Acc : 0.2826704680919647 train_loss : 0.12319228435425382\n",
            "Acc : 0.2877209484577179 train_loss : 0.1227500952844476\n",
            "Acc : 0.29277145862579346 train_loss : 0.12126924309058715\n",
            "Acc : 0.2976641356945038 train_loss : 0.12218580975507697\n",
            "Acc : 0.30271464586257935 train_loss : 0.12066345597754737\n",
            "Acc : 0.3076073229312897 train_loss : 0.12230119154217743\n",
            "Acc : 0.31265783309936523 train_loss : 0.12126552488004404\n",
            "Acc : 0.31755051016807556 train_loss : 0.12087267913739197\n",
            "Acc : 0.3224431872367859 train_loss : 0.12430524227137749\n",
            "Acc : 0.32749369740486145 train_loss : 0.12268216862823024\n",
            "Acc : 0.3325441777706146 train_loss : 0.12115227375457537\n",
            "Acc : 0.3372790515422821 train_loss : 0.12361128803561716\n",
            "Acc : 0.3423295319080353 train_loss : 0.12207271768778995\n",
            "Acc : 0.34706440567970276 train_loss : 0.12307311152773244\n",
            "Acc : 0.3519570827484131 train_loss : 0.12602115333290168\n",
            "Acc : 0.3568497598171234 train_loss : 0.12588584707635972\n",
            "Acc : 0.36174243688583374 train_loss : 0.12518278841082364\n",
            "Acc : 0.3667929172515869 train_loss : 0.12462264807844484\n",
            "Acc : 0.3718434274196625 train_loss : 0.12339947024981181\n",
            "Acc : 0.3767361044883728 train_loss : 0.12276586586315381\n",
            "Acc : 0.38178661465644836 train_loss : 0.1221336524981957\n",
            "Acc : 0.3866792917251587 train_loss : 0.12353628663680492\n",
            "Acc : 0.39172980189323425 train_loss : 0.12246743976315365\n",
            "Acc : 0.3967803120613098 train_loss : 0.12142114485614001\n",
            "Acc : 0.40167298913002014 train_loss : 0.12098548142814342\n",
            "Acc : 0.40640783309936523 train_loss : 0.12566907662989163\n",
            "Acc : 0.4114583432674408 train_loss : 0.12463122705020101\n",
            "Acc : 0.4163510203361511 train_loss : 0.1253483427599782\n",
            "Acc : 0.4214015007019043 train_loss : 0.1245310221963069\n",
            "Acc : 0.4262941777706146 train_loss : 0.12512394209760566\n",
            "Acc : 0.4313446879386902 train_loss : 0.124021976886467\n",
            "Acc : 0.4360795319080353 train_loss : 0.1262834834527563\n",
            "Acc : 0.44113004207611084 train_loss : 0.1252548211160001\n",
            "Acc : 0.4461805522441864 train_loss : 0.12453158729606205\n",
            "Acc : 0.45123106241226196 train_loss : 0.12378050730778621\n",
            "Acc : 0.4561237394809723 train_loss : 0.12317908336610897\n",
            "Acc : 0.46117424964904785 train_loss : 0.1256218822572821\n",
            "Acc : 0.4662247598171234 train_loss : 0.12496560328501335\n",
            "Acc : 0.4712752401828766 train_loss : 0.1240420138365344\n",
            "Acc : 0.4761679172515869 train_loss : 0.12511487468145788\n",
            "Acc : 0.48106059432029724 train_loss : 0.12501611737246365\n",
            "Acc : 0.4861111044883728 train_loss : 0.12424385912564335\n",
            "Acc : 0.49116161465644836 train_loss : 0.12319723610775639\n",
            "Acc : 0.4962121248245239 train_loss : 0.12219293875619769\n",
            "Acc : 0.5012626051902771 train_loss : 0.12130323161214294\n",
            "Acc : 0.5061553120613098 train_loss : 0.12283177693904031\n",
            "Acc : 0.511205792427063 train_loss : 0.12198452121641451\n",
            "Acc : 0.5160984992980957 train_loss : 0.12488582219856863\n",
            "Acc : 0.5209911465644836 train_loss : 0.12485536824734438\n",
            "Acc : 0.5257260203361511 train_loss : 0.12685281071671337\n",
            "Acc : 0.5304608345031738 train_loss : 0.12933568981972254\n",
            "Acc : 0.5355113744735718 train_loss : 0.12828688073420413\n",
            "Acc : 0.540561854839325 train_loss : 0.12770905268780136\n",
            "Acc : 0.5454545617103577 train_loss : 0.12812852586873552\n",
            "Acc : 0.5505050420761108 train_loss : 0.12728397798229446\n",
            "Acc : 0.5553977489471436 train_loss : 0.12836084948919183\n",
            "Acc : 0.5601325631141663 train_loss : 0.1293357118337819\n",
            "Acc : 0.5651831030845642 train_loss : 0.1285163001892598\n",
            "Acc : 0.5699179172515869 train_loss : 0.12986501350027063\n",
            "Acc : 0.5749684572219849 train_loss : 0.12892273863263684\n",
            "Acc : 0.580018937587738 train_loss : 0.1280690333368177\n",
            "Acc : 0.5850694179534912 train_loss : 0.12783607510628842\n",
            "Acc : 0.5901199579238892 train_loss : 0.12732783634559947\n",
            "Acc : 0.5950126051902771 train_loss : 0.127758710567529\n",
            "Acc : 0.600063145160675 train_loss : 0.12689011570158576\n",
            "Acc : 0.6046401262283325 train_loss : 0.1309250824794662\n",
            "Acc : 0.6095328330993652 train_loss : 0.13184481103548673\n",
            "Acc : 0.6145833134651184 train_loss : 0.13097686272475026\n",
            "Acc : 0.6196338534355164 train_loss : 0.13016447421908378\n",
            "Acc : 0.6246843338012695 train_loss : 0.12951723051567873\n",
            "Acc : 0.6297348737716675 train_loss : 0.12921327296791113\n",
            "Acc : 0.6346275210380554 train_loss : 0.12907246124814264\n",
            "Acc : 0.6396780014038086 train_loss : 0.128791673325522\n",
            "Acc : 0.6444128751754761 train_loss : 0.12925157074171764\n",
            "Acc : 0.6494633555412292 train_loss : 0.12876733361195972\n",
            "Acc : 0.654356062412262 train_loss : 0.12908999957708697\n",
            "Acc : 0.6594065427780151 train_loss : 0.12829461360448285\n",
            "Acc : 0.6641414165496826 train_loss : 0.13133383155869904\n",
            "Acc : 0.6691918969154358 train_loss : 0.1305420276881368\n",
            "Acc : 0.6740846037864685 train_loss : 0.13073554576155455\n",
            "Acc : 0.6789772510528564 train_loss : 0.13142451357069243\n",
            "Acc : 0.6840277910232544 train_loss : 0.13072893343379963\n",
            "Acc : 0.6890782713890076 train_loss : 0.13003111181767296\n",
            "Acc : 0.6939709782600403 train_loss : 0.13002051888033747\n",
            "Acc : 0.6983901262283325 train_loss : 0.1331279815326557\n",
            "Acc : 0.703125 train_loss : 0.13425874087015088\n",
            "Acc : 0.7081754803657532 train_loss : 0.13379256001354514\n",
            "Acc : 0.7130681872367859 train_loss : 0.13406648881371236\n",
            "Acc : 0.7179608345031738 train_loss : 0.13441437093605255\n",
            "Acc : 0.7230113744735718 train_loss : 0.13382239561936218\n",
            "Acc : 0.728061854839325 train_loss : 0.1330505001935221\n",
            "Acc : 0.7329545617103577 train_loss : 0.1329917348040318\n",
            "Acc : 0.7378472089767456 train_loss : 0.13279885594926824\n",
            "Acc : 0.7427399158477783 train_loss : 0.1339950158322851\n",
            "Acc : 0.7476325631141663 train_loss : 0.13427399781020666\n",
            "Acc : 0.7526831030845642 train_loss : 0.13347534639270683\n",
            "Acc : 0.7577335834503174 train_loss : 0.13324742140917997\n",
            "Acc : 0.7627840638160706 train_loss : 0.13263657205290608\n",
            "Acc : 0.7676767706871033 train_loss : 0.13251739548098657\n",
            "Acc : 0.7725694179534912 train_loss : 0.13303077755830225\n",
            "Acc : 0.7776199579238892 train_loss : 0.1326464557438899\n",
            "Acc : 0.7825126051902771 train_loss : 0.13304103010251553\n",
            "Acc : 0.7874053120613098 train_loss : 0.13364816541379354\n",
            "Acc : 0.7922979593276978 train_loss : 0.13393248678185046\n",
            "Acc : 0.7971906661987305 train_loss : 0.13383689851309202\n",
            "Acc : 0.8022411465644836 train_loss : 0.13317000291045802\n",
            "Acc : 0.8071338534355164 train_loss : 0.13337204367653724\n",
            "Acc : 0.8120265007019043 train_loss : 0.1334777912806447\n",
            "Acc : 0.8170770406723022 train_loss : 0.13286440483096873\n",
            "Acc : 0.8221275210380554 train_loss : 0.13223472355404892\n",
            "Acc : 0.8271780014038086 train_loss : 0.1318498000971036\n",
            "Acc : 0.8319128751754761 train_loss : 0.1322854256723076\n",
            "Acc : 0.8369633555412292 train_loss : 0.1316330901749388\n",
            "Acc : 0.8420138955116272 train_loss : 0.1310399048468646\n",
            "Acc : 0.8469065427780151 train_loss : 0.1312277063465955\n",
            "Acc : 0.8517992496490479 train_loss : 0.1318439344853856\n",
            "Acc : 0.856849730014801 train_loss : 0.13131359191557576\n",
            "Acc : 0.861900269985199 train_loss : 0.13138639625033427\n",
            "Acc : 0.8667929172515869 train_loss : 0.1312497881267752\n",
            "Acc : 0.8718434572219849 train_loss : 0.13074147595431318\n",
            "Acc : 0.876893937587738 train_loss : 0.13040382600268402\n",
            "Acc : 0.8819444179534912 train_loss : 0.12994738576117526\n",
            "Acc : 0.8868371248245239 train_loss : 0.12993614900045555\n",
            "Acc : 0.8918876051902771 train_loss : 0.12941218608369429\n",
            "Acc : 0.896938145160675 train_loss : 0.12884241825185758\n",
            "Acc : 0.901830792427063 train_loss : 0.1296377606628524\n",
            "Acc : 0.9068813323974609 train_loss : 0.12915688129192818\n",
            "Acc : 0.9117739796638489 train_loss : 0.12923160967741\n",
            "Acc : 0.9166666865348816 train_loss : 0.12994116088627158\n",
            "Acc : 0.9215593338012695 train_loss : 0.1298787493999767\n",
            "Acc : 0.9266098737716675 train_loss : 0.1293836976376606\n",
            "Acc : 0.9316603541374207 train_loss : 0.12890195683080782\n",
            "Acc : 0.9363952279090881 train_loss : 0.12981700608457522\n",
            "Acc : 0.9414457082748413 train_loss : 0.1292609500845796\n",
            "Acc : 0.9464961886405945 train_loss : 0.12866021350942825\n",
            "Acc : 0.9515467286109924 train_loss : 0.1281368722799622\n",
            "Acc : 0.9565972089767456 train_loss : 0.12768701157518156\n",
            "Acc : 0.9616477489471436 train_loss : 0.1272192470508522\n",
            "Acc : 0.9666982293128967 train_loss : 0.1268117829058797\n",
            "Acc : 0.9717487096786499 train_loss : 0.1264662700277582\n",
            "Acc : 0.9763257503509521 train_loss : 0.12748328897496922\n",
            "Acc : 0.9813762903213501 train_loss : 0.12746510976887862\n",
            "Validating....\n",
            "Acc 0.018308080732822418 train_loss 0.556359589099884\n",
            "Acc 0.036616161465644836 train_loss 0.6437731385231018\n",
            "Acc 0.05681818351149559 train_loss 0.4342434449742238\n",
            "Acc 0.07449495047330856 train_loss 0.42282001255080104\n",
            "Acc 0.09343434125185013 train_loss 0.4068457629531622\n",
            "Acc 0.11047979444265366 train_loss 0.5043362996851405\n",
            "Acc 0.12689393758773804 train_loss 0.6117393648517984\n",
            "Acc 0.14646464586257935 train_loss 0.5671939866151661\n",
            "Acc 0.16477273404598236 train_loss 0.5757479152331749\n",
            "Acc 0.18371212482452393 train_loss 0.5867729915305973\n",
            "Acc 0.2013888955116272 train_loss 0.6158490788868882\n",
            "Acc 0.21843434870243073 train_loss 0.6464086185830334\n",
            "Acc 0.23674242198467255 train_loss 0.616341759235813\n",
            "Acc 0.25631314516067505 train_loss 0.5911692517942616\n",
            "Acc 0.2765151560306549 train_loss 0.554942033564051\n",
            "Acc 0.2941919267177582 train_loss 0.5824720318196341\n",
            "Acc 0.31313130259513855 train_loss 0.5600553600884536\n",
            "Acc 0.33143940567970276 train_loss 0.5541989462864068\n",
            "Acc 0.3516414165496826 train_loss 0.5268021189656696\n",
            "Acc 0.3718434274196625 train_loss 0.50169035512954\n",
            "Acc 0.3920454680919647 train_loss 0.4783767681302769\n",
            "Acc 0.41161614656448364 train_loss 0.4721240637908605\n",
            "Acc 0.4305555522441864 train_loss 0.4687560271552723\n",
            "Acc 0.4488636255264282 train_loss 0.47776156846278656\n",
            "Acc 0.46590909361839294 train_loss 0.48972287829965355\n",
            "Acc 0.48421716690063477 train_loss 0.5023190716926295\n",
            "Acc 0.5031565427780151 train_loss 0.4957952129275159\n",
            "Acc 0.5214646458625793 train_loss 0.5022092296276242\n",
            "Acc 0.5385100841522217 train_loss 0.5251454078624475\n",
            "Acc 0.556186854839325 train_loss 0.5309307520277798\n",
            "Acc 0.5744949579238892 train_loss 0.5376775326928304\n",
            "Acc 0.5940656661987305 train_loss 0.5278726890392136\n",
            "Acc 0.6130050420761108 train_loss 0.5205775372627558\n",
            "Acc 0.629419207572937 train_loss 0.538376305005787\n",
            "Validating....\n",
            "Acc 0.6477272510528564 train_loss 0.5431961314752698\n",
            "Acc 0.6672979593276978 train_loss 0.5345441069609175\n",
            "Acc 0.684974730014801 train_loss 0.545236031726204\n",
            "Acc 0.7026515007019043 train_loss 0.5490001486859432\n",
            "Acc 0.7196969985961914 train_loss 0.5567481181321617\n",
            "Acc 0.7354797720909119 train_loss 0.562538118637167\n",
            "Acc 0.7531565427780151 train_loss 0.5616638965495839\n",
            "Acc 0.7708333134651184 train_loss 0.5718158540049834\n",
            "Acc 0.7891414165496826 train_loss 0.5733100369702592\n",
            "Acc 0.8068181872367859 train_loss 0.5787152945783667\n",
            "Acc 0.8257575631141663 train_loss 0.5729013283840484\n",
            "Acc 0.8440656661987305 train_loss 0.5684973325215928\n",
            "Acc 0.8617424368858337 train_loss 0.5725096945868845\n",
            "Acc 0.8806818127632141 train_loss 0.568851258586316\n",
            "Acc 0.8989899158477783 train_loss 0.5699282434331824\n",
            "Acc 0.9078282713890076 train_loss 0.5790533649735152\n",
            "F1_score : 0.8286384976525821\n",
            "Epoch completed in 1.4365379095077515 minutes\n",
            "Training completed in 16.35578771829605 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKP1r_H-NOuq"
      },
      "source": [
        "class MakeTestDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        cleaned_tweet = data_row['cleaned_tweet']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "                                              cleaned_tweet,\n",
        "                                              add_special_tokens = True,\n",
        "                                              max_length = self.max_token_len,\n",
        "                                              return_token_type_ids = False,\n",
        "                                              padding = \"max_length\",\n",
        "                                              truncation = True,\n",
        "                                              return_attention_mask = True,\n",
        "                                              return_tensors = 'pt',\n",
        "                                             )\n",
        "        return dict(\n",
        "                    cleaned_tweet = cleaned_tweet,\n",
        "                    input_ids = encoding[\"input_ids\"].flatten(),\n",
        "                    attention_mask = encoding[\"attention_mask\"].flatten(),\n",
        "                   )\n",
        "\n",
        "valid_dataset = MakeTestDataset(\n",
        "                               valid_df,\n",
        "                               tokenizer,\n",
        "                               max_token_len = MAX_TOKEN_COUNT\n",
        "                              )\n",
        "test_dataset = MakeTestDataset(\n",
        "                               test,\n",
        "                               tokenizer,\n",
        "                               max_token_len = MAX_TOKEN_COUNT\n",
        "                              )\n",
        "\n",
        "valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                valid_dataset,\n",
        "                                                                num_replicas = xm.xrt_world_size(),\n",
        "                                                                rank = xm.get_ordinal(),\n",
        "                                                                shuffle = False\n",
        "                                                               )\n",
        "\n",
        "test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                                                                test_dataset,\n",
        "                                                                num_replicas = xm.xrt_world_size(),\n",
        "                                                                rank = xm.get_ordinal(),\n",
        "                                                                shuffle = False\n",
        "                                                               )\n",
        "BATCH_SIZE = 32\n",
        "valid_data_loader = DataLoader(\n",
        "                               valid_dataset,\n",
        "                               sampler = valid_sampler,\n",
        "                               batch_size = BATCH_SIZE,\n",
        "                               num_workers = 2\n",
        "                              )\n",
        "test_data_loader = DataLoader(\n",
        "                               test_dataset,\n",
        "                               sampler = test_sampler,\n",
        "                               batch_size = BATCH_SIZE,\n",
        "                               num_workers = 2\n",
        "                              )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlvsEYY4WOE"
      },
      "source": [
        "#predict valid set\n",
        "val_preds = []\n",
        "model = model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(valid_data_loader):\n",
        "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs,_ = torch.max(outputs, dim = 1)\n",
        "        val_preds.extend(outputs.tolist())"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "ndoFEmci8zKT",
        "outputId": "79f54305-f95f-42f0-a4af-1ebf3655de8d"
      },
      "source": [
        "#plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(valid_df['label'], val_preds)\n",
        "\n",
        "# calculate the g-mean for each threshold\n",
        "gmeans = np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "# locate the index of the largest g-mean\n",
        "ix = np.argmax(gmeans)\n",
        "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.figure(num = 0, figsize = [6.4, 4.8])\n",
        "plt.plot([0,1], [0,1], linestyle = '--', label = 'No Skill')\n",
        "plt.plot(fpr, tpr, marker = '.', label = 'Bert')\n",
        "plt.scatter(fpr[ix], tpr[ix], marker = 'o', color = 'black', label = 'Best')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Threshold=0.023303, G-Mean=0.924\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 460.8x345.6 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uufGSnSU6Cva"
      },
      "source": [
        "val_prediction = [int(pred > thresholds[ix]) for pred in val_preds]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXh4Uypp6iTi",
        "outputId": "1f0e65f5-3a44-47ad-abc4-dfa38af9e657"
      },
      "source": [
        "print(classification_report(valid_df['label'], val_prediction))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      1152\n",
            "           1       0.79      0.94      0.86       432\n",
            "\n",
            "    accuracy                           0.91      1584\n",
            "   macro avg       0.88      0.92      0.90      1584\n",
            "weighted avg       0.92      0.91      0.92      1584\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQwDuTYrQs6I"
      },
      "source": [
        "#predict test set\n",
        "preds = []\n",
        "model = model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_data_loader):\n",
        "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        outputs,_ = torch.max(outputs, dim = 1)\n",
        "        preds.extend(outputs.tolist())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XGgZbF_RLE_"
      },
      "source": [
        "prediction = [int(pred > thresholds[ix]) for pred in preds]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pd6m4GmNqOe"
      },
      "source": [
        "ss['label'] = prediction"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXtgDKReDYTe",
        "outputId": "0f9820d0-2d7a-46f1-c30f-bc0a95f9662b"
      },
      "source": [
        "ss['label'].value_counts()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1291\n",
              "1     662\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DXLRzmykN0b0",
        "outputId": "84c609c8-bf39-457a-da89-e79a57e60263"
      },
      "source": [
        "from google.colab import files\n",
        "ss.to_csv('sub8.csv', index = False)\n",
        "files.download('sub8.csv')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_76a95444-aae1-4ca8-a34b-f037f0b62b68\", \"sub8.csv\", 13680)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}